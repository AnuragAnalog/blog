{
  
    
        "post0": {
            "title": "Tabular Playground Series Mar 2022",
            "content": "!pip3 install kaggle !pip3 install prophet !pip3 install pystan==2.19.1.1 . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.1) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.63.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: prophet in /usr/local/lib/python3.7/dist-packages (1.0.1) Requirement already satisfied: pystan~=2.19.1.1 in /usr/local/lib/python3.7/dist-packages (from prophet) (2.19.1.1) Requirement already satisfied: numpy&gt;=1.15.4 in /usr/local/lib/python3.7/dist-packages (from prophet) (1.21.5) Requirement already satisfied: holidays&gt;=0.10.2 in /usr/local/lib/python3.7/dist-packages (from prophet) (0.10.5.2) Requirement already satisfied: pandas&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from prophet) (1.3.5) Requirement already satisfied: convertdate&gt;=2.1.2 in /usr/local/lib/python3.7/dist-packages (from prophet) (2.4.0) Requirement already satisfied: python-dateutil&gt;=2.8.0 in /usr/local/lib/python3.7/dist-packages (from prophet) (2.8.2) Requirement already satisfied: tqdm&gt;=4.36.1 in /usr/local/lib/python3.7/dist-packages (from prophet) (4.63.0) Requirement already satisfied: matplotlib&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from prophet) (3.2.2) Requirement already satisfied: cmdstanpy==0.9.68 in /usr/local/lib/python3.7/dist-packages (from prophet) (0.9.68) Requirement already satisfied: setuptools-git&gt;=1.2 in /usr/local/lib/python3.7/dist-packages (from prophet) (1.2) Requirement already satisfied: LunarCalendar&gt;=0.0.9 in /usr/local/lib/python3.7/dist-packages (from prophet) (0.0.9) Requirement already satisfied: Cython&gt;=0.22 in /usr/local/lib/python3.7/dist-packages (from prophet) (0.29.28) Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from cmdstanpy==0.9.68-&gt;prophet) (5.1.0) Requirement already satisfied: pymeeus&lt;=1,&gt;=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate&gt;=2.1.2-&gt;prophet) (0.5.11) Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays&gt;=0.10.2-&gt;prophet) (0.2.1) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from holidays&gt;=0.10.2-&gt;prophet) (1.15.0) Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays&gt;=0.10.2-&gt;prophet) (2.2.3) Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from LunarCalendar&gt;=0.0.9-&gt;prophet) (2018.9) Requirement already satisfied: ephem&gt;=3.7.5.3 in /usr/local/lib/python3.7/dist-packages (from LunarCalendar&gt;=0.0.9-&gt;prophet) (4.1.3) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.0.0-&gt;prophet) (1.4.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.0.0-&gt;prophet) (3.0.7) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.0.0-&gt;prophet) (0.11.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib&gt;=2.0.0-&gt;prophet) (3.10.0.2) Requirement already satisfied: pystan==2.19.1.1 in /usr/local/lib/python3.7/dist-packages (2.19.1.1) Requirement already satisfied: Cython!=0.25.1,&gt;=0.22 in /usr/local/lib/python3.7/dist-packages (from pystan==2.19.1.1) (0.29.28) Requirement already satisfied: numpy&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from pystan==2.19.1.1) (1.21.5) . import tqdm import numpy as np import pandas as pd import seaborn as sns from zipfile import ZipFile from prophet import Prophet from matplotlib import pyplot as plt . %matplotlib inline plt.rcParams[&#39;figure.figsize&#39;] = (12, 12) . Before running the below cell, upload your kaggle token, to make sure an error doesn&#39;t popup. . !mkdir ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . mkdir: cannot create directory ‘/root/.kaggle’: File exists . !kaggle competitions download -c tabular-playground-series-mar-2022 . tabular-playground-series-mar-2022.zip: Skipping, found more recently modified local copy (use --force to force download) . with ZipFile(&#39;/content/tabular-playground-series-mar-2022.zip&#39;, &#39;r&#39;) as zf: zf.extractall(&#39;./&#39;) . Loading the data . train = pd.read_csv(&#39;train.csv&#39;, index_col=&#39;row_id&#39;, parse_dates=[&#39;time&#39;]) train.head() . time x y direction congestion . row_id . 0 1991-04-01 | 0 | 0 | EB | 70 | . 1 1991-04-01 | 0 | 0 | NB | 49 | . 2 1991-04-01 | 0 | 0 | SB | 24 | . 3 1991-04-01 | 0 | 1 | EB | 18 | . 4 1991-04-01 | 0 | 1 | NB | 60 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train.info() train.describe() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 848835 entries, 0 to 848834 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 time 848835 non-null datetime64[ns] 1 x 848835 non-null int64 2 y 848835 non-null int64 3 direction 848835 non-null object 4 congestion 848835 non-null int64 dtypes: datetime64[ns](1), int64(3), object(1) memory usage: 38.9+ MB . x y congestion . count 848835.000000 | 848835.000000 | 848835.000000 | . mean 1.138462 | 1.630769 | 47.815305 | . std 0.801478 | 1.089379 | 16.799392 | . min 0.000000 | 0.000000 | 0.000000 | . 25% 0.000000 | 1.000000 | 35.000000 | . 50% 1.000000 | 2.000000 | 47.000000 | . 75% 2.000000 | 3.000000 | 60.000000 | . max 2.000000 | 3.000000 | 100.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; sns.heatmap(train.corr(), annot=True, vmin=-1, vmax=1, cmap=&#39;RdYlGn&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa74ad97410&gt; . test = pd.read_csv(&#39;test.csv&#39;, index_col=&#39;row_id&#39;, parse_dates=[&#39;time&#39;]) test.head() . time x y direction . row_id . 848835 1991-09-30 12:00:00 | 0 | 0 | EB | . 848836 1991-09-30 12:00:00 | 0 | 0 | NB | . 848837 1991-09-30 12:00:00 | 0 | 0 | SB | . 848838 1991-09-30 12:00:00 | 0 | 1 | EB | . 848839 1991-09-30 12:00:00 | 0 | 1 | NB | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; There are no missing values, in the data. . if train.isna().any().any(): print(train.isna().sum()/train.shape[0]) else: print(&quot;No Missing values&quot;) . No Missing values . Preparation . test[&#39;congestion&#39;] = 0.0 . grouped_train_data = train.groupby([&#39;time&#39;, &#39;x&#39;, &#39;y&#39;, &#39;direction&#39;]) grouped_test_data = test.groupby([&#39;time&#39;, &#39;x&#39;, &#39;y&#39;, &#39;direction&#39;]) . train_dict = dict() test_dict = dict() for g in grouped_train_data: if (g[0][1], g[0][2], g[0][3]) in train_dict.keys(): train_dict[(g[0][1], g[0][2], g[0][3])].append((g[0][0], g[1][&#39;congestion&#39;].values[0])) else: train_dict[(g[0][1], g[0][2], g[0][3])] = [(g[0][0], g[1][&#39;congestion&#39;].values[0])] for g in grouped_test_data: if (g[0][1], g[0][2], g[0][3]) in test_dict.keys(): test_dict[(g[0][1], g[0][2], g[0][3])].append((g[0][0], g[1][&#39;congestion&#39;].values[0])) else: test_dict[(g[0][1], g[0][2], g[0][3])] = [(g[0][0], g[1][&#39;congestion&#39;].values[0])] . for idx, li in train_dict.items(): train_dict[idx] = pd.DataFrame(columns=[&#39;ds&#39;, &#39;y&#39;], data=li) for idx, li in test_dict.items(): test_dict[idx] = pd.DataFrame(columns=[&#39;ds&#39;, &#39;y&#39;], data=li).drop([&#39;y&#39;], axis=1) . Modelling . Approach-1 . In this method, I have grouped the data into a number of instances and made the predictions on that instances. . An instance is uniquely identifiable by its a key which is a combination of its cordinates and the direction. . for idx, train_data in tqdm.tqdm(train_dict.items()): model = Prophet() model.fit(train_data) forecast = model.predict(test_dict[idx]) test_dict[idx][&#39;congestion&#39;] = np.round(forecast[&#39;yhat&#39;]) test_dict[idx][&#39;x&#39;] = idx[0] test_dict[idx][&#39;y&#39;] = idx[1] test_dict[idx][&#39;direction&#39;] = idx[2] . 0%| | 0/65 [00:00&lt;?, ?it/s]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 2%|▏ | 1/65 [00:08&lt;08:55, 8.36s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 3%|▎ | 2/65 [00:12&lt;06:23, 6.09s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 5%|▍ | 3/65 [00:20&lt;06:55, 6.70s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 6%|▌ | 4/65 [00:29&lt;07:53, 7.76s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 8%|▊ | 5/65 [00:36&lt;07:21, 7.37s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 9%|▉ | 6/65 [00:41&lt;06:39, 6.77s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 11%|█ | 7/65 [00:48&lt;06:37, 6.85s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 12%|█▏ | 8/65 [00:55&lt;06:24, 6.75s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 14%|█▍ | 9/65 [01:00&lt;05:49, 6.24s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 15%|█▌ | 10/65 [01:06&lt;05:35, 6.10s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 17%|█▋ | 11/65 [01:11&lt;05:08, 5.71s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 18%|█▊ | 12/65 [01:16&lt;04:56, 5.60s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 20%|██ | 13/65 [01:24&lt;05:25, 6.25s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 22%|██▏ | 14/65 [01:28&lt;04:46, 5.63s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 23%|██▎ | 15/65 [01:34&lt;04:46, 5.72s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 25%|██▍ | 16/65 [01:39&lt;04:31, 5.55s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 26%|██▌ | 17/65 [01:45&lt;04:26, 5.55s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 28%|██▊ | 18/65 [01:51&lt;04:29, 5.74s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 29%|██▉ | 19/65 [01:57&lt;04:23, 5.72s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 31%|███ | 20/65 [02:02&lt;04:14, 5.65s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 32%|███▏ | 21/65 [02:06&lt;03:41, 5.03s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 34%|███▍ | 22/65 [02:11&lt;03:46, 5.26s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 35%|███▌ | 23/65 [02:18&lt;03:52, 5.54s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 37%|███▋ | 24/65 [02:21&lt;03:24, 4.99s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 38%|███▊ | 25/65 [02:28&lt;03:43, 5.58s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 40%|████ | 26/65 [02:37&lt;04:09, 6.40s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 42%|████▏ | 27/65 [02:44&lt;04:15, 6.72s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 43%|████▎ | 28/65 [02:51&lt;04:10, 6.78s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 45%|████▍ | 29/65 [02:56&lt;03:44, 6.24s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 46%|████▌ | 30/65 [03:02&lt;03:34, 6.14s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 48%|████▊ | 31/65 [03:08&lt;03:34, 6.29s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 49%|████▉ | 32/65 [03:14&lt;03:17, 5.99s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 51%|█████ | 33/65 [03:19&lt;03:07, 5.85s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 52%|█████▏ | 34/65 [03:23&lt;02:43, 5.28s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 54%|█████▍ | 35/65 [03:29&lt;02:39, 5.33s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 55%|█████▌ | 36/65 [03:35&lt;02:47, 5.76s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 57%|█████▋ | 37/65 [03:41&lt;02:41, 5.77s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 58%|█████▊ | 38/65 [03:46&lt;02:30, 5.57s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 60%|██████ | 39/65 [03:49&lt;02:04, 4.78s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 62%|██████▏ | 40/65 [03:55&lt;02:07, 5.12s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 63%|██████▎ | 41/65 [04:03&lt;02:24, 6.03s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 65%|██████▍ | 42/65 [04:10&lt;02:20, 6.10s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 66%|██████▌ | 43/65 [04:14&lt;02:03, 5.62s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 68%|██████▊ | 44/65 [04:21&lt;02:06, 6.01s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 69%|██████▉ | 45/65 [04:27&lt;02:01, 6.08s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 71%|███████ | 46/65 [04:32&lt;01:48, 5.70s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 72%|███████▏ | 47/65 [04:37&lt;01:39, 5.54s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 74%|███████▍ | 48/65 [04:42&lt;01:29, 5.29s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 75%|███████▌ | 49/65 [04:49&lt;01:34, 5.92s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 77%|███████▋ | 50/65 [04:55&lt;01:25, 5.69s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 78%|███████▊ | 51/65 [05:00&lt;01:17, 5.51s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 80%|████████ | 52/65 [05:04&lt;01:07, 5.20s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 82%|████████▏ | 53/65 [05:11&lt;01:08, 5.74s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 83%|████████▎ | 54/65 [05:17&lt;01:05, 5.93s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 85%|████████▍ | 55/65 [05:23&lt;00:57, 5.77s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 86%|████████▌ | 56/65 [05:28&lt;00:51, 5.73s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 88%|████████▊ | 57/65 [05:35&lt;00:48, 6.03s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 89%|████████▉ | 58/65 [05:40&lt;00:40, 5.78s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 91%|█████████ | 59/65 [05:46&lt;00:33, 5.58s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 92%|█████████▏| 60/65 [05:51&lt;00:28, 5.67s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 94%|█████████▍| 61/65 [05:55&lt;00:20, 5.05s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 95%|█████████▌| 62/65 [05:59&lt;00:14, 4.68s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 97%|█████████▋| 63/65 [06:04&lt;00:09, 4.79s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 98%|█████████▊| 64/65 [06:08&lt;00:04, 4.56s/it]INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this. 100%|██████████| 65/65 [06:11&lt;00:00, 5.72s/it] . preds_semi_final = pd.concat(test_dict.values(), ignore_index=True) preds_final = test.reset_index().merge(preds_semi_final, left_on=[&#39;time&#39;, &#39;x&#39;, &#39;y&#39;, &#39;direction&#39;], right_on=[&#39;ds&#39;, &#39;x&#39;, &#39;y&#39;, &#39;direction&#39;])[[&#39;row_id&#39;, &#39;congestion_y&#39;]] . submission = pd.read_csv(&#39;/content/sample_submission.csv&#39;) submission = submission.merge(preds_final, on=&#39;row_id&#39;)[[&#39;row_id&#39;, &#39;congestion_y&#39;]].rename(columns={&#39;congestion_y&#39;: &#39;congestion&#39;}) submission.to_csv(&#39;output.csv&#39;, index=False) . !kaggle competitions submit -c tabular-playground-series-mar-2022 -f output.csv -m &quot;FB Prophet correct 2 with round&quot; . 100% 27.4k/27.4k [00:00&lt;00:00, 150kB/s] Successfully submitted to Tabular Playground Series - Mar 2022 .",
            "url": "https://anuraganalog.github.io/blog/kaggle/fbprophet/jupyter/tps/2022/03/24/Tabular-Playground-Series-Mar-2022-FB-Prophet.html",
            "relUrl": "/kaggle/fbprophet/jupyter/tps/2022/03/24/Tabular-Playground-Series-Mar-2022-FB-Prophet.html",
            "date": " • Mar 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Analyze Your Runkeeper Fitness Data",
            "content": "1. Obtain and review raw data . One day, my old running friend and I were chatting about our running styles, training habits, and achievements, when I suddenly realized that I could take an in-depth analytical look at my training. I have been using a popular GPS fitness tracker called Runkeeper for years and decided it was time to analyze my running data to see how I was doing. . Since 2012, I&#39;ve been using the Runkeeper app, and it&#39;s great. One key feature: its excellent data export. Anyone who has a smartphone can download the app and analyze their data like we will in this notebook. . . After logging your run, the first step is to export the data from Runkeeper (which I&#39;ve done already). Then import the data and start exploring to find potential problems. After that, create data cleaning strategies to fix the issues. Finally, analyze and visualize the clean time-series data. . I exported seven years worth of my training data, from 2012 through 2018. The data is a CSV file where each row is a single training activity. Let&#39;s load and inspect it. . # ... YOUR CODE FOR TASK 1 ... import numpy as np import pandas as pd # Define file containing dataset runkeeper_file = &#39;datasets/cardioActivities.csv&#39; # Create DataFrame with parse_dates and index_col parameters df_activities = pd.read_csv(runkeeper_file, parse_dates=[&#39;Date&#39;], index_col=[&#39;Date&#39;]) # First look at exported data: select sample of 3 random rows display(df_activities.sample(3)) # Print DataFrame summary # ... YOUR CODE FOR TASK 1 ... df_activities.info() . Activity Id Type Route Name Distance (km) Duration Average Pace Average Speed (km/h) Calories Burned Climb (m) Average Heart Rate (bpm) Friend&#39;s Tagged Notes GPX File . Date . 2016-11-21 18:57:32 dcb51374-c574-4fac-8f30-d2f2080368eb | Running | NaN | 10.05 | 56:45 | 5:39 | 10.63 | 704.0 | 96 | 146.0 | NaN | TomTom MySports Watch | 2016-11-21-185732.gpx | . 2015-06-21 17:53:46 ee566558-df2c-4874-8470-4c96bc16e11b | Cycling | NaN | 34.49 | 1:35:39 | 2:46 | 21.63 | 751.0 | 318 | NaN | NaN | NaN | 2015-06-21-175346.gpx | . 2017-05-30 18:19:20 58569994-61c8-400c-a647-114e161ff8df | Running | NaN | 8.42 | 42:48 | 5:05 | 11.80 | 584.0 | 85 | 154.0 | NaN | TomTom MySports Watch | 2017-05-30-181920.gpx | . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 508 entries, 2018-11-11 14:05:12 to 2012-08-22 18:53:54 Data columns (total 13 columns): Activity Id 508 non-null object Type 508 non-null object Route Name 1 non-null object Distance (km) 508 non-null float64 Duration 508 non-null object Average Pace 508 non-null object Average Speed (km/h) 508 non-null float64 Calories Burned 508 non-null float64 Climb (m) 508 non-null int64 Average Heart Rate (bpm) 294 non-null float64 Friend&#39;s Tagged 0 non-null float64 Notes 231 non-null object GPX File 504 non-null object dtypes: float64(5), int64(1), object(7) memory usage: 55.6+ KB . 2. Data preprocessing . Lucky for us, the column names Runkeeper provides are informative, and we don&#39;t need to rename any columns. . But, we do notice missing values using the info() method. What are the reasons for these missing values? It depends. Some heart rate information is missing because I didn&#39;t always use a cardio sensor. In the case of the Notes column, it is an optional field that I sometimes left blank. Also, I only used the Route Name column once, and never used the Friend&#39;s Tagged column. . We&#39;ll fill in missing values in the heart rate column to avoid misleading results later, but right now, our first data preprocessing steps will be to: . Remove columns not useful for our analysis. | Replace the &quot;Other&quot; activity type to &quot;Unicycling&quot; because that was always the &quot;Other&quot; activity. | Count missing values. | . cols_to_drop = [&#39;Friend &#39;s Tagged&#39;,&#39;Route Name&#39;,&#39;GPX File&#39;,&#39;Activity Id&#39;,&#39;Calories Burned&#39;, &#39;Notes&#39;] # Delete unnecessary columns # ... YOUR CODE FOR TASK 2 ... df_activities.drop(cols_to_drop, axis=1, inplace=True) # Count types of training activities display(df_activities[&#39;Type&#39;].value_counts()) # Rename &#39;Other&#39; type to &#39;Unicycling&#39; df_activities[&#39;Type&#39;] = df_activities[&#39;Type&#39;].str.replace(&#39;Other&#39;, &#39;Unicycling&#39;) # Count missing values for each column # ... YOUR CODE FOR TASK 2 ... df_activities.isnull().sum() . Running 459 Cycling 29 Walking 18 Other 2 Name: Type, dtype: int64 . Type 0 Distance (km) 0 Duration 0 Average Pace 0 Average Speed (km/h) 0 Climb (m) 0 Average Heart Rate (bpm) 214 dtype: int64 . 3. Dealing with missing values . As we can see from the last output, there are 214 missing entries for my average heart rate. . We can&#39;t go back in time to get those data, but we can fill in the missing values with an average value. This process is called mean imputation. When imputing the mean to fill in missing data, we need to consider that the average heart rate varies for different activities (e.g., walking vs. running). We&#39;ll filter the DataFrames by activity type (Type) and calculate each activity&#39;s mean heart rate, then fill in the missing values with those means. . avg_hr_run = df_activities[df_activities[&#39;Type&#39;] == &#39;Running&#39;][&#39;Average Heart Rate (bpm)&#39;].mean() avg_hr_cycle = df_activities[df_activities[&#39;Type&#39;] == &#39;Cycling&#39;][&#39;Average Heart Rate (bpm)&#39;].mean() # Split whole DataFrame into several, specific for different activities df_run = df_activities[df_activities[&#39;Type&#39;] == &#39;Running&#39;].copy() df_walk = df_activities[df_activities[&#39;Type&#39;] == &#39;Walking&#39;].copy() df_cycle = df_activities[df_activities[&#39;Type&#39;] == &#39;Cycling&#39;].copy() # Filling missing values with counted means df_walk[&#39;Average Heart Rate (bpm)&#39;].fillna(110, inplace=True) df_run[&#39;Average Heart Rate (bpm)&#39;].fillna(int(avg_hr_run), inplace=True) df_cycle[&#39;Average Heart Rate (bpm)&#39;].fillna(int(avg_hr_cycle), inplace=True) # ... YOUR CODE FOR TASK 3 ... # Count missing values for each column in running data # ... YOUR CODE FOR TASK 3 ... df_run.isnull().sum() . Type 0 Distance (km) 0 Duration 0 Average Pace 0 Average Speed (km/h) 0 Climb (m) 0 Average Heart Rate (bpm) 0 dtype: int64 . 4. Plot running data . Now we can create our first plot! As we found earlier, most of the activities in my data were running (459 of them to be exact). There are only 29, 18, and two instances for cycling, walking, and unicycling, respectively. So for now, let&#39;s focus on plotting the different running metrics. . An excellent first visualization is a figure with four subplots, one for each running metric (each numerical column). Each subplot will have a different y-axis, which is explained in each legend. The x-axis, Date, is shared among all subplots. . %matplotlib inline # Import matplotlib, set style and ignore warning import matplotlib.pyplot as plt %matplotlib inline import warnings plt.style.use(&#39;ggplot&#39;) warnings.filterwarnings( action=&#39;ignore&#39;, module=&#39;matplotlib.figure&#39;, category=UserWarning, message=(&#39;This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.&#39;) ) # Prepare data subsetting period from 2013 till 2018 runs_subset_2013_2018 = df_run[&#39;2018&#39;:&#39;2013&#39;] # Create, plot and customize in one step runs_subset_2013_2018.plot(subplots=True, sharex=False, figsize=(12,16), linestyle=&#39;none&#39;, marker=&#39;o&#39;, markersize=3, ) # Show plot # ... YOUR CODE FOR TASK 4 ... plt.show() . 5. Running statistics . No doubt, running helps people stay mentally and physically healthy and productive at any age. And it is great fun! When runners talk to each other about their hobby, we not only discuss our results, but we also discuss different training strategies. . You&#39;ll know you&#39;re with a group of runners if you commonly hear questions like: . What is your average distance? | How fast do you run? | Do you measure your heart rate? | How often do you train? | . Let&#39;s find the answers to these questions in my data. If you look back at plots in Task 4, you can see the answer to, Do you measure your heart rate? Before 2015: no. To look at the averages, let&#39;s only use the data from 2015 through 2018. . In pandas, the resample() method is similar to the groupby() method - with resample() you group by a specific time span. We&#39;ll use resample() to group the time series data by a sampling period and apply several methods to each sampling period. In our case, we&#39;ll resample annually and weekly. . runs_subset_2015_2018 = df_run[&#39;2018&#39;:&#39;2015&#39;] # Calculate annual statistics print(&#39;How my average run looks in last 4 years:&#39;) display(runs_subset_2015_2018.resample(&#39;A&#39;).mean()) # Calculate weekly statistics print(&#39;Weekly averages of last 4 years:&#39;) display(runs_subset_2015_2018.resample(&#39;W&#39;).mean().mean()) # Mean weekly counts weekly_counts_average = runs_subset_2015_2018[&#39;Distance (km)&#39;].resample(&#39;W&#39;).count().mean() print(&#39;How many trainings per week I had on average:&#39;, weekly_counts_average) . How my average run looks in last 4 years: . Distance (km) Average Speed (km/h) Climb (m) Average Heart Rate (bpm) . Date . 2015-12-31 13.602805 | 10.998902 | 160.170732 | 143.353659 | . 2016-12-31 11.411667 | 10.837778 | 133.194444 | 143.388889 | . 2017-12-31 12.935176 | 10.959059 | 169.376471 | 145.247059 | . 2018-12-31 13.339063 | 10.777969 | 191.218750 | 148.125000 | . Weekly averages of last 4 years: . Distance (km) 12.518176 Average Speed (km/h) 10.835473 Climb (m) 158.325444 Average Heart Rate (bpm) 144.801775 dtype: float64 . How many trainings per week I had on average: 1.5 . 6. Visualization with averages . Let&#39;s plot the long term averages of my distance run and my heart rate with their raw data to visually compare the averages to each training session. Again, we&#39;ll use the data from 2015 through 2018. . In this task, we will use matplotlib functionality for plot creation and customization. . runs_subset_2015_2018 = df_run[&#39;2018&#39;:&#39;2015&#39;] runs_distance = runs_subset_2015_2018[&#39;Distance (km)&#39;] runs_hr = runs_subset_2015_2018[&#39;Average Heart Rate (bpm)&#39;] # Create plot fig, (ax1, ax2) = plt.subplots(2, sharex=True, figsize=(12, 8)) # Plot and customize first subplot # ... YOUR CODE FOR TASK 6 ... runs_distance.plot(ax=ax1) ax1.set(ylabel=&#39;Distance (km)&#39;, title=&#39;Historical data with averages&#39;) ax1.axhline(runs_distance.mean(), color=&#39;blue&#39;, linewidth=1, linestyle=&#39;-.&#39;) # Plot and customize second subplot runs_hr.plot(ax=ax2, color=&#39;gray&#39;) ax2.set(xlabel=&#39;Date&#39;, ylabel=&#39;Average Heart Rate (bpm)&#39;) ax2.axhline(runs_hr.mean(), color=&#39;blue&#39;, linewidth=1, linestyle=&#39;-.&#39;) # ... YOUR CODE FOR TASK 6 ... # Show plot plt.show() . 7. Did I reach my goals? . To motivate myself to run regularly, I set a target goal of running 1000 km per year. Let&#39;s visualize my annual running distance (km) from 2013 through 2018 to see if I reached my goal each year. Only stars in the green region indicate success. . df_run_dist_annual = df_run[&#39;2018&#39;:&#39;2013&#39;][&#39;Distance (km)&#39;].resample(&#39;A&#39;).sum() # Create plot fig = plt.figure(figsize=(8, 5)) # Plot and customize ax = df_run_dist_annual.plot(marker=&#39;*&#39;, markersize=14, linewidth=0, color=&#39;blue&#39;) ax.set(ylim=[0, 1210], xlim=[&#39;2012&#39;,&#39;2019&#39;], ylabel=&#39;Distance (km)&#39;, xlabel=&#39;Years&#39;, title=&#39;Annual totals for distance&#39;) ax.axhspan(1000, 1210, color=&#39;green&#39;, alpha=0.4) ax.axhspan(800, 1000, color=&#39;yellow&#39;, alpha=0.3) ax.axhspan(0, 800, color=&#39;red&#39;, alpha=0.3) # ... YOUR CODE FOR TASK 7 ... # Show plot # ... YOUR CODE FOR TASK 7 ... plt.show() . 8. Am I progressing? . Let&#39;s dive a little deeper into the data to answer a tricky question: am I progressing in terms of my running skills? . To answer this question, we&#39;ll decompose my weekly distance run and visually compare it to the raw data. A red trend line will represent the weekly distance run. . We are going to use statsmodels library to decompose the weekly trend. . # ... YOUR CODE FOR TASK 8 ... import statsmodels.api as sm # Prepare data df_run_dist_wkly = df_run[&#39;2018&#39;:&#39;2013&#39;][&#39;Distance (km)&#39;].resample(&#39;W&#39;).bfill() decomposed = sm.tsa.seasonal_decompose(df_run_dist_wkly, extrapolate_trend=1, freq=52) # Create plot fig = plt.figure(figsize=(12, 5)) # Plot and customize ax = decomposed.trend.plot(label=&#39;Trend&#39;, linewidth=2) ax = decomposed.observed.plot(label=&#39;Observed&#39;, linewidth=0.5) ax.legend() ax.set_title(&#39;Running distance trend&#39;) # Show plot plt.show() . /usr/local/lib/python3.6/dist-packages/matplotlib/figure.py:2369: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(&#34;This figure includes Axes that are not compatible &#34; . 9. Training intensity . Heart rate is a popular metric used to measure training intensity. Depending on age and fitness level, heart rates are grouped into different zones that people can target depending on training goals. A target heart rate during moderate-intensity activities is about 50-70% of maximum heart rate, while during vigorous physical activity it’s about 70-85% of maximum. . We&#39;ll create a distribution plot of my heart rate data by training intensity. It will be a visual presentation for the number of activities from predefined training zones. . hr_zones = [100, 125, 133, 142, 151, 173] zone_names = [&#39;Easy&#39;, &#39;Moderate&#39;, &#39;Hard&#39;, &#39;Very hard&#39;, &#39;Maximal&#39;] zone_colors = [&#39;green&#39;, &#39;yellow&#39;, &#39;orange&#39;, &#39;tomato&#39;, &#39;red&#39;] df_run_hr_all = df_run[df_run.index &gt;= &#39;2015-03-01&#39;][&#39;Average Heart Rate (bpm)&#39;] # Create plot fig, ax = plt.subplots(figsize=(8, 5)) # Plot and customize n, bins, patches = ax.hist(df_run_hr_all, bins=hr_zones, alpha=0.5) for i in range(0, len(patches)): patches[i].set_facecolor(zone_colors[i]) ax.set(title=&#39;Distribution of HR&#39;, ylabel=&#39;Number of runs&#39;) ax.xaxis.set(ticks=hr_zones) # ... YOUR CODE FOR TASK 9 ... ax.set_xticklabels(labels=zone_names, rotation=-30, ha=&#39;left&#39;) # Show plot # ... YOUR CODE FOR TASK 8 ... plt.show() . /usr/local/lib/python3.6/dist-packages/matplotlib/figure.py:2369: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(&#34;This figure includes Axes that are not compatible &#34; . 10. Detailed summary report . With all this data cleaning, analysis, and visualization, let&#39;s create detailed summary tables of my training. . To do this, we&#39;ll create two tables. The first table will be a summary of the distance (km) and climb (m) variables for each training activity. The second table will list the summary statistics for the average speed (km/hr), climb (m), and distance (km) variables for each training activity. . df_run_walk_cycle = df_run.append([df_walk, df_cycle]).sort_index(ascending=False) dist_climb_cols, speed_col = [&#39;Distance (km)&#39;, &#39;Climb (m)&#39;], [&#39;Average Speed (km/h)&#39;] # Calculating total distance and climb in each type of activities df_totals = df_run_walk_cycle.groupby(&#39;Type&#39;)[dist_climb_cols].sum() print(&#39;Totals for different training types:&#39;) display(df_totals) # Calculating summary statistics for each type of activities df_summary = df_run_walk_cycle.groupby(&#39;Type&#39;)[dist_climb_cols + speed_col].describe() # Combine totals with summary for i in dist_climb_cols: df_summary[i, &#39;total&#39;] = df_totals[i] print(&#39;Summary statistics for different training types:&#39;) # ... YOUR CODE FOR TASK 10 ... df_summary.stack() . Totals for different training types: . Distance (km) Climb (m) . Type . Cycling 680.58 | 6976 | . Running 5224.50 | 57278 | . Walking 33.45 | 349 | . Summary statistics for different training types: . Average Speed (km/h) Climb (m) Distance (km) . Type . Cycling 25% 16.980000 | 139.000000 | 15.530000 | . 50% 19.500000 | 199.000000 | 20.300000 | . 75% 21.490000 | 318.000000 | 29.400000 | . count 29.000000 | 29.000000 | 29.000000 | . max 24.330000 | 553.000000 | 49.180000 | . mean 19.125172 | 240.551724 | 23.468276 | . min 11.380000 | 58.000000 | 11.410000 | . std 3.257100 | 128.960289 | 9.451040 | . total NaN | 6976.000000 | 680.580000 | . Running 25% 10.495000 | 54.000000 | 7.415000 | . 50% 10.980000 | 91.000000 | 10.810000 | . 75% 11.520000 | 171.000000 | 13.190000 | . count 459.000000 | 459.000000 | 459.000000 | . max 20.720000 | 982.000000 | 38.320000 | . mean 11.056296 | 124.788671 | 11.382353 | . min 5.770000 | 0.000000 | 0.760000 | . std 0.953273 | 103.382177 | 4.937853 | . total NaN | 57278.000000 | 5224.500000 | . Walking 25% 5.555000 | 7.000000 | 1.385000 | . 50% 5.970000 | 10.000000 | 1.485000 | . 75% 6.512500 | 15.500000 | 1.787500 | . count 18.000000 | 18.000000 | 18.000000 | . max 6.910000 | 112.000000 | 4.290000 | . mean 5.549444 | 19.388889 | 1.858333 | . min 1.040000 | 5.000000 | 1.220000 | . std 1.459309 | 27.110100 | 0.880055 | . total NaN | 349.000000 | 33.450000 | . 11. Fun facts . To wrap up, let’s pick some fun facts out of the summary tables and solve the last exercise. . These data (my running history) represent 6 years, 2 months and 21 days. And I remember how many running shoes I went through–7. . FUN FACTS - Average distance: 11.38 km - Longest distance: 38.32 km - Highest climb: 982 m - Total climb: 57,278 m - Total number of km run: 5,224 km - Total runs: 459 - Number of running shoes gone through: 7 pairs . The story of Forrest Gump is well known–the man, who for no particular reason decided to go for a &quot;little run.&quot; His epic run duration was 3 years, 2 months and 14 days (1169 days). In the picture you can see Forrest’s route of 24,700 km. . FORREST RUN FACTS - Average distance: 21.13 km - Total number of km run: 24,700 km - Total runs: 1169 - Number of running shoes gone through: ... . Assuming Forest and I go through running shoes at the same rate, figure out how many pairs of shoes Forrest needed for his run. . . average_shoes_lifetime = 5224 # Count number of shoes for Forrest&#39;s run distance shoes_for_forrest_run = 24700/average_shoes_lifetime print(&#39;Forrest Gump would need {} pairs of shoes!&#39;.format(shoes_for_forrest_run)) . Forrest Gump would need 4.728177641653905 pairs of shoes! .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2022/03/21/Analyze-Your-Runkeeper-Fitness-Data.html",
            "relUrl": "/datacamp/projects/python/2022/03/21/Analyze-Your-Runkeeper-Fitness-Data.html",
            "date": " • Mar 21, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Analyzing Netflix Data",
            "content": "1. Loading your friend&#39;s data into a dictionary . Netflix! What started in 1997 as a DVD rental service has since exploded into the largest entertainment/media company by market capitalization, boasting over 200 million subscribers as of January 2021. . Given the large number of movies and series available on the platform, it is a perfect opportunity to flex our data manipulation skills and dive into the entertainment industry. Our friend has also been brushing up on their Python skills and has taken a first crack at a CSV file containing Netflix data. For their first order of business, they have been performing some analyses, and they believe that the average duration of movies has been declining. . As evidence of this, they have provided us with the following information. For the years from 2011 to 2020, the average movie durations are 103, 101, 99, 100, 100, 95, 95, 96, 93, and 90, respectively. . If we&#39;re going to be working with this data, we know a good place to start would be to probably start working with pandas. But first we&#39;ll need to create a DataFrame from scratch. Let&#39;s start by creating a Python object covered in Intermediate Python: a dictionary! . years = list(range(2011, 2021)) durations = [103, 101, 99, 100, 100, 95, 95, 96, 93, 90] # Create a dictionary with the two lists movie_dict = {&#39;years&#39;: years, &#39;durations&#39;: durations} # Print the dictionary movie_dict . {&#39;years&#39;: [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020], &#39;durations&#39;: [103, 101, 99, 100, 100, 95, 95, 96, 93, 90]} . 2. Creating a DataFrame from a dictionary . Perfect! We now have our friend&#39;s data stored in a nice Python object. We can already perform several operations on a dictionary to manipulate its contents (such as updating or adding to it). But a more useful structure might be a pandas DataFrame, a tabular data structure containing labeled axes and rows. Luckily, DataFrames can be created very easily from the dictionary created in the previous step! . To convert our dictionary movie_dict to a pandas DataFrame, we will first need to import the library under its usual alias. We&#39;ll also want to inspect our DataFrame to ensure it was created correctly. Let&#39;s perform these steps now. . import pandas as pd # Create a DataFrame from the dictionary durations_df = pd.DataFrame.from_dict(movie_dict) # Print the DataFrame print(durations_df) . years durations 0 2011 103 1 2012 101 2 2013 99 3 2014 100 4 2015 100 5 2016 95 6 2017 95 7 2018 96 8 2019 93 9 2020 90 . 3. A visual inspection of our data . Alright, we now have a pandas DataFrame, the most common way to work with tabular data in Python. Now back to the task at hand. We want to follow up on our friend&#39;s assertion that movie lengths have been decreasing over time. A great place to start will be a visualization of the data. . Given that the data is continuous, a line plot would be a good choice, with the dates represented along the x-axis and the average length in minutes along the y-axis. This will allow us to easily spot any trends in movie durations. There are many ways to visualize data in Python, but matploblib.pyplot is one of the most common packages to do so. . Note: In order for us to correctly test your plot, you will need to initalize a matplotlib.pyplot Figure object, which we have already provided in the cell below. You can continue to create your plot as you have learned in Intermediate Python. . import matplotlib.pyplot as plt fig = plt.figure() # Draw a line plot of release_years and durations plt.plot(durations_df[&#39;years&#39;], durations_df[&#39;durations&#39;]) # Create a title plt.title(&quot;Netflix Movie Durations 2011-2020&quot;) # Show the plot plt.show() . 4. Loading the rest of the data from a CSV . Well, it looks like there is something to the idea that movie lengths have decreased over the past ten years! But equipped only with our friend&#39;s aggregations, we&#39;re limited in the further explorations we can perform. There are a few questions about this trend that we are currently unable to answer, including: . What does this trend look like over a longer period of time? | Is this explainable by something like the genre of entertainment? | Upon asking our friend for the original CSV they used to perform their analyses, they gladly oblige and send it. We now have access to the CSV file, available at the path &quot;datasets/netflix_data.csv&quot;. Let&#39;s create another DataFrame, this time with all of the data. Given the length of our friend&#39;s data, printing the whole DataFrame is probably not a good idea, so we will inspect it by printing only the first five rows. . netflix_df = pd.read_csv(&quot;datasets/netflix_data.csv&quot;) # Print the first five rows of the DataFrame netflix_df.head() . show_id type title director cast country date_added release_year duration description genre . 0 s1 | TV Show | 3% | NaN | João Miguel, Bianca Comparato, Michel Gomes, R... | Brazil | August 14, 2020 | 2020 | 4 | In a future where the elite inhabit an island ... | International TV | . 1 s2 | Movie | 7:19 | Jorge Michel Grau | Demián Bichir, Héctor Bonilla, Oscar Serrano, ... | Mexico | December 23, 2016 | 2016 | 93 | After a devastating earthquake hits Mexico Cit... | Dramas | . 2 s3 | Movie | 23:59 | Gilbert Chan | Tedd Chan, Stella Chung, Henley Hii, Lawrence ... | Singapore | December 20, 2018 | 2011 | 78 | When an army recruit is found dead, his fellow... | Horror Movies | . 3 s4 | Movie | 9 | Shane Acker | Elijah Wood, John C. Reilly, Jennifer Connelly... | United States | November 16, 2017 | 2009 | 80 | In a postapocalyptic world, rag-doll robots hi... | Action | . 4 s5 | Movie | 21 | Robert Luketic | Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar... | United States | January 1, 2020 | 2008 | 123 | A brilliant group of students become card-coun... | Dramas | . 5. Filtering for movies! . Okay, we have our data! Now we can dive in and start looking at movie lengths. . Or can we? Looking at the first five rows of our new DataFrame, we notice a column type. Scanning the column, it&#39;s clear there are also TV shows in the dataset! Moreover, the duration column we planned to use seems to represent different values depending on whether the row is a movie or a show (perhaps the number of minutes versus the number of seasons)? . Fortunately, a DataFrame allows us to filter data quickly, and we can select rows where type is Movie. While we&#39;re at it, we don&#39;t need information from all of the columns, so let&#39;s create a new DataFrame netflix_movies containing only title, country, genre, release_year, and duration. . Let&#39;s put our data subsetting skills to work! . netflix_df_movies_only = netflix_df[netflix_df[&#39;type&#39;] == &#39;Movie&#39;] # Select only the columns of interest netflix_movies_col_subset = netflix_df_movies_only[[&#39;title&#39;, &#39;country&#39;, &#39;genre&#39;, &#39;release_year&#39;, &#39;duration&#39;]] # Print the first five rows of the new DataFrame netflix_movies_col_subset.head() . title country genre release_year duration . 1 7:19 | Mexico | Dramas | 2016 | 93 | . 2 23:59 | Singapore | Horror Movies | 2011 | 78 | . 3 9 | United States | Action | 2009 | 80 | . 4 21 | United States | Dramas | 2008 | 123 | . 6 122 | Egypt | Horror Movies | 2019 | 95 | . 6. Creating a scatter plot . Okay, now we&#39;re getting somewhere. We&#39;ve read in the raw data, selected rows of movies, and have limited our DataFrame to our columns of interest. Let&#39;s try visualizing the data again to inspect the data over a longer range of time. . This time, we are no longer working with aggregates but instead with individual movies. A line plot is no longer a good choice for our data, so let&#39;s try a scatter plot instead. We will again plot the year of release on the x-axis and the movie duration on the y-axis. . Note: Although not taught in Intermediate Python, we have provided you the code fig = plt.figure(figsize=(12,8)) to increase the size of the plot (to help you see the results), as well as to assist with testing. For more information on how to create or work with a matplotlib figure, refer to the documentation. . fig = plt.figure(figsize=(12,8)) # Create a scatter plot of duration versus year plt.scatter(netflix_movies_col_subset[&#39;release_year&#39;], netfl&#39;duration&#39;) # Create a title plt.title(&quot;Movie Duration by Year of Release&quot;) # Show the plot plt.show() . &lt;Figure size 864x576 with 0 Axes&gt; . 7. Digging deeper . This is already much more informative than the simple plot we created when our friend first gave us some data. We can also see that, while newer movies are overrepresented on the platform, many short movies have been released in the past two decades. . Upon further inspection, something else is going on. Some of these films are under an hour long! Let&#39;s filter our DataFrame for movies with a duration under 60 minutes and look at the genres. This might give us some insight into what is dragging down the average. . short_movies = netflix_movies_col_subset[netflix_movies_col_subset[&#39;duration&#39;] &lt; 60] # Print the first 20 rows of short_movies short_movies.head(20) . title country genre release_year duration . 35 #Rucker50 | United States | Documentaries | 2016 | 56 | . 55 100 Things to do Before High School | United States | Uncategorized | 2014 | 44 | . 67 13TH: A Conversation with Oprah Winfrey &amp; Ava ... | NaN | Uncategorized | 2017 | 37 | . 101 3 Seconds Divorce | Canada | Documentaries | 2018 | 53 | . 146 A 3 Minute Hug | Mexico | Documentaries | 2019 | 28 | . 162 A Christmas Special: Miraculous: Tales of Lady... | France | Uncategorized | 2016 | 22 | . 171 A Family Reunion Christmas | United States | Uncategorized | 2019 | 29 | . 177 A Go! Go! Cory Carson Christmas | United States | Children | 2020 | 22 | . 178 A Go! Go! Cory Carson Halloween | NaN | Children | 2020 | 22 | . 179 A Go! Go! Cory Carson Summer Camp | NaN | Children | 2020 | 21 | . 181 A Grand Night In: The Story of Aardman | United Kingdom | Documentaries | 2015 | 59 | . 200 A Love Song for Latasha | United States | Documentaries | 2020 | 20 | . 220 A Russell Peters Christmas | Canada | Stand-Up | 2011 | 44 | . 233 A StoryBots Christmas | United States | Children | 2017 | 26 | . 237 A Tale of Two Kitchens | United States | Documentaries | 2019 | 30 | . 242 A Trash Truck Christmas | NaN | Children | 2020 | 28 | . 247 A Very Murray Christmas | United States | Comedies | 2015 | 57 | . 285 Abominable Christmas | United States | Children | 2012 | 44 | . 295 Across Grace Alley | United States | Dramas | 2013 | 24 | . 305 Adam Devine: Best Time of Our Lives | United States | Stand-Up | 2019 | 59 | . 8. Marking non-feature films . Interesting! It looks as though many of the films that are under 60 minutes fall into genres such as &quot;Children&quot;, &quot;Stand-Up&quot;, and &quot;Documentaries&quot;. This is a logical result, as these types of films are probably often shorter than 90 minute Hollywood blockbuster. . We could eliminate these rows from our DataFrame and plot the values again. But another interesting way to explore the effect of these genres on our data would be to plot them, but mark them with a different color. . In Python, there are many ways to do this, but one fun way might be to use a loop to generate a list of colors based on the contents of the genre column. Much as we did in Intermediate Python, we can then pass this list to our plotting function in a later step to color all non-typical genres in a different color! . Note: Although we are using the basic colors of red, blue, green, and black, matplotlib has many named colors you can use when creating plots. For more information, you can refer to the documentation here! . colors = list() # Iterate over rows of netflix_movies_col_subset for idx, row in netflix_movies_col_subset.iterrows(): if row[&#39;genre&#39;] == &quot;Children&quot;: colors.append(&quot;red&quot;) elif row[&#39;genre&#39;] == &quot;Documentaries&quot;: colors.append(&quot;blue&quot;) elif row[&#39;genre&#39;] == &quot;Stand-Up&quot;: colors.append(&quot;green&quot;) else: colors.append(&quot;black&quot;) # Inspect the first 10 values in your list colors[:10] . [&#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;blue&#39;] . 9. Plotting with color! . Lovely looping! We now have a colors list that we can pass to our scatter plot, which should allow us to visually inspect whether these genres might be responsible for the decline in the average duration of movies. . This time, we&#39;ll also spruce up our plot with some additional axis labels and a new theme with plt.style.use(). The latter isn&#39;t taught in Intermediate Python, but can be a fun way to add some visual flair to a basic matplotlib plot. You can find more information on customizing the style of your plot here! . plt.style.use(&#39;fivethirtyeight&#39;) fig = plt.figure(figsize=(12,8)) # Create a scatter plot of duration versus release_year netflix_movies_col_subset.plot(kind=&#39;scatter&#39;, x=&#39;release_year&#39;, y=&#39;duration&#39;, c=colors) # Create a title and axis labels plt.title(&quot;Movie duration by year of release&quot;) plt.xlabel(&quot;Release year&quot;) plt.ylabel(&quot;Duration (min)&quot;) # Show the plot plt.show() . &lt;Figure size 864x576 with 0 Axes&gt; . 10. What next? . Well, as we suspected, non-typical genres such as children&#39;s movies and documentaries are all clustered around the bottom half of the plot. But we can&#39;t know for certain until we perform additional analyses. . Congratulations, you&#39;ve performed an exploratory analysis of some entertainment data, and there are lots of fun ways to develop your skills as a Pythonic data scientist. These include learning how to analyze data further with statistics, creating more advanced visualizations, and perhaps most importantly, learning more advanced ways of working with data in pandas. This latter skill is covered in our fantastic course Data Manipulation with pandas. . We hope you enjoyed this application of the skills learned in Intermediate Python, and wish you all the best on the rest of your journey! . are_movies_getting_shorter = &quot;Yes&quot; .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2021/03/31/Analyzing-Netflix-Data.html",
            "relUrl": "/datacamp/projects/python/2021/03/31/Analyzing-Netflix-Data.html",
            "date": " • Mar 31, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Disney Movies and Box Office Success",
            "content": "1. The dataset . Walt Disney Studios is the foundation on which The Walt Disney Company was built. The Studios has produced more than 600 films since their debut film, Snow White and the Seven Dwarfs in 1937. While many of its films were big hits, some of them were not. In this notebook, we will explore a dataset of Disney movies and analyze what contributes to the success of Disney movies. . . First, we will take a look at the Disney data compiled by Kelly Garrett. The data contains 579 Disney movies with six features: movie title, release date, genre, MPAA rating, total gross, and inflation-adjusted gross. . Let&#39;s load the file and see what the data looks like. . # ... YOUR CODE FOR TASK 1 ... import pandas as pd # Read the file into gross gross = pd.read_csv(&#39;datasets/disney_movies_total_gross.csv&#39;, parse_dates=[&#39;release_date&#39;]) # Print out gross gross.head() . movie_title release_date genre mpaa_rating total_gross inflation_adjusted_gross . 0 Snow White and the Seven Dwarfs | 1937-12-21 | Musical | G | 184925485 | 5228953251 | . 1 Pinocchio | 1940-02-09 | Adventure | G | 84300000 | 2188229052 | . 2 Fantasia | 1940-11-13 | Musical | G | 83320000 | 2187090808 | . 3 Song of the South | 1946-11-12 | Adventure | G | 65000000 | 1078510579 | . 4 Cinderella | 1950-02-15 | Drama | G | 85000000 | 920608730 | . 2. Top ten movies at the box office . Let&#39;s started by exploring the data. We will check which are the 10 Disney movies that have earned the most at the box office. We can do this by sorting movies by their inflation-adjusted gross (we will call it adjusted gross from this point onward). . # ... YOUR CODE FOR TASK 2 ... gross = gross.sort_values(&#39;inflation_adjusted_gross&#39;, ascending=False) # Display the top 10 movies # ... YOUR CODE FOR TASK 2 ... gross.head(10) . movie_title release_date genre mpaa_rating total_gross inflation_adjusted_gross . 0 Snow White and the Seven Dwarfs | 1937-12-21 | Musical | G | 184925485 | 5228953251 | . 1 Pinocchio | 1940-02-09 | Adventure | G | 84300000 | 2188229052 | . 2 Fantasia | 1940-11-13 | Musical | G | 83320000 | 2187090808 | . 8 101 Dalmatians | 1961-01-25 | Comedy | G | 153000000 | 1362870985 | . 6 Lady and the Tramp | 1955-06-22 | Drama | G | 93600000 | 1236035515 | . 3 Song of the South | 1946-11-12 | Adventure | G | 65000000 | 1078510579 | . 564 Star Wars Ep. VII: The Force Awakens | 2015-12-18 | Adventure | PG-13 | 936662225 | 936662225 | . 4 Cinderella | 1950-02-15 | Drama | G | 85000000 | 920608730 | . 13 The Jungle Book | 1967-10-18 | Musical | Not Rated | 141843000 | 789612346 | . 179 The Lion King | 1994-06-15 | Adventure | G | 422780140 | 761640898 | . 3. Movie genre trend . From the top 10 movies above, it seems that some genres are more popular than others. So, we will check which genres are growing stronger in popularity. To do this, we will group movies by genre and then by year to see the adjusted gross of each genre in each year. . gross[&#39;release_year&#39;] = gross[&#39;release_date&#39;].dt.year # Compute mean of adjusted gross per genre and per year group = gross.groupby([&#39;genre&#39;, &#39;release_year&#39;]).mean() # Convert the GroupBy object to a DataFrame genre_yearly = group.reset_index() # Inspect genre_yearly genre_yearly.head(10) . genre release_year total_gross inflation_adjusted_gross . 0 Action | 1981 | 0.0 | 0.0 | . 1 Action | 1982 | 26918576.0 | 77184895.0 | . 2 Action | 1988 | 17577696.0 | 36053517.0 | . 3 Action | 1990 | 59249588.5 | 118358772.0 | . 4 Action | 1991 | 28924936.5 | 57918572.5 | . 5 Action | 1992 | 29028000.0 | 58965304.0 | . 6 Action | 1993 | 21943553.5 | 44682157.0 | . 7 Action | 1994 | 19180582.0 | 39545796.0 | . 8 Action | 1995 | 63037553.5 | 122162426.5 | . 9 Action | 1996 | 135281096.0 | 257755262.5 | . 4. Visualize the genre popularity trend . We will make a plot out of these means of groups to better see how box office revenues have changed over time. . # ... YOUR CODE FOR TASK 4 ... import seaborn as sns # Plot the data # ... YOUR CODE FOR TASK 4 ... sns.relplot(kind=&#39;line&#39;, x=&#39;release_year&#39;, y=&#39;inflation_adjusted_gross&#39;, data=genre_yearly, hue=&#39;genre&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fe115d51400&gt; . 5. Data transformation . The line plot supports our belief that some genres are growing faster in popularity than others. For Disney movies, Action and Adventure genres are growing the fastest. Next, we will build a linear regression model to understand the relationship between genre and box office gross. . Since linear regression requires numerical variables and the genre variable is a categorical variable, we&#39;ll use a technique called one-hot encoding to convert the categorical variables to numerical. This technique transforms each category value into a new column and assigns a 1 or 0 to the column. . For this dataset, there will be 11 dummy variables, one for each genre except the action genre which we will use as a baseline. For example, if a movie is an adventure movie, like The Lion King, the adventure variable will be 1 and other dummy variables will be 0. Since the action genre is our baseline, if a movie is an action movie, such as The Avengers, all dummy variables will be 0. . genre_dummies = pd.get_dummies(gross[&#39;genre&#39;], drop_first=True) # Inspect genre_dummies genre_dummies.head() . Adventure Black Comedy Comedy Concert/Performance Documentary Drama Horror Musical Romantic Comedy Thriller/Suspense Western . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 1 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 8 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . 6. The genre effect . Now that we have dummy variables, we can build a linear regression model to predict the adjusted gross using these dummy variables. . From the regression model, we can check the effect of each genre by looking at its coefficient given in units of box office gross dollars. We will focus on the impact of action and adventure genres here. (Note that the intercept and the first coefficient values represent the effect of action and adventure genres respectively). We expect that movies like the Lion King or Star Wars would perform better for box office. . # ... YOUR CODE FOR TASK 6 ... from sklearn.linear_model import LinearRegression # Build a linear regression model regr = LinearRegression() # Fit regr to the dataset # ... YOUR CODE FOR TASK 6 ... regr.fit(genre_dummies, gross[&#39;inflation_adjusted_gross&#39;]) # Get estimated intercept and coefficient values action = regr.intercept_ adventure = regr.coef_[[0]][0] # Inspect the estimated intercept and coefficient values print((action, adventure)) . (102921757.36842026, 87475654.70909917) . 7. Confidence intervals for regression parameters (i) . Next, we will compute 95% confidence intervals for the intercept and coefficients. The 95% confidence intervals for the intercept a and coefficient bi means that the intervals have a probability of 95% to contain the true value a and coefficient bi respectively. If there is a significant relationship between a given genre and the adjusted gross, the confidence interval of its coefficient should exclude 0. . We will calculate the confidence intervals using the pairs bootstrap method. . import numpy as np # Create an array of indices to sample from inds = np.arange(0, len(gross[&#39;genre&#39;])) # Initialize 500 replicate arrays size = 500 bs_action_reps = np.empty(size) bs_adventure_reps = np.empty(size) . 8. Confidence intervals for regression parameters (ii) . After the initialization, we will perform pair bootstrap estimates for the regression parameters. Note that we will draw a sample from a set of (genre, adjusted gross) data where the genre is the original genre variable. We will perform one-hot encoding after that. . for i in range(size): # Resample the indices bs_inds = np.random.choice(inds, size=len(inds)) # Get the sampled genre and sampled adjusted gross bs_genre = gross[&#39;genre&#39;][bs_inds] bs_gross = gross[&#39;inflation_adjusted_gross&#39;][bs_inds] # Convert sampled genre to dummy variables bs_dummies = pd.get_dummies(bs_genre, drop_first=True) # Build and fit a regression model regr = LinearRegression().fit(bs_dummies, bs_gross) # Compute replicates of estimated intercept and coefficient bs_action_reps[i] = regr.intercept_ bs_adventure_reps[i] = regr.coef_[[0]][0] . 9. Confidence intervals for regression parameters (iii) . Finally, we compute 95% confidence intervals for the intercept and coefficient and examine if they exclude 0. If one of them (or both) does, then it is unlikely that the value is 0 and we can conclude that there is a significant relationship between that genre and the adjusted gross. . confidence_interval_action = np.percentile(bs_action_reps, [2.5, 97.5]) confidence_interval_adventure = np.percentile(bs_adventure_reps, [2.5, 97.5]) # Inspect the confidence intervals print(confidence_interval_action) print(confidence_interval_adventure) . AxisError Traceback (most recent call last) &lt;ipython-input-25-ab35250ede16&gt; in &lt;module&gt; 1 # Compute 95% confidence intervals for intercept and coefficient values -&gt; 2 confidence_interval_action = np.percentile(bs_action_reps, 0.95, axis=1) 3 confidence_interval_adventure = np.percentile(bs_adventure_reps, 0.95, axis=1) 4 5 # Inspect the confidence intervals &lt;__array_function__ internals&gt; in percentile(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims) 3731 raise ValueError(&#34;Percentiles must be in the range [0, 100]&#34;) 3732 return _quantile_unchecked( -&gt; 3733 a, q, axis, out, overwrite_input, interpolation, keepdims) 3734 3735 /usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims) 3851 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out, 3852 overwrite_input=overwrite_input, -&gt; 3853 interpolation=interpolation) 3854 if keepdims: 3855 return r.reshape(q.shape + k) /usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs) 3407 keepdim = list(a.shape) 3408 nd = a.ndim -&gt; 3409 axis = _nx.normalize_axis_tuple(axis, nd) 3410 3411 for ax in axis: /usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py in normalize_axis_tuple(axis, ndim, argname, allow_duplicate) 1356 pass 1357 # Going via an iterator directly is slower than via list comprehension. -&gt; 1358 axis = tuple([normalize_axis_index(ax, ndim, argname) for ax in axis]) 1359 if not allow_duplicate and len(set(axis)) != len(axis): 1360 if argname: /usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py in &lt;listcomp&gt;(.0) 1356 pass 1357 # Going via an iterator directly is slower than via list comprehension. -&gt; 1358 axis = tuple([normalize_axis_index(ax, ndim, argname) for ax in axis]) 1359 if not allow_duplicate and len(set(axis)) != len(axis): 1360 if argname: AxisError: axis 1 is out of bounds for array of dimension 1 . type(confidence_interval_action) . numpy.float64 . 10. Should Disney make more action and adventure movies? . The confidence intervals from the bootstrap method for the intercept and coefficient do not contain the value zero, as we have already seen that lower and upper bounds of both confidence intervals are positive. These tell us that it is likely that the adjusted gross is significantly correlated with the action and adventure genres. . From the results of the bootstrap analysis and the trend plot we have done earlier, we could say that Disney movies with plots that fit into the action and adventure genre, according to our data, tend to do better in terms of adjusted gross than other genres. So we could expect more Marvel, Star Wars, and live-action movies in the upcoming years! . more_action_adventure_movies = True .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2021/03/30/Disney-Movies-and-Box-Office-Success.html",
            "relUrl": "/datacamp/projects/python/2021/03/30/Disney-Movies-and-Box-Office-Success.html",
            "date": " • Mar 30, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Classify Song Genres from Audio Data",
            "content": "1. Preparing our dataset . These recommendations are so on point! How does this playlist know me so well? . . Over the past few years, streaming services with huge catalogs have become the primary means through which most people listen to their favorite music. But at the same time, the sheer amount of music on offer can mean users might be a bit overwhelmed when trying to look for newer music that suits their tastes. . For this reason, streaming services have looked into means of categorizing music to allow for personalized recommendations. One method involves direct analysis of the raw audio information in a given song, scoring the raw data on a variety of metrics. Today, we&#39;ll be examining data compiled by a research group known as The Echo Nest. Our goal is to look through this dataset and classify songs as being either &#39;Hip-Hop&#39; or &#39;Rock&#39; - all without listening to a single one ourselves. In doing so, we will learn how to clean our data, do some exploratory data visualization, and use feature reduction towards the goal of feeding our data through some simple machine learning algorithms, such as decision trees and logistic regression. . To begin with, let&#39;s load the metadata about our tracks alongside the track metrics compiled by The Echo Nest. A song is about more than its title, artist, and number of listens. We have another dataset that has musical features of each track such as danceability and acousticness on a scale from -1 to 1. These exist in two different files, which are in different formats - CSV and JSON. While CSV is a popular file format for denoting tabular data, JSON is another common file format in which databases often return the results of a given query. . Let&#39;s start by creating two pandas DataFrames out of these files that we can merge so we have features and labels (often also referred to as X and y) for the classification later on. . import pandas as pd # Read in track metadata with genre labels tracks = pd.read_csv(&#39;datasets/fma-rock-vs-hiphop.csv&#39;) # Read in track metrics with the features echonest_metrics = pd.read_json(&#39;datasets/echonest-metrics.json&#39;, precise_float=True) # Merge the relevant columns of tracks and echonest_metrics echo_tracks = echonest_metrics.merge(tracks[[&#39;track_id&#39;, &#39;genre_top&#39;]], on=&#39;track_id&#39;) # Inspect the resultant dataframe echo_tracks.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 4802 entries, 0 to 4801 Data columns (total 10 columns): acousticness 4802 non-null float64 danceability 4802 non-null float64 energy 4802 non-null float64 instrumentalness 4802 non-null float64 liveness 4802 non-null float64 speechiness 4802 non-null float64 tempo 4802 non-null float64 track_id 4802 non-null int64 valence 4802 non-null float64 genre_top 4802 non-null object dtypes: float64(8), int64(1), object(1) memory usage: 412.7+ KB . 2. Pairwise relationships between continuous variables . We typically want to avoid using variables that have strong correlations with each other -- hence avoiding feature redundancy -- for a few reasons: . To keep the model simple and improve interpretability (with many features, we run the risk of overfitting). | When our datasets are very large, using fewer features can drastically speed up our computation time. | . To get a sense of whether there are any strongly correlated features in our data, we will use built-in functions in the pandas package. . corr_metrics = echo_tracks.corr() corr_metrics.style.background_gradient() . acousticness danceability energy instrumentalness liveness speechiness tempo track_id valence . acousticness 1 | -0.0289537 | -0.281619 | 0.19478 | -0.0199914 | 0.072204 | -0.0263097 | -0.372282 | -0.0138406 | . danceability -0.0289537 | 1 | -0.242032 | -0.255217 | -0.106584 | 0.276206 | -0.242089 | 0.0494541 | 0.473165 | . energy -0.281619 | -0.242032 | 1 | 0.0282377 | 0.113331 | -0.109983 | 0.195227 | 0.140703 | 0.0386027 | . instrumentalness 0.19478 | -0.255217 | 0.0282377 | 1 | -0.0910218 | -0.366762 | 0.022215 | -0.275623 | -0.219967 | . liveness -0.0199914 | -0.106584 | 0.113331 | -0.0910218 | 1 | 0.0411725 | 0.00273169 | 0.0482307 | -0.0450931 | . speechiness 0.072204 | 0.276206 | -0.109983 | -0.366762 | 0.0411725 | 1 | 0.00824055 | -0.0269951 | 0.149894 | . tempo -0.0263097 | -0.242089 | 0.195227 | 0.022215 | 0.00273169 | 0.00824055 | 1 | -0.0253918 | 0.0522212 | . track_id -0.372282 | 0.0494541 | 0.140703 | -0.275623 | 0.0482307 | -0.0269951 | -0.0253918 | 1 | 0.0100698 | . valence -0.0138406 | 0.473165 | 0.0386027 | -0.219967 | -0.0450931 | 0.149894 | 0.0522212 | 0.0100698 | 1 | . 3. Normalizing the feature data . As mentioned earlier, it can be particularly useful to simplify our models and use as few features as necessary to achieve the best result. Since we didn&#39;t find any particular strong correlations between our features, we can instead use a common approach to reduce the number of features called principal component analysis (PCA). . It is possible that the variance between genres can be explained by just a few features in the dataset. PCA rotates the data along the axis of highest variance, thus allowing us to determine the relative contribution of each feature of our data towards the variance between classes. . However, since PCA uses the absolute variance of a feature to rotate the data, a feature with a broader range of values will overpower and bias the algorithm relative to the other features. To avoid this, we must first normalize our data. There are a few methods to do this, but a common way is through standardization, such that all features have a mean = 0 and standard deviation = 1 (the resultant is a z-score). . features = echo_tracks.drop([&#39;genre_top&#39;, &#39;track_id&#39;], axis=1) # Define our labels labels = echo_tracks[&#39;genre_top&#39;] # Import the StandardScaler from sklearn.preprocessing import StandardScaler # Scale the features and set the values to a new variable scaler = StandardScaler() scaled_train_features = scaler.fit_transform(features) . 4. Principal Component Analysis on our scaled data . Now that we have preprocessed our data, we are ready to use PCA to determine by how much we can reduce the dimensionality of our data. We can use scree-plots and cumulative explained ratio plots to find the number of components to use in further analyses. . Scree-plots display the number of components against the variance explained by each component, sorted in descending order of variance. Scree-plots help us get a better sense of which components explain a sufficient amount of variance in our data. When using scree plots, an &#39;elbow&#39; (a steep drop from one data point to the next) in the plot is typically used to decide on an appropriate cutoff. . %matplotlib inline # Import our plotting module, and PCA class #... YOUR CODE ... import matplotlib.pyplot as plt from sklearn.decomposition import PCA # Get our explained variance ratios from PCA using all features pca = PCA() pca.fit(scaled_train_features) exp_variance = pca.explained_variance_ratio_ # plot the explained variance using a barplot fig, ax = plt.subplots() ax.bar(range(features.shape[1]), exp_variance) ax.set_xlabel(&#39;Principal Component #&#39;) . Text(0.5,0,&#39;Principal Component #&#39;) . 5. Further visualization of PCA . Unfortunately, there does not appear to be a clear elbow in this scree plot, which means it is not straightforward to find the number of intrinsic dimensions using this method. . But all is not lost! Instead, we can also look at the cumulative explained variance plot to determine how many features are required to explain, say, about 85% of the variance (cutoffs are somewhat arbitrary here, and usually decided upon by &#39;rules of thumb&#39;). Once we determine the appropriate number of components, we can perform PCA with that many components, ideally reducing the dimensionality of our data. . import numpy as np # Calculate the cumulative explained variance cum_exp_variance = np.cumsum(pca.explained_variance_ratio_) # Plot the cumulative explained variance and draw a dashed line at 0.85. fig, ax = plt.subplots() ax.plot(range(features.shape[1]), cum_exp_variance) ax.axhline(y=0.85, linestyle=&#39;--&#39;) # choose the n_components where about 85% of our variance can be explained n_components = (cum_exp_variance &lt; 0.85).sum() + 1 # Perform PCA with the chosen number of components and project data onto components pca = PCA(n_components, random_state=10) pca.fit(scaled_train_features) pca_projection = pca.transform(scaled_train_features) . 6. Train a decision tree to classify genre . Now we can use the lower dimensional PCA projection of the data to classify songs into genres. To do that, we first need to split our dataset into &#39;train&#39; and &#39;test&#39; subsets, where the &#39;train&#39; subset will be used to train our model while the &#39;test&#39; dataset allows for model performance validation. . Here, we will be using a simple algorithm known as a decision tree. Decision trees are rule-based classifiers that take in features and follow a &#39;tree structure&#39; of binary decisions to ultimately classify a data point into one of two or more categories. In addition to being easy to both use and interpret, decision trees allow us to visualize the &#39;logic flowchart&#39; that the model generates from the training data. . Here is an example of a decision tree that demonstrates the process by which an input image (in this case, of a shape) might be classified based on the number of sides it has and whether it is rotated. . . # ... YOUR CODE ... from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier # Split our data train_features, test_features, train_labels, test_labels = train_test_split(pca_projection, labels, random_state=10) # Train our decision tree tree = DecisionTreeClassifier(random_state=10) tree.fit(train_features, train_labels) # Predict the labels for the test data pred_labels_tree = tree.predict(test_features) . 7. Compare our decision tree to a logistic regression . Although our tree&#39;s performance is decent, it&#39;s a bad idea to immediately assume that it&#39;s therefore the perfect tool for this job -- there&#39;s always the possibility of other models that will perform even better! It&#39;s always a worthwhile idea to at least test a few other algorithms and find the one that&#39;s best for our data. . Sometimes simplest is best, and so we will start by applying logistic regression. Logistic regression makes use of what&#39;s called the logistic function to calculate the odds that a given data point belongs to a given class. Once we have both models, we can compare them on a few performance metrics, such as false positive and false negative rate (or how many points are inaccurately classified). . from sklearn.linear_model import LogisticRegression # Train our logistic regression and predict labels for the test set logreg = LogisticRegression(random_state=10) logreg.fit(train_features, train_labels) pred_labels_logit = logreg.predict(test_features) # Create the classification report for both models from sklearn.metrics import classification_report class_rep_tree = classification_report(test_labels, pred_labels_tree) class_rep_log = classification_report(test_labels, pred_labels_logit) print(&quot;Decision Tree: n&quot;, class_rep_tree) print(&quot;Logistic Regression: n&quot;, class_rep_log) . Decision Tree: precision recall f1-score support Hip-Hop 0.66 0.66 0.66 229 Rock 0.92 0.92 0.92 972 avg / total 0.87 0.87 0.87 1201 Logistic Regression: precision recall f1-score support Hip-Hop 0.75 0.57 0.65 229 Rock 0.90 0.95 0.93 972 avg / total 0.87 0.88 0.87 1201 . 8. Balance our data for greater performance . Both our models do similarly well, boasting an average precision of 87% each. However, looking at our classification report, we can see that rock songs are fairly well classified, but hip-hop songs are disproportionately misclassified as rock songs. . Why might this be the case? Well, just by looking at the number of data points we have for each class, we see that we have far more data points for the rock classification than for hip-hop, potentially skewing our model&#39;s ability to distinguish between classes. This also tells us that most of our model&#39;s accuracy is driven by its ability to classify just rock songs, which is less than ideal. . To account for this, we can weight the value of a correct classification in each class inversely to the occurrence of data points for each class. Since a correct classification for &quot;Rock&quot; is not more important than a correct classification for &quot;Hip-Hop&quot; (and vice versa), we only need to account for differences in sample size of our data points when weighting our classes here, and not relative importance of each class. . hop_only = echo_tracks.loc[echo_tracks[&#39;genre_top&#39;] == &#39;Hip-Hop&#39;] rock_only = echo_tracks.loc[echo_tracks[&#39;genre_top&#39;] == &#39;Rock&#39;] # sample the rocks songs to be the same number as there are hip-hop songs rock_only = rock_only.sample(len(hop_only), random_state=10) # concatenate the dataframes rock_only and hop_only rock_hop_bal = pd.concat([rock_only, hop_only]) # The features, labels, and pca projection are created for the balanced dataframe features = rock_hop_bal.drop([&#39;genre_top&#39;, &#39;track_id&#39;], axis=1) labels = rock_hop_bal[&#39;genre_top&#39;] pca_projection = pca.fit_transform(scaler.fit_transform(features)) # Redefine the train and test set with the pca_projection from the balanced data train_features, test_features, train_labels, test_labels = train_test_split(pca_projection, labels, random_state=10) . 9. Does balancing our dataset improve model bias? . We&#39;ve now balanced our dataset, but in doing so, we&#39;ve removed a lot of data points that might have been crucial to training our models. Let&#39;s test to see if balancing our data improves model bias towards the &quot;Rock&quot; classification while retaining overall classification performance. . Note that we have already reduced the size of our dataset and will go forward without applying any dimensionality reduction. In practice, we would consider dimensionality reduction more rigorously when dealing with vastly large datasets and when computation times become prohibitively large. . tree = DecisionTreeClassifier(random_state=10) tree.fit(train_features, train_labels) pred_labels_tree = tree.predict(test_features) # Train our logistic regression on the balanced data logreg = LogisticRegression(random_state=10) logreg.fit(train_features, train_labels) pred_labels_logit = logreg.predict(test_features) # Compare the models print(&quot;Decision Tree: n&quot;, classification_report(test_labels, pred_labels_tree)) print(&quot;Logistic Regression: n&quot;, classification_report(test_labels, pred_labels_logit)) . Decision Tree: precision recall f1-score support Hip-Hop 0.77 0.77 0.77 230 Rock 0.76 0.76 0.76 225 avg / total 0.76 0.76 0.76 455 Logistic Regression: precision recall f1-score support Hip-Hop 0.82 0.83 0.82 230 Rock 0.82 0.81 0.82 225 avg / total 0.82 0.82 0.82 455 . 10. Using cross-validation to evaluate our models . Success! Balancing our data has removed bias towards the more prevalent class. To get a good sense of how well our models are actually performing, we can apply what&#39;s called cross-validation (CV). This step allows us to compare models in a more rigorous fashion. . Since the way our data is split into train and test sets can impact model performance, CV attempts to split the data multiple ways and test the model on each of the splits. Although there are many different CV methods, all with their own advantages and disadvantages, we will use what&#39;s known as K-fold CV here. K-fold first splits the data into K different, equally sized subsets. Then, it iteratively uses each subset as a test set while using the remainder of the data as train sets. Finally, we can then aggregate the results from each fold for a final model performance score. . from sklearn.model_selection import KFold, cross_val_score # Set up our K-fold cross-validation kf = KFold(n_splits=10) tree = DecisionTreeClassifier() logreg = LogisticRegression() # Train our models using KFold cv tree_score = cross_val_score(tree, pca_projection, labels) logit_score = cross_val_score(logreg, pca_projection, labels) # Print the mean of each array of scores print(&quot;Decision Tree:&quot;, np.mean(tree_score), &quot;Logistic Regression:&quot;, np.mean(logit_score)) . Decision Tree: 0.7538720977360894 Logistic Regression: 0.8076960656591975 .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2021/03/30/Classify-Song-Genres-from-Audio-Data.html",
            "relUrl": "/datacamp/projects/python/2021/03/30/Classify-Song-Genres-from-Audio-Data.html",
            "date": " • Mar 30, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "ASL Recognition with Deep Learning",
            "content": "1. American Sign Language (ASL) . American Sign Language (ASL) is the primary language used by many deaf individuals in North America, and it is also used by hard-of-hearing and hearing individuals. The language is as rich as spoken languages and employs signs made with the hand, along with facial gestures and bodily postures. . . A lot of recent progress has been made towards developing computer vision systems that translate sign language to spoken language. This technology often relies on complex neural network architectures that can detect subtle patterns in streaming video. However, as a first step, towards understanding how to build a translation system, we can reduce the size of the problem by translating individual letters, instead of sentences. . In this notebook, we will train a convolutional neural network to classify images of American Sign Language (ASL) letters. After loading, examining, and preprocessing the data, we will train the network and test its performance. . In the code cell below, we load the training and test data. . x_train and x_test are arrays of image data with shape (num_samples, 3, 50, 50), corresponding to the training and test datasets, respectively. | y_train and y_test are arrays of category labels with shape (num_samples,), corresponding to the training and test datasets, respectively. | . import numpy as np np.random.seed(5) import tensorflow as tf tf.set_random_seed(2) from datasets import sign_language import matplotlib.pyplot as plt %matplotlib inline # Load pre-shuffled training and test datasets (x_train, y_train), (x_test, y_test) = sign_language.load_data() . Using TensorFlow backend. . 2. Visualize the training data . Now we&#39;ll begin by creating a list of string-valued labels containing the letters that appear in the dataset. Then, we visualize the first several images in the training data, along with their corresponding labels. . labels = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;] # Print the first several training images, along with the labels fig = plt.figure(figsize=(20,5)) for i in range(36): ax = fig.add_subplot(3, 12, i + 1, xticks=[], yticks=[]) ax.imshow(np.squeeze(x_train[i])) ax.set_title(&quot;{}&quot;.format(labels[y_train[i]])) plt.show() . 3. Examine the dataset . Let&#39;s examine how many images of each letter can be found in the dataset. . Remember that dataset has already been split into training and test sets for you, where x_train and x_test contain the images, and y_train and y_test contain their corresponding labels. . Each entry in y_train and y_test is one of 0, 1, or 2, corresponding to the letters &#39;A&#39;, &#39;B&#39;, and &#39;C&#39;, respectively. . We will use the arrays y_train and y_test to verify that both the training and test sets each have roughly equal proportions of each letter. . num_A_train = sum(y_train==0) # Number of B&#39;s in the training dataset num_B_train = sum(y_train==1) # Number of C&#39;s in the training dataset num_C_train = sum(y_train==2) # Number of A&#39;s in the test dataset num_A_test = sum(y_test==0) # Number of B&#39;s in the test dataset num_B_test = sum(y_test==1) # Number of C&#39;s in the test dataset num_C_test = sum(y_test==2) # Print statistics about the dataset print(&quot;Training set:&quot;) print(&quot; tA: {}, B: {}, C: {}&quot;.format(num_A_train, num_B_train, num_C_train)) print(&quot;Test set:&quot;) print(&quot; tA: {}, B: {}, C: {}&quot;.format(num_A_test, num_B_test, num_C_test)) . Training set: A: 540, B: 528, C: 532 Test set: A: 118, B: 144, C: 138 . 4. One-hot encode the data . Currently, our labels for each of the letters are encoded as categorical integers, where &#39;A&#39;, &#39;B&#39; and &#39;C&#39; are encoded as 0, 1, and 2, respectively. However, recall that Keras models do not accept labels in this format, and we must first one-hot encode the labels before supplying them to a Keras model. . This conversion will turn the one-dimensional array of labels into a two-dimensional array. . . Each row in the two-dimensional array of one-hot encoded labels corresponds to a different image. The row has a 1 in the column that corresponds to the correct label, and 0 elsewhere. . For instance, . 0 is encoded as [1, 0, 0], | 1 is encoded as [0, 1, 0], and | 2 is encoded as [0, 0, 1]. | . from keras.utils import np_utils # One-hot encode the training labels y_train_OH = np_utils.to_categorical(y_train) # One-hot encode the test labels y_test_OH = np_utils.to_categorical(y_test) . 5. Define the model . Now it&#39;s time to define a convolutional neural network to classify the data. . This network accepts an image of an American Sign Language letter as input. The output layer returns the network&#39;s predicted probabilities that the image belongs in each category. . from keras.layers import Conv2D, MaxPooling2D from keras.layers import Flatten, Dense from keras.models import Sequential model = Sequential() # First convolutional layer accepts image input model.add(Conv2D(filters=5, kernel_size=5, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(50, 50, 3))) # Add a max pooling layer model.add(MaxPooling2D((4, 4))) # Add a convolutional layer model.add(Conv2D(filters=15, kernel_size=5, padding=&#39;same&#39;, activation=&#39;relu&#39;)) # Add another max pooling layer model.add(MaxPooling2D((4, 4))) # Flatten and feed to output layer model.add(Flatten()) model.add(Dense(3, activation=&#39;softmax&#39;)) # Summarize the model model.summary() . _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 50, 50, 5) 380 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 12, 12, 5) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 12, 12, 15) 1890 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 3, 3, 15) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 135) 0 _________________________________________________________________ dense_1 (Dense) (None, 3) 408 ================================================================= Total params: 2,678 Trainable params: 2,678 Non-trainable params: 0 _________________________________________________________________ . 6. Compile the model . After we have defined a neural network in Keras, the next step is to compile it! . model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . 7. Train the model . Once we have compiled the model, we&#39;re ready to fit it to the training data. . hist = model.fit(x=x_train, y=y_train_OH, epochs=2, validation_split=0.2, batch_size=32) . Train on 1280 samples, validate on 320 samples Epoch 1/2 1280/1280 [==============================] - 2s 2ms/step - loss: 0.9560 - acc: 0.6164 - val_loss: 0.7627 - val_acc: 0.8500 Epoch 2/2 1280/1280 [==============================] - 2s 2ms/step - loss: 0.6086 - acc: 0.8875 - val_loss: 0.4636 - val_acc: 0.9187 . 8. Test the model . To evaluate the model, we&#39;ll use the test dataset. This will tell us how the network performs when classifying images it has never seen before! . If the classification accuracy on the test dataset is similar to the training dataset, this is a good sign that the model did not overfit to the training data. . score = model.evaluate(x=x_test, y=y_test_OH, verbose=0) print(&#39;Test accuracy:&#39;, score[1]) . Test accuracy: 0.94 . 9. Visualize mistakes . Hooray! Our network gets very high accuracy on the test set! . The final step is to take a look at the images that were incorrectly classified by the model. Do any of the mislabeled images look relatively difficult to classify, even to the human eye? . Sometimes, it&#39;s possible to review the images to discover special characteristics that are confusing to the model. However, it is also often the case that it&#39;s hard to interpret what the model had in mind! . y_probs = model.predict(x_test) # Get predicted labels for test dataset y_preds = np.argmax(y_probs, axis=1) # Indices corresponding to test images which were mislabeled bad_test_idxs = np.arange(len(y_test))[y_test != y_preds] # Print mislabeled examples fig = plt.figure(figsize=(25,4)) for i, idx in enumerate(bad_test_idxs): ax = fig.add_subplot(2, np.ceil(len(bad_test_idxs)/2), i + 1, xticks=[], yticks=[]) ax.imshow(np.squeeze(x_test[idx])) ax.set_title(&quot;{} (pred: {})&quot;.format(labels[y_test[idx]], labels[y_preds[idx]])) .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2021/03/30/ASL-Recognition-with-Deep-Learning.html",
            "relUrl": "/datacamp/projects/python/2021/03/30/ASL-Recognition-with-Deep-Learning.html",
            "date": " • Mar 30, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Comparing Search Interest with Google Trends",
            "content": "1. The sisters and Google Trends . While I&#39;m not a fan nor a hater of the Kardashians and Jenners, the polarizing family intrigues me. Why? Their marketing prowess. Say what you will about them and what they stand for, they are great at the hype game. Everything they touch turns to content. . The sisters in particular over the past decade have been especially productive in this regard. Let&#39;s get some facts straight. I consider the &quot;sisters&quot; to be the following daughters of Kris Jenner. Three from her first marriage to lawyer Robert Kardashian: . Kourtney Kardashian (daughter of Robert Kardashian, born in 1979) | Kim Kardashian (daughter of Robert Kardashian, born in 1980) | Khloé Kardashian (daughter of Robert Kardashian, born in 1984) | . And two from her second marriage to Olympic gold medal-winning decathlete, Caitlyn Jenner (formerly Bruce): . Kendall Jenner (daughter of Caitlyn Jenner, born in 1995) | Kylie Jenner (daughter of Caitlyn Jenner, born in 1997) | . . This family tree can be confusing, but we aren&#39;t here to explain it. We&#39;re here to explore the data underneath the hype, and we&#39;ll do it using search interest data from Google Trends. We&#39;ll recreate the Google Trends plot to visualize their ups and downs over time, then make a few custom plots of our own. And we&#39;ll answer the big question: is Kim even the most famous sister anymore? . First, let&#39;s load and inspect our Google Trends data, which was downloaded in CSV form. The query parameters: each of the sisters, worldwide search data, 2007 to present day. (2007 was the year Kim became &quot;active&quot; according to Wikipedia.) . # ... YOUR CODE FOR TASK 1 ... import pandas as pd # Read in dataset trends = pd.read_csv(&#39;datasets/trends_kj_sisters.csv&#39;) # Inspect data # ... YOUR CODE FOR TASK 1 ... trends.head() . Month Kim Kardashian: (Worldwide) Khloé Kardashian: (Worldwide) Kourtney Kardashian: (Worldwide) Kendall Jenner: (Worldwide) Kylie Jenner: (Worldwide) . 0 2007-01 | 2 | &lt;1 | &lt;1 | &lt;1 | 2 | . 1 2007-02 | 12 | &lt;1 | &lt;1 | &lt;1 | 2 | . 2 2007-03 | 9 | &lt;1 | &lt;1 | &lt;1 | 1 | . 3 2007-04 | 6 | &lt;1 | &lt;1 | &lt;1 | 1 | . 4 2007-05 | 6 | &lt;1 | &lt;1 | &lt;1 | 2 | . 2. Better &quot;kolumn&quot; names . So we have a column for each month since January 2007 and a column for the worldwide search interest for each of the sisters each month. By the way, Google defines the values of search interest as: . Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term. . Okay, that&#39;s great Google, but you are not making this data easily analyzable for us. I see a few things. Let&#39;s do the column names first. A column named &quot;Kim Kardashian: (Worldwide)&quot; is not the most usable for coding purposes. Let&#39;s shorten those so we can access their values better. Might as well standardize all column formats, too. I like lowercase, short column names. . # ... YOUR CODE FOR TASK 2 ... trends.columns = [&#39;month&#39;, &#39;kim&#39;, &#39;khloe&#39;, &#39;kourtney&#39;, &#39;kendall&#39;, &#39;kylie&#39;] # Inspect data # ... YOUR CODE FOR TASK 2 ... trends.head() . month kim khloe kourtney kendall kylie . 0 2007-01 | 2 | &lt;1 | &lt;1 | &lt;1 | 2 | . 1 2007-02 | 12 | &lt;1 | &lt;1 | &lt;1 | 2 | . 2 2007-03 | 9 | &lt;1 | &lt;1 | &lt;1 | 1 | . 3 2007-04 | 6 | &lt;1 | &lt;1 | &lt;1 | 1 | . 4 2007-05 | 6 | &lt;1 | &lt;1 | &lt;1 | 2 | . 3. Pesky data types . That&#39;s better. We don&#39;t need to scroll our eyes across the table to read the values anymore since it is much less wide. And seeing five columns that all start with the letter &quot;k&quot; … the aesthetics … we should call them &quot;kolumns&quot; now! (Bad joke.) . The next thing I see that is going to be an issue is that &quot;&lt;&quot; sign. If &quot;a score of 0 means there was not enough data for this term,&quot; &quot;&lt;1&quot; must mean it is between 0 and 1 and Google does not want to give us the fraction from google.trends.com for whatever reason. That&#39;s fine, but this &quot;&lt;&quot; sign means we won&#39;t be able to analyze or visualize our data right away because those column values aren&#39;t going to be represented as numbers in our data structure. Let&#39;s confirm that by inspecting our data types. . # ... YOUR CODE FOR TASK 3 ... trends.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 147 entries, 0 to 146 Data columns (total 6 columns): month 147 non-null object kim 147 non-null int64 khloe 147 non-null object kourtney 147 non-null object kendall 147 non-null object kylie 147 non-null int64 dtypes: int64(2), object(4) memory usage: 7.0+ KB . 4. From object to integer . Yes, okay, the khloe, kourtney, and kendall columns aren&#39;t integers like the kim and kylie columns are. Again, because of the &quot;&lt;&quot; sign that indicates a search interest value between zero and one. Is this an early hint at the hierarchy of sister popularity? We&#39;ll see shortly. Before that, we&#39;ll need to remove that pesky &quot;&lt;&quot; sign. Then we can change the type of those columns to integer. . # ... YOUR CODE FOR TASK 4 ... for col in trends.columns: # Only modify columns that have the &quot;&lt;&quot; sign if &quot;&lt;&quot; in trends[col].to_string(): # ... YOUR CODE FOR TASK 4 ... # Remove &quot;&lt;&quot; and convert dtype to integer # ... YOUR CODE FOR TASK 4 ... # ... YOUR CODE FOR TASK 4 ... trends[col] = pd.to_numeric(trends[col].str.replace(&quot;&lt;&quot;, &quot;&quot;)) # Inspect data types and data trends.info() trends.head() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 147 entries, 0 to 146 Data columns (total 6 columns): month 147 non-null object kim 147 non-null int64 khloe 147 non-null int64 kourtney 147 non-null int64 kendall 147 non-null int64 kylie 147 non-null int64 dtypes: int64(5), object(1) memory usage: 7.0+ KB . month kim khloe kourtney kendall kylie . 0 2007-01 | 2 | 1 | 1 | 1 | 2 | . 1 2007-02 | 12 | 1 | 1 | 1 | 2 | . 2 2007-03 | 9 | 1 | 1 | 1 | 1 | . 3 2007-04 | 6 | 1 | 1 | 1 | 1 | . 4 2007-05 | 6 | 1 | 1 | 1 | 2 | . 5. From object to datetime . Okay, great, no more &quot;&lt;&quot; signs. All the sister columns are of integer type. . Now let&#39;s convert our month column from type object to datetime to make our date data more accessible. . # ... YOUR CODE FOR TASK 5 ... trends[&#39;month&#39;] = pd.to_datetime(trends[&#39;month&#39;]) # Inspect data types and data # ... YOUR CODE FOR TASK 5 ... trends.info() # ... YOUR CODE FOR TASK 5 ... trends.head() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 147 entries, 0 to 146 Data columns (total 6 columns): month 147 non-null datetime64[ns] kim 147 non-null int64 khloe 147 non-null int64 kourtney 147 non-null int64 kendall 147 non-null int64 kylie 147 non-null int64 dtypes: datetime64[ns](1), int64(5) memory usage: 7.0 KB . month kim khloe kourtney kendall kylie . 0 2007-01-01 | 2 | 1 | 1 | 1 | 2 | . 1 2007-02-01 | 12 | 1 | 1 | 1 | 2 | . 2 2007-03-01 | 9 | 1 | 1 | 1 | 1 | . 3 2007-04-01 | 6 | 1 | 1 | 1 | 1 | . 4 2007-05-01 | 6 | 1 | 1 | 1 | 2 | . 6. Set month as index . And finally, let&#39;s set the month column as our index to wrap our data cleaning. Having month as index rather than the zero-based row numbers will allow us to write shorter lines of code to create plots, where month will represent our x-axis. . trends = trends.set_index(&#39;month&#39;, drop=True) # Inspect the data trends.head() . kim khloe kourtney kendall kylie . month . 2007-01-01 2 | 1 | 1 | 1 | 2 | . 2007-02-01 12 | 1 | 1 | 1 | 2 | . 2007-03-01 9 | 1 | 1 | 1 | 1 | . 2007-04-01 6 | 1 | 1 | 1 | 1 | . 2007-05-01 6 | 1 | 1 | 1 | 2 | . 7. The early Kim hype . Okay! So our data is ready to plot. Because we cleaned our data, we only need one line of code (and just thirteen characters!) to remake the Google Trends chart, plus another line to make the plot show up in our notebook. . # ... YOUR CODE FOR TASK 7 ... %matplotlib inline # ... YOUR CODE FOR TASK 7 ... trends.plot(y=&#39;kim&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f26bd6cb0f0&gt; . 8. Kylie&#39;s rise . Oh my! There is so much to make sense of here. Kim&#39;s sharp rise in 2007, with the beginning of Keeping Up with the Kardashians, among other things. There was no significant search interest for the other four sisters until mid-2009 when Kourtney and Khloé launched the reality television series, Kourtney and Khloé Take Miami. Then there was Kim&#39;s rise from famous to literally more famous than God in 2011. This Cosmopolitan article covers the timeline that includes the launch of music videos, fragrances, iPhone and Android games, another television series, joining Instagram, and more. Then there was Kim&#39;s ridiculous spike in December 2014: posing naked on the cover of Paper Magazine in a bid to break the internet will do that for you. . A curious thing starts to happen after that bid as well. Let&#39;s zoom in… . # ... YOUR CODE FOR TASK 8 ... trends.loc[&#39;01-01-2014&#39;:&#39;01-03-2019&#39;, :].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f26bb512668&gt; . 9. Smooth out the fluctuations with rolling means . It looks like my suspicion may be true: Kim is not always the most searched Kardashian or Jenner sister. Since late-2016, at various months, Kylie overtakes Kim. Two big spikes where she smashed Kim&#39;s search interest: in September 2017 when it was reported that Kylie was expecting her first child with rapper Travis Scott and in February 2018 when she gave birth to her daughter, Stormi Webster. The continued success of Kylie Cosmetics has kept her in the news, not to mention making her the &quot;The Youngest Self-Made Billionaire Ever&quot; according to Forbes. . These fluctuations are descriptive but do not really help us answer our question: is Kim even the most famous sister anymore? We can use rolling means to smooth out short-term fluctuations in time series data and highlight long-term trends. Let&#39;s make the window twelve months a.k.a. one year. . # ... YOUR CODE FOR TASK 9 ... trends.rolling(window=12).mean().plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f26bb342a90&gt; . 10. Who&#39;s more famous? The Kardashians or the Jenners? . Whoa, okay! So by this metric, Kim is still the most famous sister despite Kylie being close and nearly taking her crown. Honestly, the biggest takeaway from this whole exercise might be Kendall not showing up that much. It makes sense, though, despite her wildly successful modeling career. Some have called her &quot;the only normal one in her family&quot; as she tends to shy away from the more dramatic and controversial parts of the media limelight that generate oh so many clicks. . Let&#39;s end this analysis with one last plot. In it, we will plot (pun!) the Kardashian sisters against the Jenner sisters to see which family line is more popular now. We will use average search interest to make things fair, i.e., total search interest divided by the number of sisters in the family line. . The answer? Since 2015, it has been a toss-up. And in the future? With this family and their penchant for big events, who knows? . # ... YOUR CODE FOR TASK 10 ... trends[&#39;kardashian&#39;] = (trends[&#39;kim&#39;] + trends[&#39;khloe&#39;] + trends[&#39;kourtney&#39;]) / 3 # ... YOUR CODE FOR TASK 10 ... trends[&#39;jenner&#39;] = (trends[&#39;kendall&#39;] + trends[&#39;kylie&#39;]) / 2 # Plot average family line search interest vs. month # ... YOUR CODE FOR TASK 10 ... trends[[&#39;kardashian&#39;, &#39;jenner&#39;]].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f26bb3f5438&gt; .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2021/03/29/Comparing-Search-Interest-with-Google-Trends.html",
            "relUrl": "/datacamp/projects/python/2021/03/29/Comparing-Search-Interest-with-Google-Trends.html",
            "date": " • Mar 29, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "A Visual History of Nobel Prize Winners",
            "content": "1. The most Nobel of Prizes . The Nobel Prize is perhaps the world&#39;s most well known scientific award. Except for the honor, prestige and substantial prize money the recipient also gets a gold medal showing Alfred Nobel (1833 - 1896) who established the prize. Every year it&#39;s given to scientists and scholars in the categories chemistry, literature, physics, physiology or medicine, economics, and peace. The first Nobel Prize was handed out in 1901, and at that time the Prize was very Eurocentric and male-focused, but nowadays it&#39;s not biased in any way whatsoever. Surely. Right? . Well, we&#39;re going to find out! The Nobel Foundation has made a dataset available of all prize winners from the start of the prize, in 1901, to 2016. Let&#39;s load it in and take a look. . # ... YOUR CODE FOR TASK 1 ... import numpy as np import pandas as pd import seaborn as sns # Reading in the Nobel Prize data nobel = pd.read_csv(&#39;datasets/nobel.csv&#39;) # Taking a look at the first several winners # ... YOUR CODE FOR TASK 1 ... nobel.head(6) . year category prize motivation prize_share laureate_id laureate_type full_name birth_date birth_city birth_country sex organization_name organization_city organization_country death_date death_city death_country . 0 1901 | Chemistry | The Nobel Prize in Chemistry 1901 | &quot;in recognition of the extraordinary services ... | 1/1 | 160 | Individual | Jacobus Henricus van &#39;t Hoff | 1852-08-30 | Rotterdam | Netherlands | Male | Berlin University | Berlin | Germany | 1911-03-01 | Berlin | Germany | . 1 1901 | Literature | The Nobel Prize in Literature 1901 | &quot;in special recognition of his poetic composit... | 1/1 | 569 | Individual | Sully Prudhomme | 1839-03-16 | Paris | France | Male | NaN | NaN | NaN | 1907-09-07 | Châtenay | France | . 2 1901 | Medicine | The Nobel Prize in Physiology or Medicine 1901 | &quot;for his work on serum therapy, especially its... | 1/1 | 293 | Individual | Emil Adolf von Behring | 1854-03-15 | Hansdorf (Lawice) | Prussia (Poland) | Male | Marburg University | Marburg | Germany | 1917-03-31 | Marburg | Germany | . 3 1901 | Peace | The Nobel Peace Prize 1901 | NaN | 1/2 | 462 | Individual | Jean Henry Dunant | 1828-05-08 | Geneva | Switzerland | Male | NaN | NaN | NaN | 1910-10-30 | Heiden | Switzerland | . 4 1901 | Peace | The Nobel Peace Prize 1901 | NaN | 1/2 | 463 | Individual | Frédéric Passy | 1822-05-20 | Paris | France | Male | NaN | NaN | NaN | 1912-06-12 | Paris | France | . 5 1901 | Physics | The Nobel Prize in Physics 1901 | &quot;in recognition of the extraordinary services ... | 1/1 | 1 | Individual | Wilhelm Conrad Röntgen | 1845-03-27 | Lennep (Remscheid) | Prussia (Germany) | Male | Munich University | Munich | Germany | 1923-02-10 | Munich | Germany | . 2. So, who gets the Nobel Prize? . Just looking at the first couple of prize winners, or Nobel laureates as they are also called, we already see a celebrity: Wilhelm Conrad Röntgen, the guy who discovered X-rays. And actually, we see that all of the winners in 1901 were guys that came from Europe. But that was back in 1901, looking at all winners in the dataset, from 1901 to 2016, which sex and which country is the most commonly represented? . (For country, we will use the birth_country of the winner, as the organization_country is NaN for all shared Nobel Prizes.) . # out between 1901 and 2016 # ... YOUR CODE FOR TASK 2 ... display(nobel[nobel[&#39;prize_share&#39;] != &#39;1/1&#39;]) # Display the number of prizes won by male and female recipients. # ... YOUR CODE FOR TASK 2 ... display(nobel[&#39;sex&#39;].value_counts()) # Display the number of prizes won by the top 10 nationalities. # ... YOUR CODE FOR TASK 2 ... nobel[&#39;birth_country&#39;].value_counts().head(10) . year category prize motivation prize_share laureate_id laureate_type full_name birth_date birth_city birth_country sex organization_name organization_city organization_country death_date death_city death_country . 3 1901 | Peace | The Nobel Peace Prize 1901 | NaN | 1/2 | 462 | Individual | Jean Henry Dunant | 1828-05-08 | Geneva | Switzerland | Male | NaN | NaN | NaN | 1910-10-30 | Heiden | Switzerland | . 4 1901 | Peace | The Nobel Peace Prize 1901 | NaN | 1/2 | 463 | Individual | Frédéric Passy | 1822-05-20 | Paris | France | Male | NaN | NaN | NaN | 1912-06-12 | Paris | France | . 9 1902 | Peace | The Nobel Peace Prize 1902 | NaN | 1/2 | 464 | Individual | Élie Ducommun | 1833-02-19 | Geneva | Switzerland | Male | NaN | NaN | NaN | 1906-12-07 | Bern | Switzerland | . 10 1902 | Peace | The Nobel Peace Prize 1902 | NaN | 1/2 | 465 | Individual | Charles Albert Gobat | 1843-05-21 | Tramelan | Switzerland | Male | NaN | NaN | NaN | 1914-03-16 | Bern | Switzerland | . 11 1902 | Physics | The Nobel Prize in Physics 1902 | &quot;in recognition of the extraordinary service t... | 1/2 | 2 | Individual | Hendrik Antoon Lorentz | 1853-07-18 | Arnhem | Netherlands | Male | Leiden University | Leiden | Netherlands | 1928-02-04 | NaN | Netherlands | . 12 1902 | Physics | The Nobel Prize in Physics 1902 | &quot;in recognition of the extraordinary service t... | 1/2 | 3 | Individual | Pieter Zeeman | 1865-05-25 | Zonnemaire | Netherlands | Male | Amsterdam University | Amsterdam | Netherlands | 1943-10-09 | Amsterdam | Netherlands | . 17 1903 | Physics | The Nobel Prize in Physics 1903 | &quot;in recognition of the extraordinary services ... | 1/2 | 4 | Individual | Antoine Henri Becquerel | 1852-12-15 | Paris | France | Male | École Polytechnique | Paris | France | 1908-08-25 | NaN | France | . 18 1903 | Physics | The Nobel Prize in Physics 1903 | &quot;in recognition of the extraordinary services ... | 1/4 | 5 | Individual | Pierre Curie | 1859-05-15 | Paris | France | Male | École municipale de physique et de chimie indu... | Paris | France | 1906-04-19 | Paris | France | . 19 1903 | Physics | The Nobel Prize in Physics 1903 | &quot;in recognition of the extraordinary services ... | 1/4 | 6 | Individual | Marie Curie, née Sklodowska | 1867-11-07 | Warsaw | Russian Empire (Poland) | Female | NaN | NaN | NaN | 1934-07-04 | Sallanches | France | . 21 1904 | Literature | The Nobel Prize in Literature 1904 | &quot;in recognition of the fresh originality and t... | 1/2 | 573 | Individual | Frédéric Mistral | 1830-09-08 | Maillane | France | Male | NaN | NaN | NaN | 1914-03-25 | Maillane | France | . 22 1904 | Literature | The Nobel Prize in Literature 1904 | &quot;in recognition of the numerous and brilliant ... | 1/2 | 574 | Individual | José Echegaray y Eizaguirre | 1832-04-19 | Madrid | Spain | Male | NaN | NaN | NaN | 1916-09-04 | Madrid | Spain | . 33 1906 | Medicine | The Nobel Prize in Physiology or Medicine 1906 | &quot;in recognition of their work on the structure... | 1/2 | 298 | Individual | Camillo Golgi | 1843-07-07 | Corteno | Italy | Male | Pavia University | Pavia | Italy | 1926-01-21 | Pavia | Italy | . 34 1906 | Medicine | The Nobel Prize in Physiology or Medicine 1906 | &quot;in recognition of their work on the structure... | 1/2 | 299 | Individual | Santiago Ramón y Cajal | 1852-05-01 | Petilla de Aragó | Spain | Male | Madrid University | Madrid | Spain | 1934-10-17 | Madrid | Spain | . 40 1907 | Peace | The Nobel Peace Prize 1907 | NaN | 1/2 | 471 | Individual | Ernesto Teodoro Moneta | 1833-09-20 | Milan | Austrian Empire (Italy) | Male | NaN | NaN | NaN | 1918-02-10 | Milan | Italy | . 41 1907 | Peace | The Nobel Peace Prize 1907 | NaN | 1/2 | 472 | Individual | Louis Renault | 1843-05-21 | Autun | France | Male | Sorbonne University | Paris | France | 1918-02-08 | Barbizon | France | . 45 1908 | Medicine | The Nobel Prize in Physiology or Medicine 1908 | &quot;in recognition of their work on immunity&quot; | 1/2 | 301 | Individual | Ilya Ilyich Mechnikov | 1845-05-15 | Kharkov (Kharkiv) | Russian Empire (Ukraine) | Male | Institut Pasteur | Paris | France | 1916-07-15 | Paris | France | . 46 1908 | Medicine | The Nobel Prize in Physiology or Medicine 1908 | &quot;in recognition of their work on immunity&quot; | 1/2 | 302 | Individual | Paul Ehrlich | 1854-03-14 | Strehlen (Strzelin) | Prussia (Poland) | Male | Goettingen University | Göttingen | Germany | 1915-08-20 | Bad Homburg vor der Höhe | Germany | . 47 1908 | Peace | The Nobel Peace Prize 1908 | NaN | 1/2 | 473 | Individual | Klas Pontus Arnoldson | 1844-10-27 | Gothenburg | Sweden | Male | NaN | NaN | NaN | 1916-02-20 | Stockholm | Sweden | . 48 1908 | Peace | The Nobel Peace Prize 1908 | NaN | 1/2 | 474 | Individual | Fredrik Bajer | 1837-04-21 | Næstved | Denmark | Male | NaN | NaN | NaN | 1922-01-22 | Copenhagen | Denmark | . 53 1909 | Peace | The Nobel Peace Prize 1909 | NaN | 1/2 | 475 | Individual | Auguste Marie François Beernaert | 1829-07-26 | Ostend | Belgium | Male | NaN | NaN | NaN | 1912-10-06 | Lucerne | Switzerland | . 54 1909 | Peace | The Nobel Peace Prize 1909 | NaN | 1/2 | 476 | Individual | Paul Henri Benjamin Balluet d&#39;Estournelles de ... | 1852-11-22 | La Flèche | France | Male | NaN | NaN | NaN | 1924-05-15 | Paris | France | . 55 1909 | Physics | The Nobel Prize in Physics 1909 | &quot;in recognition of their contributions to the ... | 1/2 | 13 | Individual | Guglielmo Marconi | 1874-04-25 | Bologna | Italy | Male | Marconi Wireless Telegraph Co. Ltd. | London | United Kingdom | 1937-07-20 | Rome | Italy | . 56 1909 | Physics | The Nobel Prize in Physics 1909 | &quot;in recognition of their contributions to the ... | 1/2 | 14 | Individual | Karl Ferdinand Braun | 1850-06-06 | Fulda | Hesse-Kassel (Germany) | Male | Strasbourg University | Strasbourg | Alsace (then Germany, now France) | 1918-04-20 | Brooklyn, NY | United States of America | . 65 1911 | Peace | The Nobel Peace Prize 1911 | NaN | 1/2 | 478 | Individual | Tobias Michael Carel Asser | 1838-04-28 | Amsterdam | Netherlands | Male | NaN | NaN | NaN | 1913-07-29 | the Hague | Netherlands | . 66 1911 | Peace | The Nobel Peace Prize 1911 | NaN | 1/2 | 479 | Individual | Alfred Hermann Fried | 1864-11-11 | Vienna | Austria | Male | NaN | NaN | NaN | 1921-05-05 | Vienna | Austria | . 68 1912 | Chemistry | The Nobel Prize in Chemistry 1912 | &quot;for the discovery of the so-called Grignard r... | 1/2 | 172 | Individual | Victor Grignard | 1871-05-06 | Cherbourg | France | Male | Nancy University | Nancy | France | 1935-12-13 | Lyon | France | . 69 1912 | Chemistry | The Nobel Prize in Chemistry 1912 | &quot;for his method of hydrogenating organic compo... | 1/2 | 173 | Individual | Paul Sabatier | 1854-11-05 | Carcassonne | France | Male | Toulouse University | Toulouse | France | 1941-08-14 | Toulouse | France | . 84 1915 | Physics | The Nobel Prize in Physics 1915 | &quot;for their services in the analysis of crystal... | 1/2 | 20 | Individual | Sir William Henry Bragg | 1862-07-02 | Wigton | United Kingdom | Male | University College | London | United Kingdom | 1942-03-12 | London | United Kingdom | . 85 1915 | Physics | The Nobel Prize in Physics 1915 | &quot;for their services in the analysis of crystal... | 1/2 | 21 | Individual | William Lawrence Bragg | 1890-03-31 | Adelaide | Australia | Male | Victoria University | Manchester | United Kingdom | 1971-07-01 | Ipswich | United Kingdom | . 87 1917 | Literature | The Nobel Prize in Literature 1917 | &quot;for his varied and rich poetry, which is insp... | 1/2 | 586 | Individual | Karl Adolph Gjellerup | 1857-06-02 | Roholte | Denmark | Male | NaN | NaN | NaN | 1919-10-11 | Klotzsche | Germany | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 872 2013 | Medicine | The Nobel Prize in Physiology or Medicine 2013 | &quot;for their discoveries of machinery regulating... | 1/3 | 886 | Individual | Thomas C. Südhof | 1955-12-22 | Göttingen | Germany | Male | Stanford University | Stanford, CA | United States of America | NaN | NaN | NaN | . 874 2013 | Physics | The Nobel Prize in Physics 2013 | &quot;for the theoretical discovery of a mechanism ... | 1/2 | 887 | Individual | François Englert | 1932-11-06 | Etterbeek | Belgium | Male | Université Libre de Bruxelles | Brussels | Belgium | NaN | NaN | NaN | . 875 2013 | Physics | The Nobel Prize in Physics 2013 | &quot;for the theoretical discovery of a mechanism ... | 1/2 | 888 | Individual | Peter W. Higgs | 1929-05-29 | Newcastle upon Tyne | United Kingdom | Male | University of Edinburgh | Edinburgh | United Kingdom | NaN | NaN | NaN | . 876 2014 | Chemistry | The Nobel Prize in Chemistry 2014 | &quot;for the development of super-resolved fluores... | 1/3 | 909 | Individual | Eric Betzig | 1960-01-13 | Ann Arbor, MI | United States of America | Male | Janelia Research Campus, Howard Hughes Medical... | Ashburn, VA | United States of America | NaN | NaN | NaN | . 877 2014 | Chemistry | The Nobel Prize in Chemistry 2014 | &quot;for the development of super-resolved fluores... | 1/3 | 910 | Individual | Stefan W. Hell | 1962-12-23 | Arad | Romania | Male | Max Planck Institute for Biophysical Chemistry | Göttingen | Germany | NaN | NaN | NaN | . 878 2014 | Chemistry | The Nobel Prize in Chemistry 2014 | &quot;for the development of super-resolved fluores... | 1/3 | 911 | Individual | William E. Moerner | 1953-06-24 | Pleasanton, CA | United States of America | Male | Stanford University | Stanford, CA | United States of America | NaN | NaN | NaN | . 881 2014 | Medicine | The Nobel Prize in Physiology or Medicine 2014 | &quot;for their discoveries of cells that constitut... | 1/2 | 903 | Individual | John O&#39;Keefe | 1939-11-18 | New York, NY | United States of America | Male | University College | London | United Kingdom | NaN | NaN | NaN | . 882 2014 | Medicine | The Nobel Prize in Physiology or Medicine 2014 | &quot;for their discoveries of cells that constitut... | 1/4 | 904 | Individual | May-Britt Moser | 1963-01-04 | Fosnavåg | Norway | Female | Norwegian University of Science and Technology... | Trondheim | Norway | NaN | NaN | NaN | . 883 2014 | Medicine | The Nobel Prize in Physiology or Medicine 2014 | &quot;for their discoveries of cells that constitut... | 1/4 | 905 | Individual | Edvard I. Moser | 1962-04-27 | Ålesund | Norway | Male | Norwegian University of Science and Technology... | Trondheim | Norway | NaN | NaN | NaN | . 884 2014 | Peace | The Nobel Peace Prize 2014 | &quot;for their struggle against the suppression of... | 1/2 | 913 | Individual | Kailash Satyarthi | 1954-01-11 | Vidisha | India | Male | NaN | NaN | NaN | NaN | NaN | NaN | . 885 2014 | Peace | The Nobel Peace Prize 2014 | &quot;for their struggle against the suppression of... | 1/2 | 914 | Individual | Malala Yousafzai | 1997-07-12 | Mingora | Pakistan | Female | NaN | NaN | NaN | NaN | NaN | NaN | . 886 2014 | Physics | The Nobel Prize in Physics 2014 | &quot;for the invention of efficient blue light-emi... | 1/3 | 906 | Individual | Isamu Akasaki | 1929-01-30 | Chiran | Japan | Male | Meijo University | Nagoya | Japan | NaN | NaN | NaN | . 887 2014 | Physics | The Nobel Prize in Physics 2014 | &quot;for the invention of efficient blue light-emi... | 1/3 | 907 | Individual | Hiroshi Amano | 1960-09-11 | Hamamatsu | Japan | Male | Nagoya University | Nagoya | Japan | NaN | NaN | NaN | . 888 2014 | Physics | The Nobel Prize in Physics 2014 | &quot;for the invention of efficient blue light-emi... | 1/3 | 908 | Individual | Shuji Nakamura | 1954-05-22 | Ikata | Japan | Male | University of California | Santa Barbara, CA | United States of America | NaN | NaN | NaN | . 889 2015 | Chemistry | The Nobel Prize in Chemistry 2015 | &quot;for mechanistic studies of DNA repair&quot; | 1/3 | 921 | Individual | Tomas Lindahl | 1938-01-28 | Stockholm | Sweden | Male | Francis Crick Institute | Hertfordshire | United Kingdom | NaN | NaN | NaN | . 890 2015 | Chemistry | The Nobel Prize in Chemistry 2015 | &quot;for mechanistic studies of DNA repair&quot; | 1/3 | 922 | Individual | Paul Modrich | 1946-06-13 | Raton, NM | United States of America | Male | Howard Hughes Medical Institute | Durham, NC | United States of America | NaN | NaN | NaN | . 891 2015 | Chemistry | The Nobel Prize in Chemistry 2015 | &quot;for mechanistic studies of DNA repair&quot; | 1/3 | 923 | Individual | Aziz Sancar | 1946-09-08 | Savur | Turkey | Male | University of North Carolina | Chapel Hill, NC | United States of America | NaN | NaN | NaN | . 894 2015 | Medicine | The Nobel Prize in Physiology or Medicine 2015 | &quot;for their discoveries concerning a novel ther... | 1/4 | 916 | Individual | William C. Campbell | 1930-06-28 | Ramelton | Ireland | Male | Drew University | Madison, NJ | United States of America | NaN | NaN | NaN | . 895 2015 | Medicine | The Nobel Prize in Physiology or Medicine 2015 | &quot;for their discoveries concerning a novel ther... | 1/4 | 917 | Individual | Satoshi Ōmura | 1935-07-12 | Yamanashi Prefecture | Japan | Male | Kitasato University | Tokyo | Japan | NaN | NaN | NaN | . 896 2015 | Medicine | The Nobel Prize in Physiology or Medicine 2015 | &quot;for her discoveries concerning a novel therap... | 1/2 | 918 | Individual | Youyou Tu | 1930-12-30 | Zhejiang Ningbo | China | Female | China Academy of Traditional Chinese Medicine | Beijing | China | NaN | NaN | NaN | . 898 2015 | Physics | The Nobel Prize in Physics 2015 | &quot;for the discovery of neutrino oscillations, w... | 1/2 | 919 | Individual | Takaaki Kajita | 1959-03-09 | Higashimatsuyama | Japan | Male | University of Tokyo | Kashiwa | Japan | NaN | NaN | NaN | . 899 2015 | Physics | The Nobel Prize in Physics 2015 | &quot;for the discovery of neutrino oscillations, w... | 1/2 | 920 | Individual | Arthur B. McDonald | 1943-08-29 | Sydney | Canada | Male | Queen&#39;s University | Kingston | Canada | NaN | NaN | NaN | . 900 2016 | Chemistry | The Nobel Prize in Chemistry 2016 | &quot;for the design and synthesis of molecular mac... | 1/3 | 931 | Individual | Jean-Pierre Sauvage | 1944-10-21 | Paris | France | Male | University of Strasbourg | Strasbourg | France | NaN | NaN | NaN | . 901 2016 | Chemistry | The Nobel Prize in Chemistry 2016 | &quot;for the design and synthesis of molecular mac... | 1/3 | 932 | Individual | Sir J. Fraser Stoddart | 1942-05-24 | Edinburgh | United Kingdom | Male | Northwestern University | Evanston, IL | United States of America | NaN | NaN | NaN | . 902 2016 | Chemistry | The Nobel Prize in Chemistry 2016 | &quot;for the design and synthesis of molecular mac... | 1/3 | 933 | Individual | Bernard L. Feringa | 1951-05-18 | Barger-Compascuum | Netherlands | Male | University of Groningen | Groningen | Netherlands | NaN | NaN | NaN | . 903 2016 | Economics | The Sveriges Riksbank Prize in Economic Scienc... | &quot;for their contributions to contract theory&quot; | 1/2 | 935 | Individual | Oliver Hart | 1948-10-09 | London | United Kingdom | Male | Harvard University | Cambridge, MA | United States of America | NaN | NaN | NaN | . 904 2016 | Economics | The Sveriges Riksbank Prize in Economic Scienc... | &quot;for their contributions to contract theory&quot; | 1/2 | 936 | Individual | Bengt Holmström | 1949-04-18 | Helsinki | Finland | Male | Massachusetts Institute of Technology (MIT) | Cambridge, MA | United States of America | NaN | NaN | NaN | . 908 2016 | Physics | The Nobel Prize in Physics 2016 | &quot;for theoretical discoveries of topological ph... | 1/2 | 928 | Individual | David J. Thouless | 1934-09-21 | Bearsden | United Kingdom | Male | University of Washington | Seattle, WA | United States of America | NaN | NaN | NaN | . 909 2016 | Physics | The Nobel Prize in Physics 2016 | &quot;for theoretical discoveries of topological ph... | 1/4 | 929 | Individual | F. Duncan M. Haldane | 1951-09-14 | London | United Kingdom | Male | Princeton University | Princeton, NJ | United States of America | NaN | NaN | NaN | . 910 2016 | Physics | The Nobel Prize in Physics 2016 | &quot;for theoretical discoveries of topological ph... | 1/4 | 930 | Individual | J. Michael Kosterlitz | 1943-06-22 | Aberdeen | United Kingdom | Male | Brown University | Providence, RI | United States of America | NaN | NaN | NaN | . 567 rows × 18 columns . Male 836 Female 49 Name: sex, dtype: int64 . United States of America 259 United Kingdom 85 Germany 61 France 51 Sweden 29 Japan 24 Canada 18 Netherlands 18 Italy 17 Russia 17 Name: birth_country, dtype: int64 . 3. USA dominance . Not so surprising perhaps: the most common Nobel laureate between 1901 and 2016 was a man born in the United States of America. But in 1901 all the winners were European. When did the USA start to dominate the Nobel Prize charts? . nobel[&#39;usa_born_winner&#39;] = nobel[&#39;birth_country&#39;] == &quot;United States of America&quot; nobel[&#39;decade&#39;] = np.floor((nobel[&#39;year&#39;] - 1910)/10 + 1).astype(&#39;int64&#39;) prop_usa_winners = nobel.groupby(&#39;decade&#39;, as_index=False).agg({&#39;usa_born_winner&#39;: &#39;mean&#39;}) # Display the proportions of USA born winners per decade # ... YOUR CODE FOR TASK 3 ... display(prop_usa_winners) . decade usa_born_winner . 0 0 | 0.017544 | . 1 1 | 0.075000 | . 2 2 | 0.074074 | . 3 3 | 0.250000 | . 4 4 | 0.302326 | . 5 5 | 0.291667 | . 6 6 | 0.265823 | . 7 7 | 0.317308 | . 8 8 | 0.319588 | . 9 9 | 0.403846 | . 10 10 | 0.422764 | . 11 11 | 0.292683 | . 4. USA dominance, visualized . A table is OK, but to see when the USA started to dominate the Nobel charts we need a plot! . sns.set() # and setting the size of all plots. import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [11, 7] # Plotting USA born winners ax = sns.lineplot(x=&#39;decade&#39;, y=&#39;usa_born_winner&#39;, data=nobel) # Adding %-formatting to the y-axis from matplotlib.ticker import PercentFormatter # ... YOUR CODE FOR TASK 4 ... ax.yaxis.set_major_formatter(PercentFormatter()) . 5. What is the gender of a typical Nobel Prize winner? . So the USA became the dominating winner of the Nobel Prize first in the 1930s and had kept the leading position ever since. But one group that was in the lead from the start, and never seems to let go, are men. Maybe it shouldn&#39;t come as a shock that there is some imbalance between how many male and female prize winners there are, but how significant is this imbalance? And is it better or worse within specific prize categories like physics, medicine, literature, etc.? . nobel[&#39;female_winner&#39;] = nobel[&#39;sex&#39;] == &quot;Female&quot; prop_female_winners = nobel.groupby([&#39;decade&#39;, &#39;category&#39;], as_index=False).agg({&#39;female_winner&#39;: &#39;mean&#39;}) # Plotting USA born winners with % winners on the y-axis # ... YOUR CODE FOR TASK 5 ... ax = sns.lineplot(x=&#39;decade&#39;, y=&#39;female_winner&#39;, hue=&#39;category&#39;, data=nobel) # Adding %-formatting to the y-axis from matplotlib.ticker import PercentFormatter # ... YOUR CODE FOR TASK 4 ... ax.yaxis.set_major_formatter(PercentFormatter()) . 6. The first woman to win the Nobel Prize . The plot above is a bit messy as the lines are overplotting. But it does show some interesting trends and patterns. Overall the imbalance is pretty large with physics, economics, and chemistry having the largest imbalance. Medicine has a somewhat positive trend, and since the 1990s the literature prize is also now more balanced. The big outlier is the peace prize during the 2010s, but keep in mind that this just covers the years 2010 to 2016. . Given this imbalance, who was the first woman to receive a Nobel Prize? And in what category? . # ... YOUR CODE FOR TASK 5 ... nobel[nobel[&#39;sex&#39;] == &quot;Female&quot;].nsmallest(1, &#39;year&#39;) . year category prize motivation prize_share laureate_id laureate_type full_name birth_date birth_city ... sex organization_name organization_city organization_country death_date death_city death_country usa_born_winner decade female_winner . 19 1903 | Physics | The Nobel Prize in Physics 1903 | &quot;in recognition of the extraordinary services ... | 1/4 | 6 | Individual | Marie Curie, née Sklodowska | 1867-11-07 | Warsaw | ... | Female | NaN | NaN | NaN | 1934-07-04 | Sallanches | France | False | 0 | True | . 1 rows × 21 columns . 7. Repeat laureates . For most scientists/writers/activists a Nobel Prize would be the crowning achievement of a long career. But for some people, one is just not enough, and few have gotten it more than once. Who are these lucky few? (Having won no Nobel Prize myself, I&#39;ll assume it&#39;s just about luck.) . # ... YOUR CODE FOR TASK 5 ... nobel.groupby(&#39;full_name&#39;).filter(lambda x: x[&#39;year&#39;].count() &gt;= 2) . year category prize motivation prize_share laureate_id laureate_type full_name birth_date birth_city ... sex organization_name organization_city organization_country death_date death_city death_country usa_born_winner decade female_winner . 19 1903 | Physics | The Nobel Prize in Physics 1903 | &quot;in recognition of the extraordinary services ... | 1/4 | 6 | Individual | Marie Curie, née Sklodowska | 1867-11-07 | Warsaw | ... | Female | NaN | NaN | NaN | 1934-07-04 | Sallanches | France | False | 0 | True | . 62 1911 | Chemistry | The Nobel Prize in Chemistry 1911 | &quot;in recognition of her services to the advance... | 1/1 | 6 | Individual | Marie Curie, née Sklodowska | 1867-11-07 | Warsaw | ... | Female | Sorbonne University | Paris | France | 1934-07-04 | Sallanches | France | False | 1 | True | . 89 1917 | Peace | The Nobel Peace Prize 1917 | NaN | 1/1 | 482 | Organization | Comité international de la Croix Rouge (Intern... | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | False | 1 | False | . 215 1944 | Peace | The Nobel Peace Prize 1944 | NaN | 1/1 | 482 | Organization | Comité international de la Croix Rouge (Intern... | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | False | 4 | False | . 278 1954 | Chemistry | The Nobel Prize in Chemistry 1954 | &quot;for his research into the nature of the chemi... | 1/1 | 217 | Individual | Linus Carl Pauling | 1901-02-28 | Portland, OR | ... | Male | California Institute of Technology (Caltech) | Pasadena, CA | United States of America | 1994-08-19 | Big Sur, CA | United States of America | True | 5 | False | . 283 1954 | Peace | The Nobel Peace Prize 1954 | NaN | 1/1 | 515 | Organization | Office of the United Nations High Commissioner... | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | False | 5 | False | . 298 1956 | Physics | The Nobel Prize in Physics 1956 | &quot;for their researches on semiconductors and th... | 1/3 | 66 | Individual | John Bardeen | 1908-05-23 | Madison, WI | ... | Male | University of Illinois | Urbana, IL | United States of America | 1991-01-30 | Boston, MA | United States of America | True | 5 | False | . 306 1958 | Chemistry | The Nobel Prize in Chemistry 1958 | &quot;for his work on the structure of proteins, es... | 1/1 | 222 | Individual | Frederick Sanger | 1918-08-13 | Rendcombe | ... | Male | University of Cambridge | Cambridge | United Kingdom | 2013-11-19 | Cambridge | United Kingdom | False | 5 | False | . 340 1962 | Peace | The Nobel Peace Prize 1962 | NaN | 1/1 | 217 | Individual | Linus Carl Pauling | 1901-02-28 | Portland, OR | ... | Male | California Institute of Technology (Caltech) | Pasadena, CA | United States of America | 1994-08-19 | Big Sur, CA | United States of America | True | 6 | False | . 348 1963 | Peace | The Nobel Peace Prize 1963 | NaN | 1/2 | 482 | Organization | Comité international de la Croix Rouge (Intern... | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | False | 6 | False | . 424 1972 | Physics | The Nobel Prize in Physics 1972 | &quot;for their jointly developed theory of superco... | 1/3 | 66 | Individual | John Bardeen | 1908-05-23 | Madison, WI | ... | Male | University of Illinois | Urbana, IL | United States of America | 1991-01-30 | Boston, MA | United States of America | True | 7 | False | . 505 1980 | Chemistry | The Nobel Prize in Chemistry 1980 | &quot;for their contributions concerning the determ... | 1/4 | 222 | Individual | Frederick Sanger | 1918-08-13 | Rendcombe | ... | Male | MRC Laboratory of Molecular Biology | Cambridge | United Kingdom | 2013-11-19 | Cambridge | United Kingdom | False | 8 | False | . 523 1981 | Peace | The Nobel Peace Prize 1981 | NaN | 1/1 | 515 | Organization | Office of the United Nations High Commissioner... | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | False | 8 | False | . 13 rows × 21 columns . 8. How old are you when you get the prize? . The list of repeat winners contains some illustrious names! We again meet Marie Curie, who got the prize in physics for discovering radiation and in chemistry for isolating radium and polonium. John Bardeen got it twice in physics for transistors and superconductivity, Frederick Sanger got it twice in chemistry, and Linus Carl Pauling got it first in chemistry and later in peace for his work in promoting nuclear disarmament. We also learn that organizations also get the prize as both the Red Cross and the UNHCR have gotten it twice. . But how old are you generally when you get the prize? . nobel[&#39;birth_date&#39;] = pd.to_datetime(nobel[&#39;birth_date&#39;], format=&#39;%Y-%m-%d&#39;) # Calculating the age of Nobel Prize winners nobel[&#39;age&#39;] = nobel[&#39;year&#39;] - nobel[&#39;birth_date&#39;].dt.year # Plotting the age of Nobel Prize winners sns.lmplot(x=&#39;year&#39;, y=&#39;age&#39;, data=nobel, lowess=True, aspect=2, line_kws={&#39;color&#39;: &#39;black&#39;}) . &lt;seaborn.axisgrid.FacetGrid at 0x7f181c5ffe10&gt; . 9. Age differences between prize categories . The plot above shows us a lot! We see that people use to be around 55 when they received the price, but nowadays the average is closer to 65. But there is a large spread in the laureates&#39; ages, and while most are 50+, some are very young. . We also see that the density of points is much high nowadays than in the early 1900s -- nowadays many more of the prizes are shared, and so there are many more winners. We also see that there was a disruption in awarded prizes around the Second World War (1939 - 1945). . Let&#39;s look at age trends within different prize categories. . # ... YOUR CODE FOR TASK 9 ... sns.lmplot(x=&#39;year&#39;, y=&#39;age&#39;, data=nobel, row=&#39;category&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f181c5edfd0&gt; . 10. Oldest and youngest winners . More plots with lots of exciting stuff going on! We see that both winners of the chemistry, medicine, and physics prize have gotten older over time. The trend is strongest for physics: the average age used to be below 50, and now it&#39;s almost 70. Literature and economics are more stable. We also see that economics is a newer category. But peace shows an opposite trend where winners are getting younger! . In the peace category we also a winner around 2010 that seems exceptionally young. This begs the questions, who are the oldest and youngest people ever to have won a Nobel Prize? . # ... YOUR CODE FOR TASK 10 ... display(nobel.nlargest(1, &#39;age&#39;)) # The youngest winner of a Nobel Prize as of 2016 # ... YOUR CODE FOR TASK 10 ... display(nobel.nsmallest(1, &#39;age&#39;)) . year category prize motivation prize_share laureate_id laureate_type full_name birth_date birth_city ... organization_name organization_city organization_country death_date death_city death_country usa_born_winner decade female_winner age . 793 2007 | Economics | The Sveriges Riksbank Prize in Economic Scienc... | &quot;for having laid the foundations of mechanism ... | 1/3 | 820 | Individual | Leonid Hurwicz | 1917-08-21 | Moscow | ... | University of Minnesota | Minneapolis, MN | United States of America | 2008-06-24 | Minneapolis, MN | United States of America | False | 10 | False | 90.0 | . 1 rows × 22 columns . year category prize motivation prize_share laureate_id laureate_type full_name birth_date birth_city ... organization_name organization_city organization_country death_date death_city death_country usa_born_winner decade female_winner age . 885 2014 | Peace | The Nobel Peace Prize 2014 | &quot;for their struggle against the suppression of... | 1/2 | 914 | Individual | Malala Yousafzai | 1997-07-12 | Mingora | ... | NaN | NaN | NaN | NaN | NaN | NaN | False | 11 | True | 17.0 | . 1 rows × 22 columns . 11. You get a prize! . Hey! You get a prize for making it to the very end of this notebook! It might not be a Nobel Prize, but I made it myself in paint so it should count for something. But don&#39;t despair, Leonid Hurwicz was 90 years old when he got his prize, so it might not be too late for you. Who knows. . Before you leave, what was again the name of the youngest winner ever who in 2014 got the prize for &quot;[her] struggle against the suppression of children and young people and for the right of all children to education&quot;? . youngest_winner = &#39;Malala Yousafzai&#39; .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2021/03/29/A-Visual-History-of-Nobel-Prize-Winners.html",
            "relUrl": "/datacamp/projects/python/2021/03/29/A-Visual-History-of-Nobel-Prize-Winners.html",
            "date": " • Mar 29, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Comparing Cosmetics by Ingredients",
            "content": "1. Cosmetics, chemicals... it&#39;s complicated . Whenever I want to try a new cosmetic item, it&#39;s so difficult to choose. It&#39;s actually more than difficult. It&#39;s sometimes scary because new items that I&#39;ve never tried end up giving me skin trouble. We know the information we need is on the back of each product, but it&#39;s really hard to interpret those ingredient lists unless you&#39;re a chemist. You may be able to relate to this situation. . . So instead of buying and hoping for the best, why don&#39;t we use data science to help us predict which products may be good fits for us? In this notebook, we are going to create a content-based recommendation system where the &#39;content&#39; will be the chemical components of cosmetics. Specifically, we will process ingredient lists for 1472 cosmetics on Sephora via word embedding, then visualize ingredient similarity using a machine learning method called t-SNE and an interactive visualization library called Bokeh. Let&#39;s inspect our data first. . # ... YOUR CODE FOR TASK 1 ... import numpy as np import pandas as pd from sklearn.manifold import TSNE # Load the data df = pd.read_csv(&#39;datasets/cosmetics.csv&#39;) # Check the first five rows # ... YOUR CODE FOR TASK 1 ... df.head() # Inspect the types of products # ... YOUR CODE FOR TASK 1 ... df[&#39;Label&#39;].value_counts() . Moisturizer 298 Cleanser 281 Face Mask 266 Treatment 248 Eye cream 209 Sun protect 170 Name: Label, dtype: int64 . 2. Focus on one product category and one skin type . There are six categories of product in our data (moisturizers, cleansers, face masks, eye creams, and sun protection) and there are five different skin types (combination, dry, normal, oily and sensitive). Because individuals have different product needs as well as different skin types, let&#39;s set up our workflow so its outputs (a t-SNE model and a visualization of that model) can be customized. For the example in this notebook, let&#39;s focus in on moisturizers for those with dry skin by filtering the data accordingly. . moisturizers = df[(df[&#39;Label&#39;] == &#39;Moisturizer&#39;)] # Filter for dry skin as well moisturizers_dry = moisturizers[(moisturizers[&#39;Dry&#39;] == 1)] # Reset index moisturizers_dry = moisturizers_dry.reset_index(drop=True) . 3. Tokenizing the ingredients . To get to our end goal of comparing ingredients in each product, we first need to do some preprocessing tasks and bookkeeping of the actual words in each product&#39;s ingredients list. The first step will be tokenizing the list of ingredients in Ingredients column. After splitting them into tokens, we&#39;ll make a binary bag of words. Then we will create a dictionary with the tokens, ingredient_idx, which will have the following format: . { &quot;ingredient&quot;: index value, … } . ingredient_idx = {} corpus = [] idx = 0 # For loop for tokenization for i in range(len(moisturizers_dry)): ingredients = moisturizers_dry[&#39;Ingredients&#39;][i] ingredients_lower = ingredients.lower() tokens = ingredients_lower.split(&#39;, &#39;) corpus.append(tokens) for ingredient in tokens: if ingredient not in ingredient_idx: ingredient_idx[ingredient] = idx idx += 1 # Check the result print(&quot;The index for decyl oleate is&quot;, ingredient_idx[&#39;decyl oleate&#39;]) . The index for decyl oleate is 25 . 4. Initializing a document-term matrix (DTM) . The next step is making a document-term matrix (DTM). Here each cosmetic product will correspond to a document, and each chemical composition will correspond to a term. This means we can think of the matrix as a “cosmetic-ingredient” matrix. The size of the matrix should be as the picture shown below. To create this matrix, we&#39;ll first make an empty matrix filled with zeros. The length of the matrix is the total number of cosmetic products in the data. The width of the matrix is the total number of ingredients. After initializing this empty matrix, we&#39;ll fill it in the following tasks. . M = len(moisturizers_dry) N = len(ingredient_idx) # Initialize a matrix of zeros A = np.zeros((M, N)) . 5. Creating a counter function . Before we can fill the matrix, let&#39;s create a function to count the tokens (i.e., an ingredients list) for each row. Our end goal is to fill the matrix with 1 or 0: if an ingredient is in a cosmetic, the value is 1. If not, it remains 0. The name of this function, oh_encoder, will become clear next. . def oh_encoder(tokens): x = np.zeros((N,)) for ingredient in tokens: # Get the index for each ingredient idx = ingredient_idx[ingredient] # Put 1 at the corresponding indices x[idx] = 1 return x . 6. The Cosmetic-Ingredient matrix! . Now we&#39;ll apply the oh_encoder() functon to the tokens in corpus and set the values at each row of this matrix. So the result will tell us what ingredients each item is composed of. For example, if a cosmetic item contains water, niacin, decyl aleate and sh-polypeptide-1, the outcome of this item will be as follows. This is what we called one-hot encoding. By encoding each ingredient in the items, the Cosmetic-Ingredient matrix will be filled with binary values. . i = 0 for tokens in corpus: A[i, :] = oh_encoder(tokens) # ... YOUR CODE FOR TASK 6 ... i = i + 1 . 7. Dimension reduction with t-SNE . The dimensions of the existing matrix is (190, 2233), which means there are 2233 features in our data. For visualization, we should downsize this into two dimensions. We&#39;ll use t-SNE for reducing the dimension of the data here. . T-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique that is well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, this technique can reduce the dimension of data while keeping the similarities between the instances. This enables us to make a plot on the coordinate plane, which can be said as vectorizing. All of these cosmetic items in our data will be vectorized into two-dimensional coordinates, and the distances between the points will indicate the similarities between the items. . model = TSNE(n_components=2, learning_rate=200, random_state=42) tsne_features = model.fit_transform(A) # Make X, Y columns moisturizers_dry[&#39;X&#39;] = tsne_features[:, 0] moisturizers_dry[&#39;Y&#39;] = tsne_features[:, 1] . 8. Let&#39;s map the items with Bokeh . We are now ready to start creating our plot. With the t-SNE values, we can plot all our items on the coordinate plane. And the coolest part here is that it will also show us the name, the brand, the price and the rank of each item. Let&#39;s make a scatter plot using Bokeh and add a hover tool to show that information. Note that we won&#39;t display the plot yet as we will make some more additions to it. . from bokeh.io import show, output_notebook, push_notebook from bokeh.plotting import figure from bokeh.models import ColumnDataSource, HoverTool output_notebook() # Make a source and a scatter plot source = ColumnDataSource(moisturizers_dry) plot = figure(x_axis_label = &#39;T-SNE 1&#39;, y_axis_label = &#39;T-SNE 2&#39;, width = 500, height = 400) plot.circle(x = &#39;X&#39;, y = &#39;Y&#39;, source = source, size = 10, color = &#39;#FF7373&#39;, alpha = .8) . Loading BokehJS ... GlyphRenderer(id&nbsp;=&nbsp;&#39;1177&#39;, &hellip;)data_source&nbsp;=&nbsp;ColumnDataSource(id=&#39;1139&#39;, ...),glyph&nbsp;=&nbsp;Circle(id=&#39;1175&#39;, ...),hover_glyph&nbsp;=&nbsp;None,js_event_callbacks&nbsp;=&nbsp;{},js_property_callbacks&nbsp;=&nbsp;{},level&nbsp;=&nbsp;&#39;glyph&#39;,muted&nbsp;=&nbsp;False,muted_glyph&nbsp;=&nbsp;None,name&nbsp;=&nbsp;None,nonselection_glyph&nbsp;=&nbsp;Circle(id=&#39;1176&#39;, ...),selection_glyph&nbsp;=&nbsp;None,subscribed_events&nbsp;=&nbsp;[],tags&nbsp;=&nbsp;[],view&nbsp;=&nbsp;CDSView(id=&#39;1178&#39;, ...),visible&nbsp;=&nbsp;True,x_range_name&nbsp;=&nbsp;&#39;default&#39;,y_range_name&nbsp;=&nbsp;&#39;default&#39;) 9. Adding a hover tool . Why don&#39;t we add a hover tool? Adding a hover tool allows us to check the information of each item whenever the cursor is directly over a glyph. We&#39;ll add tooltips with each product&#39;s name, brand, price, and rank (i.e., rating). . hover = HoverTool(tooltips = [(&#39;Item&#39;, &#39;@Name&#39;), (&#39;Brand&#39;, &#39;@Brand&#39;), (&#39;Price&#39;, &#39;$@Price&#39;), (&#39;Rank&#39;, &#39;@Rank&#39;)]) plot.add_tools(hover) . 10. Mapping the cosmetic items . Finally, it&#39;s show time! Let&#39;s see how the map we&#39;ve made looks like. Each point on the plot corresponds to the cosmetic items. Then what do the axes mean here? The axes of a t-SNE plot aren&#39;t easily interpretable in terms of the original data. Like mentioned above, t-SNE is a visualizing technique to plot high-dimensional data in a low-dimensional space. Therefore, it&#39;s not desirable to interpret a t-SNE plot quantitatively. . Instead, what we can get from this map is the distance between the points (which items are close and which are far apart). The closer the distance between the two items is, the more similar the composition they have. Therefore this enables us to compare the items without having any chemistry background. . # ... YOUR CODE FOR TASK 10 ... show(plot) . 11. Comparing two products . Since there are so many cosmetics and so many ingredients, the plot doesn&#39;t have many super obvious patterns that simpler t-SNE plots can have (example). Our plot requires some digging to find insights, but that&#39;s okay! . Say we enjoyed a specific product, there&#39;s an increased chance we&#39;d enjoy another product that is similar in chemical composition. Say we enjoyed AmorePacific&#39;s Color Control Cushion Compact Broad Spectrum SPF 50+. We could find this product on the plot and see if a similar product(s) exist. And it turns out it does! If we look at the points furthest left on the plot, we see LANEIGE&#39;s BB Cushion Hydra Radiance SPF 50 essentially overlaps with the AmorePacific product. By looking at the ingredients, we can visually confirm the compositions of the products are similar (though it is difficult to do, which is why we did this analysis in the first place!), plus LANEIGE&#39;s version is $22 cheaper and actually has higher ratings. . It&#39;s not perfect, but it&#39;s useful. In real life, we can actually use our little ingredient-based recommendation engine help us make educated cosmetic purchase choices. . cosmetic_1 = moisturizers_dry[moisturizers_dry[&#39;Name&#39;] == &quot;Color Control Cushion Compact Broad Spectrum SPF 50+&quot;] cosmetic_2 = moisturizers_dry[moisturizers_dry[&#39;Name&#39;] == &quot;BB Cushion Hydra Radiance SPF 50&quot;] # Display each item&#39;s data and ingredients display(cosmetic_1) print(cosmetic_1.Ingredients.values) display(cosmetic_2) print(cosmetic_2.Ingredients.values) . Label Brand Name Price Rank Ingredients Combination Dry Normal Oily Sensitive X Y . 45 Moisturizer | AMOREPACIFIC | Color Control Cushion Compact Broad Spectrum S... | 60 | 4.0 | Phyllostachis Bambusoides Juice, Cyclopentasil... | 1 | 1 | 1 | 1 | 1 | 2.775364 | -0.274434 | . [&#39;Phyllostachis Bambusoides Juice, Cyclopentasiloxane, Cyclohexasiloxane, Peg-10 Dimethicone, Phenyl Trimethicone, Butylene Glycol, Butylene Glycol Dicaprylate/Dicaprate, Alcohol, Arbutin, Lauryl Peg-9 Polydimethylsiloxyethyl Dimethicone, Acrylates/Ethylhexyl Acrylate/Dimethicone Methacrylate Copolymer, Polyhydroxystearic Acid, Sodium Chloride, Polymethyl Methacrylate, Aluminium Hydroxide, Stearic Acid, Disteardimonium Hectorite, Triethoxycaprylylsilane, Ethylhexyl Palmitate, Lecithin, Isostearic Acid, Isopropyl Palmitate, Phenoxyethanol, Polyglyceryl-3 Polyricinoleate, Acrylates/Stearyl Acrylate/Dimethicone Methacrylate Copolymer, Dimethicone, Disodium Edta, Trimethylsiloxysilicate, Ethylhexyglycerin, Dimethicone/Vinyl Dimethicone Crosspolymer, Water, Silica, Camellia Japonica Seed Oil, Camillia Sinensis Leaf Extract, Caprylyl Glycol, 1,2-Hexanediol, Fragrance, Titanium Dioxide, Iron Oxides (Ci 77492, Ci 77491, Ci77499).&#39;] . Label Brand Name Price Rank Ingredients Combination Dry Normal Oily Sensitive X Y . 55 Moisturizer | LANEIGE | BB Cushion Hydra Radiance SPF 50 | 38 | 4.3 | Water, Cyclopentasiloxane, Zinc Oxide (CI 7794... | 1 | 1 | 1 | 1 | 1 | 2.814905 | -0.277909 | . [&#39;Water, Cyclopentasiloxane, Zinc Oxide (CI 77947), Ethylhexyl Methoxycinnamate, PEG-10 Dimethicone, Cyclohexasiloxane, Phenyl Trimethicone, Iron Oxides (CI 77492), Butylene Glycol Dicaprylate/Dicaprate, Niacinamide, Lauryl PEG-9 Polydimethylsiloxyethyl Dimethicone, Acrylates/Ethylhexyl Acrylate/Dimethicone Methacrylate Copolymer, Titanium Dioxide (CI 77891 , Iron Oxides (CI 77491), Butylene Glycol, Sodium Chloride, Iron Oxides (CI 77499), Aluminum Hydroxide, HDI/Trimethylol Hexyllactone Crosspolymer, Stearic Acid, Methyl Methacrylate Crosspolymer, Triethoxycaprylylsilane, Phenoxyethanol, Fragrance, Disteardimonium Hectorite, Caprylyl Glycol, Yeast Extract, Acrylates/Stearyl Acrylate/Dimethicone Methacrylate Copolymer, Dimethicone, Trimethylsiloxysilicate, Polysorbate 80, Disodium EDTA, Hydrogenated Lecithin, Dimethicone/Vinyl Dimethicone Crosspolymer, Mica (CI 77019), Silica, 1,2-Hexanediol, Polypropylsilsesquioxane, Chenopodium Quinoa Seed Extract, Magnesium Sulfate, Calcium Chloride, Camellia Sinensis Leaf Extract, Manganese Sulfate, Zinc Sulfate, Ascorbyl Glucoside.&#39;] .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2021/03/16/Comparing-Cosmetics-by-Ingredients.html",
            "relUrl": "/datacamp/projects/python/2021/03/16/Comparing-Cosmetics-by-Ingredients.html",
            "date": " • Mar 16, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "A New Era of Data Analysis in Baseball",
            "content": "1. The Statcast revolution . This is Aaron Judge. Judge is one of the physically largest players in Major League Baseball standing 6 feet 7 inches (2.01 m) tall and weighing 282 pounds (128 kg). He also hit the hardest home run ever recorded. How do we know this? Statcast. . Statcast is a state-of-the-art tracking system that uses high-resolution cameras and radar equipment to measure the precise location and movement of baseballs and baseball players. Introduced in 2015 to all 30 major league ballparks, Statcast data is revolutionizing the game. Teams are engaging in an &quot;arms race&quot; of data analysis, hiring analysts left and right in an attempt to gain an edge over their competition. This video describing the system is incredible. . In this notebook, we&#39;re going to wrangle, analyze, and visualize Statcast data to compare Mr. Judge and another (extremely large) teammate of his. Let&#39;s start by loading the data into our Notebook. There are two CSV files, judge.csv and stanton.csv, both of which contain Statcast data for 2015-2017. We&#39;ll use pandas DataFrames to store this data. Let&#39;s also load our data visualization libraries, matplotlib and seaborn. . import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline # Load Aaron Judge&#39;s Statcast data judge = pd.read_csv(&#39;datasets/judge.csv&#39;) # Load Giancarlo Stanton&#39;s Statcast data stanton = pd.read_csv(&#39;datasets/stanton.csv&#39;) . 2. What can Statcast measure? . The better question might be, what can&#39;t Statcast measure? . Starting with the pitcher, Statcast can measure simple data points such as velocity. At the same time, Statcast digs a whole lot deeper, also measuring the release point and spin rate of every pitch. . Moving on to hitters, Statcast is capable of measuring the exit velocity, launch angle and vector of the ball as it comes off the bat. From there, Statcast can also track the hang time and projected distance that a ball travels. . Let&#39;s inspect the last five rows of the judge DataFrame. You&#39;ll see that each row represents one pitch thrown to a batter. You&#39;ll also see that some columns have esoteric names. If these don&#39;t make sense now, don&#39;t worry. The relevant ones will be explained as necessary. . pd.set_option(&#39;display.max_columns&#39;, None) # Display the last five rows of the Aaron Judge file judge.tail() . pitch_type game_date release_speed release_pos_x release_pos_z player_name batter pitcher events description spin_dir spin_rate_deprecated break_angle_deprecated break_length_deprecated zone des game_type stand p_throws home_team away_team type hit_location bb_type balls strikes game_year pfx_x pfx_z plate_x plate_z on_3b on_2b on_1b outs_when_up inning inning_topbot hc_x hc_y tfs_deprecated tfs_zulu_deprecated pos2_person_id umpire sv_id vx0 vy0 vz0 ax ay az sz_top sz_bot hit_distance_sc launch_speed launch_angle effective_speed release_spin_rate release_extension game_pk pos1_person_id pos2_person_id.1 pos3_person_id pos4_person_id pos5_person_id pos6_person_id pos7_person_id pos8_person_id pos9_person_id release_pos_y estimated_ba_using_speedangle estimated_woba_using_speedangle woba_value woba_denom babip_value iso_value launch_speed_angle at_bat_number pitch_number . 3431 CH | 2016-08-13 | 85.6 | -1.9659 | 5.9113 | Aaron Judge | 592450 | 542882 | NaN | ball | NaN | NaN | NaN | NaN | 14.0 | NaN | R | R | R | NYY | TB | B | NaN | NaN | 0 | 0 | 2016 | -0.379108 | 0.370567 | 0.739 | 1.442 | NaN | NaN | NaN | 0 | 5 | Bot | NaN | NaN | NaN | NaN | 571912.0 | NaN | 160813_144259 | 6.960 | -124.371 | -4.756 | -2.821 | 23.634 | -30.220 | 3.93 | 1.82 | NaN | NaN | NaN | 84.459 | 1552.0 | 5.683 | 448611 | 542882.0 | 571912.0 | 543543.0 | 523253.0 | 446334.0 | 622110.0 | 545338.0 | 595281.0 | 543484.0 | 54.8144 | 0.00 | 0.000 | NaN | NaN | NaN | NaN | NaN | 36 | 1 | . 3432 CH | 2016-08-13 | 87.6 | -1.9318 | 5.9349 | Aaron Judge | 592450 | 542882 | home_run | hit_into_play_score | NaN | NaN | NaN | NaN | 4.0 | Aaron Judge homers (1) on a fly ball to center... | R | R | R | NYY | TB | X | NaN | fly_ball | 1 | 2 | 2016 | -0.295608 | 0.320400 | -0.419 | 3.273 | NaN | NaN | NaN | 2 | 2 | Bot | 130.45 | 14.58 | NaN | NaN | 571912.0 | NaN | 160813_135833 | 4.287 | -127.452 | -0.882 | -1.972 | 24.694 | -30.705 | 4.01 | 1.82 | 446.0 | 108.8 | 27.410 | 86.412 | 1947.0 | 5.691 | 448611 | 542882.0 | 571912.0 | 543543.0 | 523253.0 | 446334.0 | 622110.0 | 545338.0 | 595281.0 | 543484.0 | 54.8064 | 0.98 | 1.937 | 2.0 | 1.0 | 0.0 | 3.0 | 6.0 | 14 | 4 | . 3433 CH | 2016-08-13 | 87.2 | -2.0285 | 5.8656 | Aaron Judge | 592450 | 542882 | NaN | ball | NaN | NaN | NaN | NaN | 14.0 | NaN | R | R | R | NYY | TB | B | NaN | NaN | 0 | 2 | 2016 | -0.668575 | 0.198567 | 0.561 | 0.960 | NaN | NaN | NaN | 2 | 2 | Bot | NaN | NaN | NaN | NaN | 571912.0 | NaN | 160813_135815 | 7.491 | -126.665 | -5.862 | -6.393 | 21.952 | -32.121 | 4.01 | 1.82 | NaN | NaN | NaN | 86.368 | 1761.0 | 5.721 | 448611 | 542882.0 | 571912.0 | 543543.0 | 523253.0 | 446334.0 | 622110.0 | 545338.0 | 595281.0 | 543484.0 | 54.7770 | 0.00 | 0.000 | NaN | NaN | NaN | NaN | NaN | 14 | 3 | . 3434 CU | 2016-08-13 | 79.7 | -1.7108 | 6.1926 | Aaron Judge | 592450 | 542882 | NaN | foul | NaN | NaN | NaN | NaN | 4.0 | NaN | R | R | R | NYY | TB | S | NaN | NaN | 0 | 1 | 2016 | 0.397442 | -0.614133 | -0.803 | 2.742 | NaN | NaN | NaN | 2 | 2 | Bot | NaN | NaN | NaN | NaN | 571912.0 | NaN | 160813_135752 | 1.254 | -116.062 | 0.439 | 5.184 | 21.328 | -39.866 | 4.01 | 1.82 | 9.0 | 55.8 | -24.973 | 77.723 | 2640.0 | 5.022 | 448611 | 542882.0 | 571912.0 | 543543.0 | 523253.0 | 446334.0 | 622110.0 | 545338.0 | 595281.0 | 543484.0 | 55.4756 | 0.00 | 0.000 | NaN | NaN | NaN | NaN | 1.0 | 14 | 2 | . 3435 FF | 2016-08-13 | 93.2 | -1.8476 | 6.0063 | Aaron Judge | 592450 | 542882 | NaN | called_strike | NaN | NaN | NaN | NaN | 8.0 | NaN | R | R | R | NYY | TB | S | NaN | NaN | 0 | 0 | 2016 | -0.823050 | 1.623300 | -0.273 | 2.471 | NaN | NaN | NaN | 2 | 2 | Bot | NaN | NaN | NaN | NaN | 571912.0 | NaN | 160813_135736 | 5.994 | -135.497 | -6.736 | -9.360 | 26.782 | -13.446 | 4.01 | 1.82 | NaN | NaN | NaN | 92.696 | 2271.0 | 6.068 | 448611 | 542882.0 | 571912.0 | 543543.0 | 523253.0 | 446334.0 | 622110.0 | 545338.0 | 595281.0 | 543484.0 | 54.4299 | 0.00 | 0.000 | NaN | NaN | NaN | NaN | NaN | 14 | 1 | . 3. Aaron Judge and Giancarlo Stanton, prolific sluggers . This is Giancarlo Stanton. He is also a very large human being, standing 6 feet 6 inches tall and weighing 245 pounds. Despite not wearing the same jersey as Judge in the pictures provided, in 2018 they will be teammates on the New York Yankees. They are similar in a lot of ways, one being that they hit a lot of home runs. Stanton and Judge led baseball in home runs in 2017, with 59 and 52, respectively. These are exceptional totals - the player in third &quot;only&quot; had 45 home runs. . Stanton and Judge are also different in many ways. One is batted ball events, which is any batted ball that produces a result. This includes outs, hits, and errors. Next, you&#39;ll find the counts of batted ball events for each player in 2017. The frequencies of other events are quite different. . judge_events_2017 = judge[judge[&#39;game_date&#39;] &gt; &#39;2017-01-01&#39;][&#39;events&#39;] print(&quot;Aaron Judge batted ball event totals, 2017:&quot;) print(judge_events_2017.value_counts()) # All of Giancarlo Stanton&#39;s batted ball events in 2017 stanton_events_2017 = stanton[stanton[&#39;game_date&#39;] &gt; &#39;2017-01-01&#39;][&#39;events&#39;] print(&quot; nGiancarlo Stanton batted ball event totals, 2017:&quot;) print(stanton_events_2017.value_counts()) . Aaron Judge batted ball event totals, 2017: strikeout 207 field_out 146 walk 116 single 75 home_run 52 double 24 grounded_into_double_play 15 intent_walk 11 force_out 11 hit_by_pitch 5 sac_fly 4 fielders_choice_out 4 field_error 4 triple 3 strikeout_double_play 1 Name: events, dtype: int64 Giancarlo Stanton batted ball event totals, 2017: field_out 239 strikeout 161 single 77 walk 72 home_run 59 double 32 intent_walk 13 grounded_into_double_play 13 hit_by_pitch 7 force_out 7 field_error 5 sac_fly 3 strikeout_double_play 2 fielders_choice_out 2 pickoff_1b 1 Name: events, dtype: int64 . 4. Analyzing home runs with Statcast data . So Judge walks and strikes out more than Stanton. Stanton flies out more than Judge. But let&#39;s get into their hitting profiles in more detail. Two of the most groundbreaking Statcast metrics are launch angle and exit velocity: . Launch angle: the vertical angle at which the ball leaves a player&#39;s bat | Exit velocity: the speed of the baseball as it comes off the bat | . This new data has changed the way teams value both hitters and pitchers. Why? As per the Washington Post: . Balls hit with a high launch angle are more likely to result in a hit. Hit fast enough and at the right angle, they become home runs. . Let&#39;s look at exit velocity vs. launch angle and let&#39;s focus on home runs only (2015-2017). The first two plots show data points. The second two show smoothed contours to represent density. . judge_hr = judge[judge[&#39;events&#39;] == &#39;home_run&#39;] stanton_hr = stanton[stanton[&#39;events&#39;] == &#39;home_run&#39;] # Create a figure with two scatter plots of launch speed vs. launch angle, one for each player&#39;s home runs fig1, axs1 = plt.subplots(ncols=2, sharex=True, sharey=True) sns.regplot(x=&#39;launch_speed&#39;, y=&#39;launch_angle&#39;, fit_reg=False, color=&#39;tab:blue&#39;, data=judge_hr, ax=axs1[0]).set_title(&#39;Aaron Judge nHome Runs, 2015-2017&#39;) sns.regplot(x=&#39;launch_speed&#39;, y=&#39;launch_angle&#39;, fit_reg=False, color=&#39;tab:blue&#39;, data=stanton_hr, ax=axs1[1]).set_title(&#39;Giancarlo Stanton nHome Runs, 2015-2017&#39;) # Create a figure with two KDE plots of launch speed vs. launch angle, one for each player&#39;s home runs fig2, axs2 = plt.subplots(ncols=2, sharex=True, sharey=True) sns.kdeplot(judge_hr[&#39;launch_speed&#39;], judge_hr[&#39;launch_angle&#39;], cmap=&quot;Blues&quot;, shade=True, shade_lowest=False, ax=axs2[0]).set_title(&#39;Aaron Judge nHome Runs, 2015-2017&#39;) sns.kdeplot(stanton_hr[&#39;launch_speed&#39;], stanton_hr[&#39;launch_angle&#39;], cmap=&quot;Blues&quot;, shade=True, shade_lowest=False, ax=axs2[1]).set_title(&#39;Giancarlo Stanton nHome Runs, 2015-2017&#39;) . Text(0.5,1,&#39;Giancarlo Stanton nHome Runs, 2015-2017&#39;) . 5. Home runs by pitch velocity . It appears that Stanton hits his home runs slightly lower and slightly harder than Judge, though this needs to be taken with a grain of salt given the small sample size of home runs. . Not only does Statcast measure the velocity of the ball coming off of the bat, it measures the velocity of the ball coming out of the pitcher&#39;s hand and begins its journey towards the plate. We can use this data to compare Stanton and Judge&#39;s home runs in terms of pitch velocity. Next you&#39;ll find box plots displaying the five-number summaries for each player: minimum, first quartile, median, third quartile, and maximum. . judge_stanton_hr = pd.concat([judge_hr, stanton_hr], axis=0) # Create a boxplot that describes the pitch velocity of each player&#39;s home runs sns.boxplot(judge_stanton_hr[&#39;release_speed&#39;]).set_title(&#39;Home Runs, 2015-2017&#39;) . Text(0.5,1,&#39;Home Runs, 2015-2017&#39;) . 6. Home runs by pitch location (I) . So Judge appears to hit his home runs off of faster pitches than Stanton. We might call Judge a fastball hitter. Stanton appears agnostic to pitch speed and likely pitch movement since slower pitches (e.g. curveballs, sliders, and changeups) tend to have more break. Statcast does track pitch movement and type but let&#39;s move on to something else: pitch location. Statcast tracks the zone the pitch is in when it crosses the plate. The zone numbering looks like this (from the catcher&#39;s point of view): . . We can plot this using a 2D histogram. For simplicity, let&#39;s only look at strikes, which gives us a 9x9 grid. We can view each zone as coordinates on a 2D plot, the bottom left corner being (1,1) and the top right corner being (3,3). Let&#39;s set up a function to assign x-coordinates to each pitch. . def assign_x_coord(row): &quot;&quot;&quot; Assigns an x-coordinate to Statcast&#39;s strike zone numbers. Zones 11, 12, 13, and 14 are ignored for plotting simplicity. &quot;&quot;&quot; # Left third of strike zone if row.zone in [1, 4, 7]: return 1 # Middle third of strike zone if row.zone in [2, 5, 8]: return 2 # Right third of strike zone if row.zone in [3, 6, 9]: return 3 . 7. Home runs by pitch location (II) . And let&#39;s do the same but for y-coordinates. . def assign_y_coord(row): &quot;&quot;&quot; Assigns a y-coordinate to Statcast&#39;s strike zone numbers. Zones 11, 12, 13, and 14 are ignored for plotting simplicity. &quot;&quot;&quot; # Upper third of strike zone if row.zone in [1, 2, 3]: return 3 # Middle third of strike zone if row.zone in [4, 5, 6]: return 2 # Lower third of strike zone if row.zone in [7, 8, 9]: return 1 . 8. Aaron Judge&#39;s home run zone . Now we can apply the functions we&#39;ve created then construct our 2D histograms. First, for Aaron Judge (again, for pitches in the strike zone that resulted in home runs). . judge_strike_hr = judge_hr.copy().loc[judge_hr.zone &lt;= 9] # Assign Cartesian coordinates to pitches in the strike zone for Judge home runs judge_strike_hr[&#39;zone_x&#39;] = judge_strike_hr.apply(assign_x_coord, axis=1) judge_strike_hr[&#39;zone_y&#39;] = judge_strike_hr.apply(assign_y_coord, axis=1) # Plot Judge&#39;s home run zone as a 2D histogram with a colorbar plt.hist2d(judge_strike_hr[&#39;zone_x&#39;], judge_strike_hr[&#39;zone_y&#39;], bins = 3, cmap=&#39;Blues&#39;) plt.title(&#39;Aaron Judge Home Runs on n Pitches in the Strike Zone, 2015-2017&#39;) plt.gca().get_xaxis().set_visible(False) plt.gca().get_yaxis().set_visible(False) cb = plt.colorbar() cb.set_label(&#39;Counts in Bin&#39;) . 9. Giancarlo Stanton&#39;s home run zone . And now for Giancarlo Stanton. . stanton_strike_hr = stanton_hr.copy().loc[stanton_hr.zone &lt;= 9] # Assign Cartesian coordinates to pitches in the strike zone for Stanton home runs ome runs stanton_strike_hr[&#39;zone_x&#39;] = stanton_strike_hr.apply(assign_x_coord, axis=1) stanton_strike_hr[&#39;zone_y&#39;] = stanton_strike_hr.apply(assign_y_coord, axis=1) # Plot Stanton&#39;s home run zone as a 2D histogram with a colorbar plt.hist2d(stanton_strike_hr[&#39;zone_x&#39;], stanton_strike_hr[&#39;zone_y&#39;], bins = 3, cmap=&#39;Blues&#39;) plt.title(&#39;Giancarlo Stanton Home Runs on n Pitches in the Strike Zone, 2015-2017&#39;) plt.gca().get_xaxis().set_visible(False) plt.gca().get_yaxis().set_visible(False) cb = plt.colorbar() cb.set_label(&#39;Counts in Bin&#39;) . File &#34;&lt;ipython-input-286-e2275b3e6291&gt;&#34;, line 5 ome runs ^ SyntaxError: invalid syntax . 10. Should opposing pitchers be scared? . A few takeaways: . Stanton does not hit many home runs on pitches in the upper third of the strike zone. | Like pretty much every hitter ever, both players love pitches in the horizontal and vertical middle of the plate. | Judge&#39;s least favorite home run pitch appears to be high-away while Stanton&#39;s appears to be low-away. | If we were to describe Stanton&#39;s home run zone, it&#39;d be middle-inside. Judge&#39;s home run zone is much more spread out. | . The grand takeaway from this whole exercise: Aaron Judge and Giancarlo Stanton are not identical despite their superficial similarities. In terms of home runs, their launch profiles, as well as their pitch speed and location preferences, are different. . Should opposing pitchers still be scared? . should_pitchers_be_scared = True .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2020/04/08/A-New-Era-of-Data-Analysis-in-Baseball.html",
            "relUrl": "/datacamp/projects/python/2020/04/08/A-New-Era-of-Data-Analysis-in-Baseball.html",
            "date": " • Apr 8, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Visualizing COVID-19",
            "content": "1. From epidemic to pandemic . In December 2019, COVID-19 coronavirus was first identified in the Wuhan region of China. By March 11, 2020, the World Health Organization (WHO) categorized the COVID-19 outbreak as a pandemic. A lot has happened in the months in between with major outbreaks in Iran, South Korea, and Italy. . We know that COVID-19 spreads through respiratory droplets, such as through coughing, sneezing, or speaking. But, how quickly did the virus spread across the globe? And, can we see any effect from country-wide policies, like shutdowns and quarantines? . Fortunately, organizations around the world have been collecting data so that governments can monitor and learn from this pandemic. Notably, the Johns Hopkins University Center for Systems Science and Engineering created a publicly available data repository to consolidate this data from sources like the WHO, the Centers for Disease Control and Prevention (CDC), and the Ministry of Health from multiple countries. . In this notebook, you will visualize COVID-19 data from the first several weeks of the outbreak to see at what point this virus became a global pandemic. . Please note that information and data regarding COVID-19 is frequently being updated. The data used in this project was pulled on March 17, 2020, and should not be considered to be the most up to date data available. . library(readr) library(ggplot2) library(dplyr) # Read datasets/confirmed_cases_worldwide.csv into confirmed_cases_worldwide confirmed_cases_worldwide &lt;- read_csv(&#39;datasets/confirmed_cases_worldwide.csv&#39;) # See the result confirmed_cases_worldwide . Parsed with column specification: cols( date = col_date(format = &#34;&#34;), cum_cases = col_double() ) . A spec_tbl_df: 56 x 2 datecum_cases . &lt;date&gt;&lt;dbl&gt; . 2020-01-22 | 555 | . 2020-01-23 | 653 | . 2020-01-24 | 941 | . 2020-01-25 | 1434 | . 2020-01-26 | 2118 | . 2020-01-27 | 2927 | . 2020-01-28 | 5578 | . 2020-01-29 | 6166 | . 2020-01-30 | 8234 | . 2020-01-31 | 9927 | . 2020-02-01 | 12038 | . 2020-02-02 | 16787 | . 2020-02-03 | 19881 | . 2020-02-04 | 23892 | . 2020-02-05 | 27635 | . 2020-02-06 | 30817 | . 2020-02-07 | 34391 | . 2020-02-08 | 37120 | . 2020-02-09 | 40150 | . 2020-02-10 | 42762 | . 2020-02-11 | 44802 | . 2020-02-12 | 45221 | . 2020-02-13 | 60368 | . 2020-02-14 | 66885 | . 2020-02-15 | 69030 | . 2020-02-16 | 71224 | . 2020-02-17 | 73258 | . 2020-02-18 | 75136 | . 2020-02-19 | 75639 | . 2020-02-20 | 76197 | . 2020-02-21 | 76823 | . 2020-02-22 | 78579 | . 2020-02-23 | 78965 | . 2020-02-24 | 79568 | . 2020-02-25 | 80413 | . 2020-02-26 | 81395 | . 2020-02-27 | 82754 | . 2020-02-28 | 84120 | . 2020-02-29 | 86011 | . 2020-03-01 | 88369 | . 2020-03-02 | 90306 | . 2020-03-03 | 92840 | . 2020-03-04 | 95120 | . 2020-03-05 | 97882 | . 2020-03-06 | 101784 | . 2020-03-07 | 105821 | . 2020-03-08 | 109795 | . 2020-03-09 | 113561 | . 2020-03-10 | 118592 | . 2020-03-11 | 125865 | . 2020-03-12 | 128343 | . 2020-03-13 | 145193 | . 2020-03-14 | 156097 | . 2020-03-15 | 167449 | . 2020-03-16 | 181531 | . 2020-03-17 | 197146 | . 2. Confirmed cases throughout the world . The table above shows the cumulative confirmed cases of COVID-19 worldwide by date. Just reading numbers in a table makes it hard to get a sense of the scale and growth of the outbreak. Let&#39;s draw a line plot to visualize the confirmed cases worldwide. . # Label the y-axis ggplot(data=confirmed_cases_worldwide, aes(x=date, y=cum_cases)) + geom_line() + geom_point() + ylab(&quot;Cumulative confirmed cases&quot;) . 3. China compared to the rest of the world . The y-axis in that plot is pretty scary, with the total number of confirmed cases around the world approaching 200,000. Beyond that, some weird things are happening: there is an odd jump in mid February, then the rate of new cases slows down for a while, then speeds up again in March. We need to dig deeper to see what is happening. . Early on in the outbreak, the COVID-19 cases were primarily centered in China. Let&#39;s plot confirmed COVID-19 cases in China and the rest of the world separately to see if it gives us any insight. . We&#39;ll build on this plot in future tasks. One thing that will be important for the following tasks is that you add aesthetics within the line geometry of your ggplot, rather than making them global aesthetics. . confirmed_cases_china_vs_world &lt;- read_csv(&#39;datasets/confirmed_cases_china_vs_world.csv&#39;) # See the result confirmed_cases_china_vs_world # Draw a line plot of cumulative cases vs. date, grouped and colored by is_china # Define aesthetics within the line geom plt_cum_confirmed_cases_china_vs_world &lt;- ggplot(data=confirmed_cases_china_vs_world) + geom_line(aes(x=date, y=cum_cases, group=is_china, color=is_china)) + ylab(&quot;Cumulative confirmed cases&quot;) # See the plot plt_cum_confirmed_cases_china_vs_world . Parsed with column specification: cols( is_china = col_character(), date = col_date(format = &#34;&#34;), cases = col_double(), cum_cases = col_double() ) . A spec_tbl_df: 112 x 4 is_chinadatecasescum_cases . &lt;chr&gt;&lt;date&gt;&lt;dbl&gt;&lt;dbl&gt; . China | 2020-01-22 | 548 | 548 | . China | 2020-01-23 | 95 | 643 | . China | 2020-01-24 | 277 | 920 | . China | 2020-01-25 | 486 | 1406 | . China | 2020-01-26 | 669 | 2075 | . China | 2020-01-27 | 802 | 2877 | . China | 2020-01-28 | 2632 | 5509 | . China | 2020-01-29 | 578 | 6087 | . China | 2020-01-30 | 2054 | 8141 | . China | 2020-01-31 | 1661 | 9802 | . China | 2020-02-01 | 2089 | 11891 | . China | 2020-02-02 | 4739 | 16630 | . China | 2020-02-03 | 3086 | 19716 | . China | 2020-02-04 | 3991 | 23707 | . China | 2020-02-05 | 3733 | 27440 | . China | 2020-02-06 | 3147 | 30587 | . China | 2020-02-07 | 3523 | 34110 | . China | 2020-02-08 | 2704 | 36814 | . China | 2020-02-09 | 3015 | 39829 | . China | 2020-02-10 | 2525 | 42354 | . China | 2020-02-11 | 2032 | 44386 | . China | 2020-02-12 | 373 | 44759 | . China | 2020-02-13 | 15136 | 59895 | . China | 2020-02-14 | 6463 | 66358 | . China | 2020-02-15 | 2055 | 68413 | . China | 2020-02-16 | 2100 | 70513 | . China | 2020-02-17 | 1921 | 72434 | . China | 2020-02-18 | 1777 | 74211 | . China | 2020-02-19 | 408 | 74619 | . China | 2020-02-20 | 458 | 75077 | . ... | ... | ... | ... | . Not China | 2020-02-17 | 113 | 824 | . Not China | 2020-02-18 | 101 | 925 | . Not China | 2020-02-19 | 95 | 1020 | . Not China | 2020-02-20 | 100 | 1120 | . Not China | 2020-02-21 | 153 | 1273 | . Not China | 2020-02-22 | 305 | 1578 | . Not China | 2020-02-23 | 365 | 1943 | . Not China | 2020-02-24 | 384 | 2327 | . Not China | 2020-02-25 | 332 | 2659 | . Not China | 2020-02-26 | 570 | 3229 | . Not China | 2020-02-27 | 925 | 4154 | . Not China | 2020-02-28 | 1038 | 5192 | . Not China | 2020-02-29 | 1463 | 6655 | . Not China | 2020-03-01 | 1782 | 8437 | . Not China | 2020-03-02 | 1733 | 10170 | . Not China | 2020-03-03 | 2409 | 12579 | . Not China | 2020-03-04 | 2155 | 14734 | . Not China | 2020-03-05 | 2611 | 17345 | . Not China | 2020-03-06 | 3749 | 21094 | . Not China | 2020-03-07 | 3957 | 25051 | . Not China | 2020-03-08 | 3921 | 28972 | . Not China | 2020-03-09 | 3729 | 32701 | . Not China | 2020-03-10 | 5004 | 37705 | . Not China | 2020-03-11 | 7239 | 44944 | . Not China | 2020-03-12 | 2467 | 47411 | . Not China | 2020-03-13 | 16837 | 64248 | . Not China | 2020-03-14 | 10872 | 75120 | . Not China | 2020-03-15 | 11326 | 86446 | . Not China | 2020-03-16 | 14052 | 100498 | . Not China | 2020-03-17 | 15590 | 116088 | . 4. Let&#39;s annotate! . Wow! The two lines have very different shapes. In February, the majority of cases were in China. That changed in March when it really became a global outbreak: around March 14, the total number of cases outside China overtook the cases inside China. This was days after the WHO declared a pandemic. . There were a couple of other landmark events that happened during the outbreak. For example, the huge jump in the China line on February 13, 2020 wasn&#39;t just a bad day regarding the outbreak; China changed the way it reported figures on that day (CT scans were accepted as evidence for COVID-19, rather than only lab tests). . By annotating events like this, we can better interpret changes in the plot. . who_events &lt;- tribble( ~ date, ~ event, &quot;2020-01-30&quot;, &quot;Global health nemergency declared&quot;, &quot;2020-03-11&quot;, &quot;Pandemic ndeclared&quot;, &quot;2020-02-13&quot;, &quot;China reporting nchange&quot; ) %&gt;% mutate(date = as.Date(date)) # Using who_events, add vertical dashed lines with an xintercept at date # and text at date, labeled by event, and at 100000 on the y-axis plt_cum_confirmed_cases_china_vs_world + geom_vline(data=who_events, linetype=&#39;dashed&#39;, aes(xintercept=date)) + geom_text(data=who_events, y=1e+05, aes(x=date, label=event)) . 5. Adding a trend line to China . When trying to assess how big future problems are going to be, we need a measure of how fast the number of cases is growing. A good starting point is to see if the cases are growing faster or slower than linearly. . There is a clear surge of cases around February 13, 2020, with the reporting change in China. However, a couple of days after, the growth of cases in China slows down. How can we describe COVID-19&#39;s growth in China after February 15, 2020? . china_after_feb15 &lt;- filter(confirmed_cases_china_vs_world, date &gt;= &quot;2020-02-15&quot; &amp; is_china == &#39;China&#39;) # Using china_after_feb15, draw a line plot cum_cases vs. date # Add a smooth trend line using linear regression, no error bars ggplot(data=china_after_feb15, aes(x=date, y=cum_cases)) + geom_line() + geom_smooth(method=&#39;lm&#39;, se=FALSE) + ylab(&quot;Cumulative confirmed cases&quot;) . `geom_smooth()` using formula &#39;y ~ x&#39; . 6. And the rest of the world? . From the plot above, the growth rate in China is slower than linear. That&#39;s great news because it indicates China has at least somewhat contained the virus in late February and early March. . How does the rest of the world compare to linear growth? . not_china &lt;- filter(confirmed_cases_china_vs_world, is_china != &#39;China&#39;) # Using not_china, draw a line plot cum_cases vs. date # Add a smooth trend line using linear regression, no error bars plt_not_china_trend_lin &lt;- ggplot(data=not_china, aes(x=date, y=cum_cases)) + geom_line() + geom_smooth(method=&#39;lm&#39;, se=FALSE) + ylab(&quot;Cumulative confirmed cases&quot;) # See the result plt_not_china_trend_lin . `geom_smooth()` using formula &#39;y ~ x&#39; . 7. Adding a logarithmic scale . From the plot above, we can see a straight line does not fit well at all, and the rest of the world is growing much faster than linearly. What if we added a logarithmic scale to the y-axis? . plt_not_china_trend_lin + scale_y_log10(TRUE) . `geom_smooth()` using formula &#39;y ~ x&#39; . 8. Which countries outside of China have been hit hardest? . With the logarithmic scale, we get a much closer fit to the data. From a data science point of view, a good fit is great news. Unfortunately, from a public health point of view, that means that cases of COVID-19 in the rest of the world are growing at an exponential rate, which is terrible news. . Not all countries are being affected by COVID-19 equally, and it would be helpful to know where in the world the problems are greatest. Let&#39;s find the countries outside of China with the most confirmed cases in our dataset. . confirmed_cases_by_country &lt;- read_csv(&quot;datasets/confirmed_cases_by_country.csv&quot;) glimpse(confirmed_cases_by_country) # Group by country, summarize to calculate total cases, find the top 7 top_countries_by_total_cases &lt;- confirmed_cases_by_country %&gt;% group_by(country) %&gt;% summarise(confirmed_cases_by_country = max(cum_cases)) %&gt;% top_n(7) # See the result top_countries_by_total_cases . Parsed with column specification: cols( country = col_character(), province = col_character(), date = col_date(format = &#34;&#34;), cases = col_double(), cum_cases = col_double() ) . Observations: 13,272 Variables: 5 $ country &lt;chr&gt; &#34;Afghanistan&#34;, &#34;Albania&#34;, &#34;Algeria&#34;, &#34;Andorra&#34;, &#34;Antigua ... $ province &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N... $ date &lt;date&gt; 2020-01-22, 2020-01-22, 2020-01-22, 2020-01-22, 2020-01-... $ cases &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... $ cum_cases &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... . Selecting by confirmed_cases_by_country . A tibble: 7 x 2 countryconfirmed_cases_by_country . &lt;chr&gt;&lt;dbl&gt; . France | 7699 | . Germany | 9257 | . Iran | 16169 | . Italy | 31506 | . Korea, South | 8320 | . Spain | 11748 | . US | 6421 | . 9. Plotting hardest hit countries as of Mid-March 2020 . Even though the outbreak was first identified in China, there is only one country from East Asia (South Korea) in the above table. Four of the listed countries (France, Germany, Italy, and Spain) are in Europe and share borders. To get more context, we can plot these countries&#39; confirmed cases over time. . Finally, congratulations on getting to the last step! If you would like to continue making visualizations or find the hardest hit countries as of today, you can do your own analyses with the latest data available here. . confirmed_cases_top7_outside_china = read_csv(&#39;datasets/confirmed_cases_top7_outside_china.csv&#39;) # glimpse(confirmed_cases_top7_outside_china) # Using confirmed_cases_top7_outside_china, draw a line plot of # cum_cases vs. date, grouped and colored by country ggplot(data=confirmed_cases_top7_outside_china, aes(x=date, y=cum_cases)) + geom_line(aes(color=country, group=country)) + ylab(&quot;Cumulative confirmed cases&quot;) . Parsed with column specification: cols( country = col_character(), date = col_date(format = &#34;&#34;), cum_cases = col_double() ) . Observations: 2,030 Variables: 3 $ country &lt;chr&gt; &#34;Germany&#34;, &#34;Iran&#34;, &#34;Italy&#34;, &#34;Korea, South&#34;, &#34;Spain&#34;, &#34;US&#34;... $ date &lt;date&gt; 2020-02-18, 2020-02-18, 2020-02-18, 2020-02-18, 2020-02-... $ cum_cases &lt;dbl&gt; 16, 0, 3, 31, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, ... .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/r/2020/04/03/Visualizing-COVID-19.html",
            "relUrl": "/datacamp/projects/r/2020/04/03/Visualizing-COVID-19.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Exploring the Evolution of Linux",
            "content": "1. Introduction . . Version control repositories like CVS, Subversion or Git can be a real gold mine for software developers. They contain every change to the source code including the date (the &quot;when&quot;), the responsible developer (the &quot;who&quot;), as well as a little message that describes the intention (the &quot;what&quot;) of a change. . In this notebook, we will analyze the evolution of a very famous open-source project &ndash; the Linux kernel. The Linux kernel is the heart of some Linux distributions like Debian, Ubuntu or CentOS. Our dataset at hand contains the history of kernel development of almost 13 years (early 2005 - late 2017). We get some insights into the work of the development efforts by . identifying the TOP 10 contributors and | visualizing the commits over the years. | . import pandas as pd data = pd.read_csv(&#39;datasets/git_log_excerpt.csv&#39;) print(data) . 1502382966#Linus Torvalds 0 1501368308#Max Gurtovoy 1 1501625560#James Smart 2 1501625559#James Smart 3 1500568442#Martin Wilck 4 1502273719#Xin Long 5 1502278684#Nikolay Borisov 6 1502238384#Girish Moodalbail 7 1502228709#Florian Fainelli 8 1502223836#Jon Paul Maloy . 2. Reading in the dataset . The dataset was created by using the command git log --encoding=latin-1 --pretty=&quot;%at#%aN&quot; in late 2017. The latin-1 encoded text output was saved in a header-less CSV file. In this file, each row is a commit entry with the following information: . timestamp: the time of the commit as a UNIX timestamp in seconds since 1970-01-01 00:00:00 (Git log placeholder &quot;%at&quot;) | author: the name of the author that performed the commit (Git log placeholder &quot;%aN&quot;) | . The columns are separated by the number sign #. The complete dataset is in the datasets/ directory. It is a gz-compressed csv file named git_log.gz. . import pandas as pd # Reading in the log file git_log = pd.read_csv(&#39;datasets/git_log.gz&#39;, sep=&#39;#&#39;, encoding=&#39;latin-1&#39;, compression=&#39;gzip&#39;, header=None, names=[&#39;timestamp&#39;, &#39;author&#39;]) # Printing out the first 5 rows print(git_log.head()) . timestamp author 0 1502826583 Linus Torvalds 1 1501749089 Adrian Hunter 2 1501749088 Adrian Hunter 3 1501882480 Kees Cook 4 1497271395 Rob Clark . 3. Getting an overview . The dataset contains the information about every single code contribution (a &quot;commit&quot;) to the Linux kernel over the last 13 years. We&#39;ll first take a look at the number of authors and their commits to the repository. . number_of_commits = len(git_log[&#39;author&#39;]) # calculating number of authors not_null = (git_log[&#39;author&#39;].isna() == False) number_of_authors = len(pd.unique(git_log[&#39;author&#39;][not_null])) # printing out the results print(&quot;%s authors committed %s code changes.&quot; % (number_of_authors, number_of_commits)) . 17385 authors committed 699071 code changes. . 4. Finding the TOP 10 contributors . There are some very important people that changed the Linux kernel very often. To see if there are any bottlenecks, we take a look at the TOP 10 authors with the most commits. . top_10_authors = git_log.groupby(&#39;author&#39;).count().sort_values(&#39;timestamp&#39;, ascending=False).head(10) # Listing contents of &#39;top_10_authors&#39; top_10_authors . timestamp . author . Linus Torvalds 23361 | . David S. Miller 9106 | . Mark Brown 6802 | . Takashi Iwai 6209 | . Al Viro 6006 | . H Hartley Sweeten 5938 | . Ingo Molnar 5344 | . Mauro Carvalho Chehab 5204 | . Arnd Bergmann 4890 | . Greg Kroah-Hartman 4580 | . 5. Wrangling the data . For our analysis, we want to visualize the contributions over time. For this, we use the information in the timestamp column to create a time series-based column. . git_log[&#39;timestamp&#39;] = pd.to_datetime(git_log[&#39;timestamp&#39;], unit=&#39;s&#39;) # summarizing the converted timestamp column git_log[[&#39;timestamp&#39;]].describe() . timestamp . count 699071 | . unique 668448 | . top 2008-09-04 05:30:19 | . freq 99 | . first 1970-01-01 00:00:01 | . last 2037-04-25 08:08:26 | . 6. Treating wrong timestamps . As we can see from the results above, some contributors had their operating system&#39;s time incorrectly set when they committed to the repository. We&#39;ll clean up the timestamp column by dropping the rows with the incorrect timestamps. . first_commit_timestamp = git_log[git_log[&#39;author&#39;] == &#39;Linus Torvalds&#39;][&#39;timestamp&#39;].min() # determining the last sensible commit timestamp last_commit_timestamp = pd.to_datetime(&#39;today&#39;) # filtering out wrong timestamps corrected_log = git_log[git_log[&#39;timestamp&#39;] &gt;= first_commit_timestamp][git_log[&#39;timestamp&#39;] &lt;= last_commit_timestamp] # summarizing the corrected timestamp column corrected_log[&#39;timestamp&#39;].describe() . count 698569 unique 667977 top 2008-09-04 05:30:19 freq 99 first 2005-04-16 22:20:36 last 2017-10-03 12:57:00 Name: timestamp, dtype: object . 7. Grouping commits per year . To find out how the development activity has increased over time, we&#39;ll group the commits by year and count them up. . commits_per_year = corrected_log.groupby(pd.Grouper(key=&#39;timestamp&#39;, freq=&#39;AS&#39;)).count() # Listing the first rows commits_per_year.head(5) . author . timestamp . 2005-01-01 16229 | . 2006-01-01 29255 | . 2007-01-01 33759 | . 2008-01-01 48847 | . 2009-01-01 52572 | . 8. Visualizing the history of Linux . Finally, we&#39;ll make a plot out of these counts to better see how the development effort on Linux has increased over the the last few years. . %matplotlib inline # plot the data commits_per_year.plot(kind=&#39;line&#39;, title=&#39;Visual&#39;, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa586a039b0&gt; . 9. Conclusion . Thanks to the solid foundation and caretaking of Linux Torvalds, many other developers are now able to contribute to the Linux kernel as well. There is no decrease of development activity at sight! . year_with_most_commits = commits_per_year[commits_per_year[&#39;author&#39;] == max(commits_per_year[&#39;author&#39;])] year_with_most_commits.index[0] . Timestamp(&#39;2016-01-01 00:00:00&#39;, freq=&#39;AS-JAN&#39;) .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2020/03/02/Exploring-the-Evolution-of-Linux.html",
            "relUrl": "/datacamp/projects/python/2020/03/02/Exploring-the-Evolution-of-Linux.html",
            "date": " • Mar 2, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Introduction to DataCamp Projects(R)",
            "content": "1. This is a Jupyter notebook! . A Jupyter notebook is a document that contains text cells (what you&#39;re reading right now) and code cells. What is special with a notebook is that it&#39;s interactive: You can change or add code cells, and then run a cell by first selecting it and then clicking the run cell button above ( ▶| Run ) or hitting ctrl + enter. . . The result will be displayed directly in the notebook. You could use a notebook as a simple calculator. For example, it&#39;s estimated that on average 256 children were born every minute in 2016. The code cell below calculates how many children were born on average on a day. . 256 * 60 * 24 # Children × minutes × hours . 368640 2. Put any code in code cells . But a code cell can contain much more than a simple one-liner! This is a notebook running R and you can put any R code in a code cell (but notebooks can run other languages too, like python). Below is a code cell where we define a whole new function (greet). To show the output of greet we can run it anywhere and the result is always printed out at the end of the code cell. . greet &lt;- function(first_name, last_name) { paste(&quot;My name is &quot;, last_name, &quot;, &quot;, first_name, &quot; &quot;, last_name, &quot;!&quot;, sep = &quot;&quot;) } # Replace with your first and last name. # That is, unless your name is already James Bond. greet(&quot;James&quot;, &quot;Bond&quot;) . &#39;My name is Bond, James Bond!&#39; 3. Jupyter notebooks &#9825; data . We&#39;ve seen that notebooks can display basic objects such as numbers and strings. But notebooks also support the objects used in data science, which makes them great for interactive data analysis! . For example, below we create a data frame by reading in a csv-file with the average global temperature for the years 1850 to 2016. If we look at the head of this data frame the notebook will render it as a nice-looking table. . global_temp &lt;- read.csv(&quot;datasets/global_temperature.csv&quot;) # Take a look at the first datapoints global_temp[1,] . A data.frame: 1 x 2 yeardegrees_celsius . &lt;int&gt;&lt;dbl&gt; . 1850 | 7.74 | . 4. Jupyter notebooks &#9825; plots . Tables are nice but — as the saying goes — &quot;a plot can show a thousand data points&quot;. Notebooks handle plots as well and all plots created in code cells will automatically be displayed inline. . Let&#39;s take a look at the global temperature for the last 150 years. . plot(global_temp$year, global_temp$degrees_celsius, type = &quot;l&quot;, col = &quot;forestgreen&quot;, xlab = &quot;Year&quot;, ylab = &quot;Celsius&quot;) . 5. Jupyter notebooks &#9825; Data Science . Tables and plots are the most common outputs when doing data science and, as these outputs are rendered inline, notebooks works great not only for doing a data analysis but also for showing a data analysis. A finished notebook contains both the result and the code that produced it. This is useful when you want to share your findings or if you need to update your analysis with new data. . Let&#39;s add some advanced data analysis to our notebook! For example, this (slightly complicated) code forecasts the global temperature 50 years into the future using an exponential smoothing state space model (ets). . Note: Global temperature is a complex phenomenon and exponential smoothing is likely not a good model here. This is just an example of how easy it is to do (and show) complex forecasting in a Jupyter notebook. . library(forecast) library(ggplot2) # Converting global_temp into a time series (ts) object. global_temp_ts &lt;- ts(global_temp$degrees_celsius, start = global_temp$year[1]) # Forecasting global temperature 50 years into the future # using an exponential smoothing state space model (ets). temperature_forecast &lt;- forecast( ets(global_temp_ts), h = 50) # Plotting the forecast plot(temperature_forecast, type=&#39;l&#39;) . 6. Goodbye for now! . This was just a short introduction to Jupyter notebooks, an open source technology that is increasingly used for data science and analysis. I hope you enjoyed it! :) . I_am_ready &lt;- TRUE # Ps. # Feel free to try out any other stuff in this notebook. # It&#39;s all yours! .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/r/2020/03/01/Introduction-to-DataCamp-Projects.html",
            "relUrl": "/datacamp/projects/r/2020/03/01/Introduction-to-DataCamp-Projects.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "A Network Analysis of Game of Thrones",
            "content": "1. Winter is Coming. Let&#39;s load the dataset ASAP! . If you haven&#39;t heard of Game of Thrones, then you must be really good at hiding. Game of Thrones is the hugely popular television series by HBO based on the (also) hugely popular book series A Song of Ice and Fire by George R.R. Martin. In this notebook, we will analyze the co-occurrence network of the characters in the Game of Thrones books. Here, two characters are considered to co-occur if their names appear in the vicinity of 15 words from one another in the books. . . This dataset constitutes a network and is given as a text file describing the edges between characters, with some attributes attached to each edge. Let&#39;s start by loading in the data for the first book A Game of Thrones and inspect it. . import pandas as pd # Reading in datasets/book1.csv book1 = pd.read_csv(&#39;datasets/book1.csv&#39;) # Printing out the head of the dataset book1.head() . Source Target Type weight book . 0 Addam-Marbrand | Jaime-Lannister | Undirected | 3 | 1 | . 1 Addam-Marbrand | Tywin-Lannister | Undirected | 6 | 1 | . 2 Aegon-I-Targaryen | Daenerys-Targaryen | Undirected | 5 | 1 | . 3 Aegon-I-Targaryen | Eddard-Stark | Undirected | 4 | 1 | . 4 Aemon-Targaryen-(Maester-Aemon) | Alliser-Thorne | Undirected | 4 | 1 | . 2. Time for some Network of Thrones . The resulting DataFrame book1 has 5 columns: Source, Target, Type, weight, and book. Source and target are the two nodes that are linked by an edge. A network can have directed or undirected edges and in this network all the edges are undirected. The weight attribute of every edge tells us the number of interactions that the characters have had over the book, and the book column tells us the book number. . Once we have the data loaded as a pandas DataFrame, it&#39;s time to create a network. We will use networkx, a network analysis library, and create a graph object for the first book. . import networkx as nx # Creating an empty graph object G_book1 = nx.Graph() . 3. Populate the network with the DataFrame . Currently, the graph object G_book1 is empty. Let&#39;s now populate it with the edges from book1. And while we&#39;re at it, let&#39;s load in the rest of the books too! . for i, se in book1.iterrows(): G_book1.add_edge(se[&#39;Source&#39;], se[&#39;Target&#39;], weights=se[&#39;weight&#39;]) # Creating a list of networks for all the books books = [G_book1] book_fnames = [&#39;datasets/book2.csv&#39;, &#39;datasets/book3.csv&#39;, &#39;datasets/book4.csv&#39;, &#39;datasets/book5.csv&#39;] for book_fname in book_fnames: book = pd.read_csv(book_fname) G_book = nx.Graph() for _, edge in book.iterrows(): G_book.add_edge(edge[&#39;Source&#39;], edge[&#39;Target&#39;], weight=edge[&#39;weight&#39;]) books.append(G_book) . 4. The most important character in Game of Thrones . Is it Jon Snow, Tyrion, Daenerys, or someone else? Let&#39;s see! Network science offers us many different metrics to measure the importance of a node in a network. Note that there is no &quot;correct&quot; way of calculating the most important node in a network, every metric has a different meaning. . First, let&#39;s measure the importance of a node in a network by looking at the number of neighbors it has, that is, the number of nodes it is connected to. For example, an influential account on Twitter, where the follower-followee relationship forms the network, is an account which has a high number of followers. This measure of importance is called degree centrality. . Using this measure, let&#39;s extract the top ten important characters from the first book (book[0]) and the fifth book (book[4]). . deg_cen_book1 = nx.degree_centrality(books[0]) # Calculating the degree centrality of book 5 deg_cen_book5 = nx.degree_centrality(books[4]) # Sorting the dictionaries according to their degree centrality and storing the top 10 sorted_deg_cen_book1 = sorted(deg_cen_book1.items(), key = lambda x:x[1], reverse=True)[:10] # Sorting the dictionaries according to their degree centrality and storing the top 10 sorted_deg_cen_book5 = sorted(deg_cen_book5.items(), key = lambda x:x[1], reverse=True)[:10] # Printing out the top 10 of book1 and book5 print(sorted_deg_cen_book1) print(sorted_deg_cen_book5) . [(&#39;Eddard-Stark&#39;, 0.3548387096774194), (&#39;Robert-Baratheon&#39;, 0.2688172043010753), (&#39;Tyrion-Lannister&#39;, 0.24731182795698928), (&#39;Catelyn-Stark&#39;, 0.23118279569892475), (&#39;Jon-Snow&#39;, 0.19892473118279572), (&#39;Robb-Stark&#39;, 0.18817204301075272), (&#39;Sansa-Stark&#39;, 0.18817204301075272), (&#39;Bran-Stark&#39;, 0.17204301075268819), (&#39;Cersei-Lannister&#39;, 0.16129032258064518), (&#39;Joffrey-Baratheon&#39;, 0.16129032258064518)] [(&#39;Jon-Snow&#39;, 0.1962025316455696), (&#39;Daenerys-Targaryen&#39;, 0.18354430379746836), (&#39;Stannis-Baratheon&#39;, 0.14873417721518986), (&#39;Tyrion-Lannister&#39;, 0.10443037974683544), (&#39;Theon-Greyjoy&#39;, 0.10443037974683544), (&#39;Cersei-Lannister&#39;, 0.08860759493670886), (&#39;Barristan-Selmy&#39;, 0.07911392405063292), (&#39;Hizdahr-zo-Loraq&#39;, 0.06962025316455696), (&#39;Asha-Greyjoy&#39;, 0.056962025316455694), (&#39;Melisandre&#39;, 0.05379746835443038)] . 5. The evolution of character importance . According to degree centrality, the most important character in the first book is Eddard Stark but he is not even in the top 10 of the fifth book. The importance of characters changes over the course of five books because, you know, stuff happens... ;) . Let&#39;s look at the evolution of degree centrality of a couple of characters like Eddard Stark, Jon Snow, and Tyrion, which showed up in the top 10 of degree centrality in the first book. . %matplotlib inline # Creating a list of degree centrality of all the books evol = [nx.degree_centrality(book) for book in books] # Creating a DataFrame from the list of degree centralities in all the books degree_evol_df = pd.DataFrame.from_records(evol) # Plotting the degree centrality evolution of Eddard-Stark, Tyrion-Lannister and Jon-Snow degree_evol_df[[&#39;Eddard-Stark&#39;, &#39;Tyrion-Lannister&#39;, &#39;Jon-Snow&#39;]].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3e61994c50&gt; . 6. What&#39;s up with Stannis Baratheon? . We can see that the importance of Eddard Stark dies off as the book series progresses. With Jon Snow, there is a drop in the fourth book but a sudden rise in the fifth book. . Now let&#39;s look at various other measures like betweenness centrality and PageRank to find important characters in our Game of Thrones character co-occurrence network and see if we can uncover some more interesting facts about this network. Let&#39;s plot the evolution of betweenness centrality of this network over the five books. We will take the evolution of the top four characters of every book and plot it. . evol = [nx.betweenness_centrality(book, weight=&#39;weight&#39;) for book in books] # Making a DataFrame from the list betweenness_evol_df = pd.DataFrame.from_records(evol).fillna(0) # Finding the top 4 characters in every book set_of_char = set() for i in range(5): set_of_char |= set(list(betweenness_evol_df.T[i].sort_values(ascending=False)[0:4].index)) list_of_char = list(set_of_char) # Plotting the evolution of the top characters betweenness_evol_df[list_of_char].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3e95616278&gt; . 7. What does Google PageRank tell us about GoT? . We see a peculiar rise in the importance of Stannis Baratheon over the books. In the fifth book, he is significantly more important than other characters in the network, even though he is the third most important character according to degree centrality. . PageRank was the initial way Google ranked web pages. It evaluates the inlinks and outlinks of webpages in the world wide web, which is, essentially, a directed network. Let&#39;s look at the importance of characters in the Game of Thrones network according to PageRank. . evol = [nx.pagerank(book, weight=&#39;weight&#39;) for book in books] # Making a DataFrame from the list pagerank_evol_df = pd.DataFrame.from_records(evol).fillna(0) # Finding the top 4 characters in every book set_of_char = set() for i in range(5): set_of_char |= set(list(pagerank_evol_df.T[i].sort_values(ascending=False)[0:4].index)) list_of_char = list(set_of_char) # Plotting the top characters pagerank_evol_df[list_of_char].plot(figsize=(13, 7)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3e64796b70&gt; . 8. Correlation between different measures . Stannis, Jon Snow, and Daenerys are the most important characters in the fifth book according to PageRank. Eddard Stark follows a similar curve but for degree centrality and betweenness centrality: He is important in the first book but dies into oblivion over the book series. . We have seen three different measures to calculate the importance of a node in a network, and all of them tells us something about the characters and their importance in the co-occurrence network. We see some names pop up in all three measures so maybe there is a strong correlation between them? . Let&#39;s look at the correlation between PageRank, betweenness centrality and degree centrality for the fifth book using Pearson correlation. . # of all the characters in the fifth book. measures = [nx.pagerank(books[4]), nx.betweenness_centrality(books[4], weight=&#39;weight&#39;), nx.degree_centrality(books[4])] # Creating the correlation DataFrame cor = pd.DataFrame.from_records(measures) # Calculating the correlation cor.corr() . Aegon-I-Targaryen Aegon-Targaryen-(son-of-Rhaegar) Aemon-Targaryen-(Maester-Aemon) Aenys-Frey Aeron-Greyjoy Aerys-II-Targaryen Aggo Alliser-Thorne Alys-Karstark Alysane-Mormont ... Wun-Weg-Wun-Dar-Wun Wylis-Manderly Wyman-Manderly Xaro-Xhoan-Daxos Yandry Yellow-Dick Yezzan-zo-Qaggaz Ygritte Ysilla Yurkhaz-zo-Yunzak . Aegon-I-Targaryen 1.000000 | 0.278893 | 0.882815 | 0.999467 | 0.320632 | 0.999048 | 0.971752 | 0.997956 | 0.999952 | 0.998448 | ... | 0.999555 | 0.999994 | -0.008652 | 0.999886 | 0.999258 | 0.995381 | 0.999977 | 0.999249 | 0.938126 | 1.000000 | . Aegon-Targaryen-(son-of-Rhaegar) 0.278893 | 1.000000 | 0.697294 | 0.310094 | 0.999043 | 0.320525 | 0.497653 | 0.216958 | 0.269464 | 0.224971 | ... | 0.307418 | 0.275465 | 0.957873 | 0.264389 | 0.315685 | 0.369802 | 0.285400 | 0.241484 | 0.594191 | 0.278893 | . Aemon-Targaryen-(Maester-Aemon) 0.882815 | 0.697294 | 1.000000 | 0.897679 | 0.727980 | 0.902468 | 0.968733 | 0.850996 | 0.878167 | 0.855281 | ... | 0.896435 | 0.881133 | 0.462065 | 0.875636 | 0.900257 | 0.923834 | 0.885981 | 0.863957 | 0.990853 | 0.882815 | . Aenys-Frey 0.999467 | 0.310094 | 0.897679 | 1.000000 | 0.351383 | 0.999940 | 0.978939 | 0.995338 | 0.999099 | 0.996097 | ... | 0.999996 | 0.999344 | 0.023997 | 0.998862 | 0.999983 | 0.997984 | 0.999665 | 0.997452 | 0.948931 | 0.999467 | . Aeron-Greyjoy 0.320632 | 0.999043 | 0.727980 | 0.351383 | 1.000000 | 0.361652 | 0.535117 | 0.259450 | 0.311330 | 0.267376 | ... | 0.348747 | 0.317250 | 0.944394 | 0.306321 | 0.356887 | 0.410089 | 0.327049 | 0.283700 | 0.628804 | 0.320632 | . Aerys-II-Targaryen 0.999048 | 0.320525 | 0.902468 | 0.999940 | 0.361652 | 1.000000 | 0.981124 | 0.994218 | 0.998572 | 0.995067 | ... | 0.999905 | 0.998886 | 0.034984 | 0.998277 | 0.999987 | 0.998622 | 0.999321 | 0.996608 | 0.952341 | 0.999048 | . Aggo 0.971752 | 0.497653 | 0.968733 | 0.978939 | 0.535117 | 0.981124 | 1.000000 | 0.954686 | 0.969392 | 0.957099 | ... | 0.978361 | 0.970904 | 0.227586 | 0.968085 | 0.980124 | 0.989921 | 0.973331 | 0.961881 | 0.993353 | 0.971752 | . Alliser-Thorne 0.997956 | 0.216958 | 0.850996 | 0.995338 | 0.259450 | 0.994218 | 0.954686 | 1.000000 | 0.998535 | 0.999966 | ... | 0.995606 | 0.998178 | -0.072532 | 0.998806 | 0.994753 | 0.987212 | 0.997500 | 0.999683 | 0.914080 | 0.997956 | . Alys-Karstark 0.999952 | 0.269464 | 0.878167 | 0.999099 | 0.311330 | 0.998572 | 0.969392 | 0.998535 | 1.000000 | 0.998946 | ... | 0.999214 | 0.999981 | -0.018455 | 0.999986 | 0.998832 | 0.994392 | 0.999862 | 0.999581 | 0.934686 | 0.999952 | . Alysane-Mormont 0.998448 | 0.224971 | 0.855281 | 0.996097 | 0.267376 | 0.995067 | 0.957099 | 0.999966 | 0.998946 | 1.000000 | ... | 0.996342 | 0.998640 | -0.064336 | 0.999174 | 0.995560 | 0.988488 | 0.998047 | 0.999856 | 0.917381 | 0.998448 | . Archibald-Yronwood 0.999945 | 0.268800 | 0.877837 | 0.999069 | 0.310674 | 0.998535 | 0.969222 | 0.998572 | 1.000000 | 0.998977 | ... | 0.999187 | 0.999976 | -0.019145 | 0.999990 | 0.998798 | 0.994318 | 0.999851 | 0.999601 | 0.934440 | 0.999945 | . Areo-Hotah 0.999389 | 0.312281 | 0.898690 | 0.999997 | 0.353536 | 0.999962 | 0.979406 | 0.995114 | 0.998999 | 0.995891 | ... | 0.999987 | 0.999258 | 0.026297 | 0.998749 | 0.999994 | 0.998128 | 0.999603 | 0.997286 | 0.949654 | 0.999389 | . Arianne-Martell 0.997788 | 0.342119 | 0.912090 | 0.999426 | 0.382894 | 0.999738 | 0.985292 | 0.991500 | 0.997088 | 0.992536 | ... | 0.999327 | 0.997544 | 0.057846 | 0.996672 | 0.999608 | 0.999561 | 0.998216 | 0.994464 | 0.959073 | 0.997788 | . Arnolf-Karstark 0.152395 | 0.991607 | 0.598771 | 0.184578 | 0.985003 | 0.195370 | 0.381337 | 0.088930 | 0.142698 | 0.097110 | ... | 0.181812 | 0.148868 | 0.986964 | 0.137484 | 0.190360 | 0.246577 | 0.159096 | 0.113997 | 0.485215 | 0.152395 | . Arron 0.999597 | 0.306032 | 0.895789 | 0.999991 | 0.347382 | 0.999884 | 0.978058 | 0.995741 | 0.999271 | 0.996465 | ... | 0.999999 | 0.999490 | 0.019728 | 0.999056 | 0.999948 | 0.997704 | 0.999767 | 0.997748 | 0.947575 | 0.999597 | . Arthor-Karstark 0.997776 | 0.214260 | 0.849541 | 0.995068 | 0.256781 | 0.993918 | 0.953860 | 0.999996 | 0.998382 | 0.999940 | ... | 0.995343 | 0.998007 | -0.075287 | 0.998667 | 0.994467 | 0.986767 | 0.997301 | 0.999609 | 0.912956 | 0.997776 | . Arya-Stark 0.481352 | 0.975994 | 0.836668 | 0.509710 | 0.984587 | 0.519136 | 0.674618 | 0.424358 | 0.472736 | 0.431783 | ... | 0.507288 | 0.478222 | 0.872330 | 0.468088 | 0.514766 | 0.563282 | 0.487287 | 0.447038 | 0.755105 | 0.481352 | . Arys-Oakheart 0.999861 | 0.294844 | 0.890514 | 0.999872 | 0.336359 | 0.999636 | 0.975547 | 0.996754 | 0.999650 | 0.997382 | ... | 0.999913 | 0.999796 | 0.007999 | 0.999497 | 0.999761 | 0.996841 | 0.999951 | 0.998466 | 0.943762 | 0.999861 | . Asha-Greyjoy -0.446205 | 0.734978 | 0.026451 | -0.416752 | 0.704614 | -0.406735 | -0.222395 | -0.502479 | -0.454958 | -0.495359 | ... | -0.419308 | -0.449395 | 0.898758 | -0.459641 | -0.411394 | -0.358224 | -0.440125 | -0.480536 | -0.108687 | -0.446205 | . Ashara-Dayne 0.998672 | 0.229056 | 0.857448 | 0.996459 | 0.271416 | 0.995474 | 0.958306 | 0.999923 | 0.999130 | 0.999991 | ... | 0.996691 | 0.998850 | -0.060148 | 0.999335 | 0.995946 | 0.989114 | 0.998300 | 0.999918 | 0.919043 | 0.998672 | . Axell-Florent -0.506689 | 0.686610 | -0.042352 | -0.478274 | 0.654152 | -0.468592 | -0.288911 | -0.560743 | -0.515117 | -0.553922 | ... | -0.480743 | -0.509761 | 0.866481 | -0.519624 | -0.473097 | -0.421577 | -0.500829 | -0.539704 | -0.176787 | -0.506689 | . Azor-Ahai 1.000000 | 0.278072 | 0.882413 | 0.999439 | 0.319822 | 0.999010 | 0.971550 | 0.998011 | 0.999960 | 0.998495 | ... | 0.999529 | 0.999996 | -0.009507 | 0.999899 | 0.999224 | 0.995298 | 0.999971 | 0.999282 | 0.937830 | 1.000000 | . Balon-Greyjoy 0.381983 | 0.994033 | 0.771322 | 0.411949 | 0.997853 | 0.421940 | 0.589299 | 0.322148 | 0.372904 | 0.329914 | ... | 0.409384 | 0.378683 | 0.920830 | 0.368012 | 0.417305 | 0.468945 | 0.388243 | 0.345897 | 0.678383 | 0.381983 | . Balon-Swann 0.926722 | 0.619294 | 0.994621 | 0.938495 | 0.653045 | 0.942233 | 0.989222 | 0.900818 | 0.922994 | 0.904355 | ... | 0.937520 | 0.925376 | 0.367715 | 0.920954 | 0.940511 | 0.958516 | 0.929250 | 0.911472 | 0.999501 | 0.926722 | . Barbrey-Dustin -0.028012 | 0.952133 | 0.444807 | 0.004635 | 0.937851 | 0.015626 | 0.208689 | -0.091830 | -0.037811 | -0.083646 | ... | 0.001822 | -0.031579 | 0.999813 | -0.043074 | 0.010521 | 0.068087 | -0.021231 | -0.066712 | 0.319879 | -0.028012 | . Barristan-Selmy 0.253308 | 0.999648 | 0.678025 | 0.284754 | 0.997530 | 0.295273 | 0.474458 | 0.190974 | 0.243811 | 0.199032 | ... | 0.282055 | 0.249855 | 0.965158 | 0.238700 | 0.290391 | 0.345014 | 0.259864 | 0.215645 | 0.572635 | 0.253308 | . Barsena 1.000000 | 0.278893 | 0.882815 | 0.999467 | 0.320632 | 0.999048 | 0.971752 | 0.997956 | 0.999952 | 0.998448 | ... | 0.999555 | 0.999994 | -0.008652 | 0.999886 | 0.999258 | 0.995381 | 0.999977 | 0.999249 | 0.938126 | 1.000000 | . Bartimus 0.999126 | 0.238517 | 0.862414 | 0.997230 | 0.280768 | 0.996352 | 0.961041 | 0.999755 | 0.999488 | 0.999903 | ... | 0.997435 | 0.999269 | -0.050433 | 0.999643 | 0.996774 | 0.990499 | 0.998820 | 0.999995 | 0.922835 | 0.999126 | . Belaquo 0.993139 | 0.389280 | 0.931688 | 0.996427 | 0.429199 | 0.997295 | 0.992683 | 0.983637 | 0.991945 | 0.985084 | ... | 0.996186 | 0.992715 | 0.108344 | 0.991264 | 0.996907 | 0.999778 | 0.993909 | 0.987864 | 0.972185 | 0.993139 | . Belwas 0.994771 | 0.375514 | 0.926172 | 0.997575 | 0.415695 | 0.998280 | 0.990774 | 0.986212 | 0.993722 | 0.987538 | ... | 0.997375 | 0.994400 | 0.093522 | 0.993119 | 0.997967 | 0.999981 | 0.995441 | 0.990068 | 0.968588 | 0.994771 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Tywin-Lannister -0.454044 | 0.728998 | 0.017674 | -0.424716 | 0.698358 | -0.414739 | -0.230945 | -0.510050 | -0.462758 | -0.502966 | ... | -0.427261 | -0.457220 | 0.894874 | -0.467420 | -0.419380 | -0.366406 | -0.447990 | -0.488217 | -0.117410 | -0.454044 | . Ulmer 0.999995 | 0.281800 | 0.884234 | 0.999561 | 0.323499 | 0.999175 | 0.972463 | 0.997758 | 0.999918 | 0.998274 | ... | 0.999641 | 0.999978 | -0.005623 | 0.999836 | 0.999370 | 0.995667 | 0.999993 | 0.999128 | 0.939171 | 0.999995 | . Unella 0.992156 | 0.156657 | 0.817171 | 0.987546 | 0.199709 | 0.985757 | 0.934628 | 0.998116 | 0.993334 | 0.997578 | ... | 0.987985 | 0.992595 | -0.133587 | 0.993927 | 0.986603 | 0.975571 | 0.991285 | 0.996253 | 0.887478 | 0.992156 | . Val 0.392030 | 0.992785 | 0.778211 | 0.421853 | 0.997080 | 0.431793 | 0.598067 | 0.332444 | 0.382992 | 0.340181 | ... | 0.419301 | 0.388745 | 0.916526 | 0.378121 | 0.427183 | 0.478541 | 0.398261 | 0.356100 | 0.686348 | 0.392030 | . Varamyr -0.149098 | 0.908006 | 0.332844 | -0.116738 | 0.888811 | -0.105815 | 0.088478 | -0.211980 | -0.158786 | -0.203944 | ... | -0.119532 | -0.152626 | 0.990075 | -0.163983 | -0.110891 | -0.053476 | -0.142388 | -0.187290 | 0.202550 | -0.149098 | . Varys 0.377108 | 0.994594 | 0.767958 | 0.407143 | 0.998184 | 0.417157 | 0.585034 | 0.317155 | 0.368010 | 0.324936 | ... | 0.404571 | 0.373801 | 0.922872 | 0.363108 | 0.412511 | 0.464285 | 0.383382 | 0.340949 | 0.674502 | 0.377108 | . Victarion-Greyjoy 0.322654 | 0.998947 | 0.729443 | 0.353382 | 0.999998 | 0.363643 | 0.536921 | 0.261513 | 0.313359 | 0.269433 | ... | 0.350748 | 0.319275 | 0.943690 | 0.308353 | 0.358882 | 0.412037 | 0.329067 | 0.285748 | 0.630464 | 0.322654 | . Viserys-Targaryen 0.565611 | 0.949696 | 0.886696 | 0.592231 | 0.962486 | 0.601052 | 0.744259 | 0.511758 | 0.557498 | 0.518799 | ... | 0.589962 | 0.562665 | 0.819748 | 0.553119 | 0.596964 | 0.642173 | 0.571192 | 0.533242 | 0.816194 | 0.565611 | . Waif 0.999395 | 0.245321 | 0.865943 | 0.997727 | 0.287491 | 0.996926 | 0.962956 | 0.999575 | 0.999688 | 0.999781 | ... | 0.997912 | 0.999513 | -0.043428 | 0.999806 | 0.997313 | 0.991439 | 0.999136 | 0.999992 | 0.925513 | 0.999395 | . Walda-Frey-(daughter-of-Merrett) 0.981860 | 0.455916 | 0.955863 | 0.987527 | 0.494411 | 0.989198 | 0.998873 | 0.967738 | 0.979954 | 0.969775 | ... | 0.987080 | 0.981178 | 0.181104 | 0.978891 | 0.988436 | 0.995528 | 0.983124 | 0.973779 | 0.986768 | 0.981860 | . Walder-Frey-(son-of-Jammos) 0.997255 | 0.207021 | 0.845612 | 0.994306 | 0.249617 | 0.993075 | 0.951610 | 0.999948 | 0.997933 | 0.999831 | ... | 0.994602 | 0.997513 | -0.082669 | 0.998258 | 0.993662 | 0.985539 | 0.996730 | 0.999375 | 0.909910 | 0.997255 | . Walder-Frey-(son-of-Merrett) 0.999998 | 0.280636 | 0.883666 | 0.999525 | 0.322351 | 0.999125 | 0.972179 | 0.997839 | 0.999932 | 0.998345 | ... | 0.999607 | 0.999986 | -0.006836 | 0.999857 | 0.999326 | 0.995553 | 0.999988 | 0.999177 | 0.938753 | 0.999998 | . Weeper 0.644369 | 0.914082 | 0.928061 | 0.668990 | 0.930946 | 0.677119 | 0.806642 | 0.594187 | 0.636840 | 0.600775 | ... | 0.666896 | 0.641636 | 0.759111 | 0.632771 | 0.673353 | 0.714810 | 0.649541 | 0.614263 | 0.869316 | 0.644369 | . Wex-Pyke 1.000000 | 0.279155 | 0.882943 | 0.999476 | 0.320891 | 0.999060 | 0.971817 | 0.997939 | 0.999949 | 0.998432 | ... | 0.999563 | 0.999993 | -0.008379 | 0.999882 | 0.999268 | 0.995407 | 0.999979 | 0.999239 | 0.938221 | 1.000000 | . Wick-Whittlestick 0.997391 | 0.347496 | 0.914423 | 0.999216 | 0.388179 | 0.999591 | 0.986255 | 0.990739 | 0.996635 | 0.991821 | ... | 0.999100 | 0.997127 | 0.063563 | 0.996189 | 0.999432 | 0.999714 | 0.997857 | 0.993845 | 0.960679 | 0.997391 | . Widower 0.999793 | 0.298377 | 0.892191 | 0.999924 | 0.339840 | 0.999729 | 0.976354 | 0.996449 | 0.999545 | 0.997107 | ... | 0.999955 | 0.999714 | 0.011698 | 0.999373 | 0.999835 | 0.997128 | 0.999908 | 0.998254 | 0.944978 | 0.999793 | . Willam-Dustin -0.409763 | 0.761718 | 0.066731 | -0.379766 | 0.732649 | -0.369575 | -0.182908 | -0.467215 | -0.418687 | -0.459935 | ... | -0.382367 | -0.413015 | 0.915703 | -0.423464 | -0.374314 | -0.320293 | -0.403566 | -0.444790 | -0.068522 | -0.409763 | . William-Foxglove 1.000000 | 0.278072 | 0.882413 | 0.999439 | 0.319822 | 0.999010 | 0.971550 | 0.998011 | 0.999960 | 0.998495 | ... | 0.999529 | 0.999996 | -0.009507 | 0.999899 | 0.999224 | 0.995298 | 0.999971 | 0.999282 | 0.937830 | 1.000000 | . Willow-Witch-eye 0.999742 | 0.256999 | 0.871913 | 0.998467 | 0.299025 | 0.997798 | 0.966139 | 0.999151 | 0.999917 | 0.999455 | ... | 0.998619 | 0.999817 | -0.031372 | 0.999971 | 0.998124 | 0.992942 | 0.999565 | 0.999872 | 0.930015 | 0.999742 | . Wulfe 0.999190 | 0.240022 | 0.863198 | 0.997344 | 0.282255 | 0.996483 | 0.961468 | 0.999719 | 0.999536 | 0.999880 | ... | 0.997545 | 0.999327 | -0.048885 | 0.999683 | 0.996898 | 0.990711 | 0.998894 | 0.999999 | 0.923431 | 0.999190 | . Wun-Weg-Wun-Dar-Wun 0.999555 | 0.307418 | 0.896435 | 0.999996 | 0.348747 | 0.999905 | 0.978361 | 0.995606 | 0.999214 | 0.996342 | ... | 1.000000 | 0.999442 | 0.021184 | 0.998992 | 0.999962 | 0.997802 | 0.999734 | 0.997649 | 0.948039 | 0.999555 | . Wylis-Manderly 0.999994 | 0.275465 | 0.881133 | 0.999344 | 0.317250 | 0.998886 | 0.970904 | 0.998178 | 0.999981 | 0.998640 | ... | 0.999442 | 1.000000 | -0.012220 | 0.999934 | 0.999114 | 0.995032 | 0.999946 | 0.999381 | 0.936884 | 0.999994 | . Wyman-Manderly -0.008652 | 0.957873 | 0.462065 | 0.023997 | 0.944394 | 0.034984 | 0.227586 | -0.072532 | -0.018455 | -0.064336 | ... | 0.021184 | -0.012220 | 1.000000 | -0.023721 | 0.029881 | 0.087392 | -0.001869 | -0.047380 | 0.338165 | -0.008652 | . Xaro-Xhoan-Daxos 0.999886 | 0.264389 | 0.875636 | 0.998862 | 0.306321 | 0.998277 | 0.968085 | 0.998806 | 0.999986 | 0.999174 | ... | 0.998992 | 0.999934 | -0.023721 | 1.000000 | 0.998563 | 0.993821 | 0.999761 | 0.999720 | 0.932801 | 0.999886 | . Yandry 0.999258 | 0.315685 | 0.900257 | 0.999983 | 0.356887 | 0.999987 | 0.980124 | 0.994753 | 0.998832 | 0.995560 | ... | 0.999962 | 0.999114 | 0.029881 | 0.998563 | 1.000000 | 0.998341 | 0.999496 | 0.997015 | 0.950771 | 0.999258 | . Yellow-Dick 0.995381 | 0.369802 | 0.923834 | 0.997984 | 0.410089 | 0.998622 | 0.989921 | 0.987212 | 0.994392 | 0.988488 | ... | 0.997802 | 0.995032 | 0.087392 | 0.993821 | 0.998341 | 1.000000 | 0.996009 | 0.990915 | 0.967039 | 0.995381 | . Yezzan-zo-Qaggaz 0.999977 | 0.285400 | 0.885981 | 0.999665 | 0.327049 | 0.999321 | 0.973331 | 0.997500 | 0.999862 | 0.998047 | ... | 0.999734 | 0.999946 | -0.001869 | 0.999761 | 0.999496 | 0.996009 | 1.000000 | 0.998964 | 0.940453 | 0.999977 | . Ygritte 0.999249 | 0.241484 | 0.863957 | 0.997452 | 0.283700 | 0.996608 | 0.961881 | 0.999683 | 0.999581 | 0.999856 | ... | 0.997649 | 0.999381 | -0.047380 | 0.999720 | 0.997015 | 0.990915 | 0.998964 | 1.000000 | 0.924008 | 0.999249 | . Ysilla 0.938126 | 0.594191 | 0.990853 | 0.948931 | 0.628804 | 0.952341 | 0.993353 | 0.914080 | 0.934686 | 0.917381 | ... | 0.948039 | 0.936884 | 0.338165 | 0.932801 | 0.950771 | 0.967039 | 0.940453 | 0.924008 | 1.000000 | 0.938126 | . Yurkhaz-zo-Yunzak 1.000000 | 0.278893 | 0.882815 | 0.999467 | 0.320632 | 0.999048 | 0.971752 | 0.997956 | 0.999952 | 0.998448 | ... | 0.999555 | 0.999994 | -0.008652 | 0.999886 | 0.999258 | 0.995381 | 0.999977 | 0.999249 | 0.938126 | 1.000000 | . 317 rows × 317 columns . 9. Conclusion . We see a high correlation between these three measures for our character co-occurrence network. . So we&#39;ve been looking at different ways to find the important characters in the Game of Thrones co-occurrence network. According to degree centrality, Eddard Stark is the most important character initially in the books. But who is/are the most important character(s) in the fifth book according to these three measures? . # according to degree centrality, betweenness centrality and pagerank. p_rank, b_cent, d_cent = cor.idxmax(axis=1) # Printing out the top character accoding to the three measures print(p_rank) print(b_rank) print(d_rank) . Jon-Snow . NameError Traceback (most recent call last) &lt;ipython-input-255-a08ddd23025c&gt; in &lt;module&gt;() 5 # Printing out the top character accoding to the three measures 6 print(p_rank) -&gt; 7 print(b_rank) 8 print(d_rank) NameError: name &#39;b_rank&#39; is not defined .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2020/02/25/A-Network-Analysis-of-Game-of-Thrones.html",
            "relUrl": "/datacamp/projects/python/2020/02/25/A-Network-Analysis-of-Game-of-Thrones.html",
            "date": " • Feb 25, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Introduction to DataCamp Projects (Python)",
            "content": "1. This is a Jupyter notebook! . A Jupyter notebook is a document that contains text cells (what you&#39;re reading right now) and code cells. What is special with a notebook is that it&#39;s interactive: You can change or add code cells, and then run a cell by first selecting it and then clicking the run cell button above ( ▶| Run ) or hitting ctrl + enter. . . The result will be displayed directly in the notebook. You could use a notebook as a simple calculator. For example, it&#39;s estimated that on average 256 children were born every minute in 2016. The code cell below calculates how many children were born on average on a day. . 256 * 60 * 24 # Children × minutes × hours . 368640 . 2. Put any code in code cells . But a code cell can contain much more than a simple one-liner! This is a notebook running python and you can put any python code in a code cell (but notebooks can run other languages too, like R). Below is a code cell where we define a whole new function (greet). To show the output of greet we run it last in the code cell as the last value is always printed out. . def greet(first_name, last_name): greeting = &#39;My name is &#39; + last_name + &#39;, &#39; + first_name + &#39; &#39; + last_name + &#39;!&#39; return greeting # Replace with your first and last name. # That is, unless your name is already James Bond. greet(&#39;Anurag&#39;, &#39;Peddi&#39;) . &#39;My name is Peddi, Anurag Peddi!&#39; . 3. Jupyter notebooks &#9825; data . We&#39;ve seen that notebooks can display basic objects such as numbers and strings. But notebooks also support the objects used in data science, which makes them great for interactive data analysis! . For example, below we create a pandas DataFrame by reading in a csv-file with the average global temperature for the years 1850 to 2016. If we look at the head of this DataFrame the notebook will render it as a nice-looking table. . import pandas as pd # Reading in the global temperature data global_temp = pd.read_csv(&#39;datasets/global_temperature.csv&#39;) # Take a look at the first datapoints # ... YOUR CODE FOR TASK 3 ... print(global_temp.head()) . year degrees_celsius 0 1850 7.74 1 1851 8.09 2 1852 7.97 3 1853 7.93 4 1854 8.19 . 4. Jupyter notebooks &#9825; plots . Tables are nice but — as the saying goes — &quot;a plot can show a thousand data points&quot;. Notebooks handle plots as well, but it requires a bit of magic. Here magic does not refer to any arcane rituals but to so-called &quot;magic commands&quot; that affect how the Jupyter notebook works. Magic commands start with either % or %% and the command we need to nicely display plots inline is %matplotlib inline. With this magic in place, all plots created in code cells will automatically be displayed inline. . Let&#39;s take a look at the global temperature for the last 150 years. . %matplotlib inline import matplotlib.pyplot as plt # Plotting global temperature in degrees celsius by year plt.plot(global_temp[&#39;year&#39;], global_temp[&#39;degrees_celsius&#39;]) # Adding some nice labels plt.xlabel(&#39;Year&#39;) plt.ylabel(&#39;Degree Celsius&#39;) . &lt;matplotlib.text.Text at 0x7fc6bdfd1d30&gt; . 5. Jupyter notebooks &#9825; a lot more . Tables and plots are the most common outputs when doing data analysis, but Jupyter notebooks can render many more types of outputs such as sound, animation, video, etc. Yes, almost anything that can be shown in a modern web browser. This also makes it possible to include interactive widgets directly in the notebook! . For example, this (slightly complicated) code will create an interactive map showing the locations of the three largest smartphone companies in 2016. You can move and zoom the map, and you can click the markers for more info! . import folium phone_map = folium.Map() # Top three smart phone companies by market share in 2016 companies = [ {&#39;loc&#39;: [37.4970, 127.0266], &#39;label&#39;: &#39;Samsung: ...%&#39;}, {&#39;loc&#39;: [37.3318, -122.0311], &#39;label&#39;: &#39;Apple: ...%&#39;}, {&#39;loc&#39;: [22.5431, 114.0579], &#39;label&#39;: &#39;Huawei: ...%&#39;}] # Adding markers to the map for company in companies: marker = folium.Marker(location=company[&#39;loc&#39;], popup=company[&#39;label&#39;]) marker.add_to(phone_map) # The last object in the cell always gets shown in the notebook phone_map . 6. Goodbye for now! . This was just a short introduction to Jupyter notebooks, an open source technology that is increasingly used for data science and analysis. I hope you enjoyed it! :) . I_am_ready = False # Ps. # Feel free to try out any other stuff in this notebook. # It&#39;s all yours! .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2020/01/29/Introduction-to-DataCamp-Projects.html",
            "relUrl": "/datacamp/projects/python/2020/01/29/Introduction-to-DataCamp-Projects.html",
            "date": " • Jan 29, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Exploring the Bitcoin Cryptocurrency Market",
            "content": "1. Bitcoin. Cryptocurrencies. So hot right now. . Since the launch of Bitcoin in 2008, hundreds of similar projects based on the blockchain technology have emerged. We call these cryptocurrencies (also coins or cryptos in the Internet slang). Some are extremely valuable nowadays, and others may have the potential to become extremely valuable in the future1. In fact, on the 6th of December of 2017, Bitcoin has a market capitalization above $200 billion. . The astonishing increase of Bitcoin market capitalization in 2017. . *1 WARNING: The cryptocurrency market is exceptionally volatile and any money you put in might disappear into thin air. Cryptocurrencies mentioned here might be scams similar to Ponzi Schemes or have many other issues (overvaluation, technical, etc.). Please do not mistake this for investment advice. * . That said, let&#39;s get to business. As a first task, we will load the current data from the coinmarketcap API and display it in the output. . import requests # Importing pandas import pandas as pd # Importing matplotlib and setting aesthetics for plotting later. import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = &#39;svg&#39; plt.style.use(&#39;fivethirtyeight&#39;) # Getting the data data = requests.get(&quot;https://api.coinmarketcap.com/v1/ticker/&quot;) # Reading in current data from coinmarketcap.com current = pd.DataFrame(data.json()) # Printing out the first few lines # ... YOUR CODE FOR TASK 1 ... current.head() . 24h_volume_usd available_supply id last_updated market_cap_usd max_supply name percent_change_1h percent_change_24h percent_change_7d price_btc price_usd rank symbol total_supply . 0 21766312573.7 | 18180900.0 | bitcoin | 1579958614 | 151521615980 | 21000000.0 | Bitcoin | 0.03 | -1.13 | -6.29 | 1.0 | 8334.10975142 | 1 | BTC | 18180900.0 | . 1 9129485280.35 | 109423844.0 | ethereum | 1579958601 | 17547851782.0 | None | Ethereum | 0.37 | -0.59 | -6.89 | 0.01924797 | 160.365886634 | 2 | ETH | 109423844.0 | . 2 1481529835.06 | 43675903665.0 | xrp | 1579958645 | 9612916998.0 | 100000000000 | XRP | 0.23 | -1.39 | -7.94 | 0.00002642 | 0.2200965794 | 3 | XRP | 99991104396.0 | . 3 2448790511.0 | 18242200.0 | bitcoin-cash | 1579958648 | 5650036276.0 | 21000000.0 | Bitcoin Cash | -0.01 | -3.55 | -13.5 | 0.03717499 | 309.723403741 | 4 | BCH | 18242200.0 | . 4 2131454629.84 | 18222577.0 | bitcoin-sv | 1579958651 | 4722454400.0 | 21000000.0 | Bitcoin SV | -1.01 | -6.5 | -0.88 | 0.03110533 | 259.154035229 | 5 | BSV | 18222577.0 | . 2. Full dataset, filtering, and reproducibility . The previous API call returns only the first 100 coins, and we want to explore as many coins as possible. Moreover, we can&#39;t produce reproducible analysis with live online data. To solve these problems, we will load a CSV we conveniently saved on the 6th of December of 2017 using the API call https://api.coinmarketcap.com/v1/ticker/?limit=0 named datasets/coinmarketcap_06122017.csv. . dec6 = pd.read_csv(&#39;datasets/coinmarketcap_06122017.csv&#39;) # Selecting the &#39;id&#39; and the &#39;market_cap_usd&#39; columns market_cap_raw = dec6[[&#39;id&#39;, &#39;market_cap_usd&#39;]] # Counting the number of values # ... YOUR CODE FOR TASK 2 ... print(market_cap_raw.count()) . id 1326 market_cap_usd 1031 dtype: int64 . 3. Discard the cryptocurrencies without a market capitalization . Why do the count() for id and market_cap_usd differ above? It is because some cryptocurrencies listed in coinmarketcap.com have no known market capitalization, this is represented by NaN in the data, and NaNs are not counted by count(). These cryptocurrencies are of little interest to us in this analysis, so they are safe to remove. . cap = market_cap_raw.query(&#39;market_cap_usd &gt; 0&#39;) # Counting the number of values again # ... YOUR CODE FOR TASK 3 ... print(cap.count()) . id 1031 market_cap_usd 1031 dtype: int64 . 4. How big is Bitcoin compared with the rest of the cryptocurrencies? . At the time of writing, Bitcoin is under serious competition from other projects, but it is still dominant in market capitalization. Let&#39;s plot the market capitalization for the top 10 coins as a barplot to better visualize this. . TOP_CAP_TITLE = &#39;Top 10 market capitalization&#39; TOP_CAP_YLABEL = &#39;% of total cap&#39; # Selecting the first 10 rows and setting the index cap10 = cap.iloc[:10, :] cap10 = cap10.set_index([&#39;id&#39;], drop=True) # Calculating market_cap_perc cap10 = cap10.assign(market_cap_perc=lambda x: 100 * x[&#39;market_cap_usd&#39;] / cap[&#39;market_cap_usd&#39;].sum()) # Plotting the barplot with the title defined above ax = cap10.plot.bar(title=TOP_CAP_TITLE) # Annotating the y axis with the label defined above # ... YOUR CODE FOR TASK 4 ... ax.set_ylabel(TOP_CAP_YLABEL) . Text(0,0.5,&#39;% of total cap&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 5. Making the plot easier to read and more informative . While the plot above is informative enough, it can be improved. Bitcoin is too big, and the other coins are hard to distinguish because of this. Instead of the percentage, let&#39;s use a log10 scale of the &quot;raw&quot; capitalization. Plus, let&#39;s use color to group similar coins and make the plot more informative1. . For the colors rationale: bitcoin-cash and bitcoin-gold are forks of the bitcoin blockchain2. Ethereum and Cardano both offer Turing Complete smart contracts. Iota and Ripple are not minable. Dash, Litecoin, and Monero get their own color. . 1 This coloring is a simplification. There are more differences and similarities that are not being represented here. . 2 The bitcoin forks are actually very different, but it is out of scope to talk about them here. Please see the warning above and do your own research. . COLORS = [&#39;orange&#39;, &#39;green&#39;, &#39;orange&#39;, &#39;cyan&#39;, &#39;cyan&#39;, &#39;blue&#39;, &#39;silver&#39;, &#39;orange&#39;, &#39;red&#39;, &#39;green&#39;] # Plotting market_cap_usd as before but adding the colors and scaling the y-axis ax = cap.plot(kind=&#39;bar&#39;, title=&#39;Top 10 market capitalization&#39;, logy=True) # Annotating the y axis with &#39;USD&#39; # ... YOUR CODE FOR TASK 5 ... ax.set_ylabel(&#39;USD&#39;) # Final touch! Removing the xlabel as it is not very informative # ... YOUR CODE FOR TASK 5 ... ax.get_xaxis().set_visible(False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 6. What is going on?! Volatility in cryptocurrencies . The cryptocurrencies market has been spectacularly volatile since the first exchange opened. This notebook didn&#39;t start with a big, bold warning for nothing. Let&#39;s explore this volatility a bit more! We will begin by selecting and plotting the 24 hours and 7 days percentage change, which we already have available. . volatility = dec6[[&#39;id&#39;, &#39;percent_change_24h&#39;, &#39;percent_change_7d&#39;]] # Setting the index to &#39;id&#39; and dropping all NaN rows volatility = volatility.set_index(&#39;id&#39;).dropna() # Sorting the DataFrame by percent_change_24h in ascending order volatility = volatility.sort_values(&#39;percent_change_24h&#39;) # Checking the first few rows # ... YOUR CODE FOR TASK 6 ... print(volatility.head()) . percent_change_24h percent_change_7d id flappycoin -95.85 -96.61 credence-coin -94.22 -95.31 coupecoin -93.93 -61.24 tyrocoin -79.02 -87.43 petrodollar -76.55 542.96 . 7. Well, we can already see that things are a bit crazy . It seems you can lose a lot of money quickly on cryptocurrencies. Let&#39;s plot the top 10 biggest gainers and top 10 losers in market capitalization. . def top10_subplot(volatility_series, title): # Making the subplot and the figure for two side by side plots fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 6)) # Plotting with pandas the barchart for the top 10 losers volatility_series.head(10).plot(kind=&#39;bar&#39;, ax=axes[0]) # Setting the figure&#39;s main title to the text passed as parameter # ... YOUR CODE FOR TASK 7 ... fig.suptitle(title) # Setting the ylabel to &#39;% change&#39; # ... YOUR CODE FOR TASK 7 ... axes[0].set_ylabel(&#39;% change&#39;) axes[1].set_ylabel(&#39;% change&#39;) # Same as above, but for the top 10 winners volatility_series[-10:].plot(kind=&#39;bar&#39;, ax=axes[1]) # Returning this for good practice, might use later return fig, ax DTITLE = &quot;24 hours top losers and winners&quot; # Calling the function above with the 24 hours period series and title DTITLE fig, ax = top10_subplot(volatility, DTITLE) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 8. Ok, those are... interesting. Let&#39;s check the weekly Series too. . 800% daily increase?! Why are we doing this tutorial and not buying random coins?1 . After calming down, let&#39;s reuse the function defined above to see what is going weekly instead of daily. . 1 Please take a moment to understand the implications of the red plots on how much value some cryptocurrencies lose in such short periods of time . volatility7d = volatility.sort_values(&#39;percent_change_7d&#39;) WTITLE = &quot;Weekly top losers and winners&quot; # Calling the top10_subplot function fig, ax = top10_subplot(volatility7d[&#39;percent_change_7d&#39;], WTITLE) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 9. How small is small? . The names of the cryptocurrencies above are quite unknown, and there is a considerable fluctuation between the 1 and 7 days percentage changes. As with stocks, and many other financial products, the smaller the capitalization, the bigger the risk and reward. Smaller cryptocurrencies are less stable projects in general, and therefore even riskier investments than the bigger ones1. Let&#39;s classify our dataset based on Investopedia&#39;s capitalization definitions for company stocks. . 1 Cryptocurrencies are a new asset class, so they are not directly comparable to stocks. Furthermore, there are no limits set in stone for what a &quot;small&quot; or &quot;large&quot; stock is. Finally, some investors argue that bitcoin is similar to gold, this would make them more comparable to a commodity instead. . largecaps = cap.query(&#39;market_cap_usd &gt; 10**10&#39;) # Printing out largecaps # ... YOUR CODE FOR TASK 9 ... print(largecaps) . id market_cap_usd 0 bitcoin 2.130493e+11 1 ethereum 4.352945e+10 2 bitcoin-cash 2.529585e+10 3 iota 1.475225e+10 . 10. Most coins are tiny . Note that many coins are not comparable to large companies in market cap, so let&#39;s divert from the original Investopedia definition by merging categories. . This is all for now. Thanks for completing this project! . # &quot;cap&quot; DataFrame. Returns an int. # INSTRUCTORS NOTE: Since you made it to the end, consider it a gift :D def capcount(query_string): return cap.query(query_string).count().id # Labels for the plot LABELS = [&quot;biggish&quot;, &quot;micro&quot;, &quot;nano&quot;] # Using capcount count the biggish cryptos biggish = capcount(&#39;market_cap_usd &gt; 3*10**8&#39;) # Same as above for micro ... micro = capcount(&#39;market_cap_usd &lt; 3*10**8 and market_cap_usd &gt; 5*10**7&#39;) # ... and for nano nano = capcount(&#39;market_cap_usd &lt; 5*10**7&#39;) # Making a list with the 3 counts values = [biggish, micro, nano] # Plotting them with matplotlib # ... YOUR CODE FOR TASK 10 ... plt.bar(values) . TypeError Traceback (most recent call last) &lt;ipython-input-83-c482fdf68251&gt; in &lt;module&gt;() 22 # Plotting them with matplotlib 23 # ... YOUR CODE FOR TASK 10 ... &gt; 24 plt.bar(values) /usr/local/lib/python3.5/dist-packages/matplotlib/pyplot.py in bar(*args, **kwargs) 2646 mplDeprecation) 2647 try: -&gt; 2648 ret = ax.bar(*args, **kwargs) 2649 finally: 2650 ax._hold = washold /usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py in inner(ax, *args, **kwargs) 1715 warnings.warn(msg % (label_namer, func.__name__), 1716 RuntimeWarning, stacklevel=2) -&gt; 1717 return func(ax, *args, **kwargs) 1718 pre_doc = inner.__doc__ 1719 if pre_doc is None: /usr/local/lib/python3.5/dist-packages/matplotlib/axes/_axes.py in bar(self, *args, **kwargs) 1963 break 1964 else: -&gt; 1965 raise exps[0] 1966 # if we matched the second-case, then the user passed in 1967 # left=val as a kwarg which we want to deprecate /usr/local/lib/python3.5/dist-packages/matplotlib/axes/_axes.py in bar(self, *args, **kwargs) 1955 for matcher in matchers: 1956 try: -&gt; 1957 dp, x, height, width, y, kwargs = matcher(*args, **kwargs) 1958 except TypeError as e: 1959 # This can only come from a no-match as there is TypeError: &lt;lambda&gt;() missing 1 required positional argument: &#39;height&#39; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt;",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2020/01/29/Exploring-the-Bitcoin-Cryptocurrency-Market.html",
            "relUrl": "/datacamp/projects/python/2020/01/29/Exploring-the-Bitcoin-Cryptocurrency-Market.html",
            "date": " • Jan 29, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Exploring 67 years of LEGO",
            "content": "1. Introduction . Everyone loves Lego (unless you ever stepped on one). Did you know by the way that &quot;Lego&quot; was derived from the Danish phrase leg godt, which means &quot;play well&quot;? Unless you speak Danish, probably not. . In this project, we will analyze a fascinating dataset on every single lego block that has ever been built! . . . 2. Reading Data . A comprehensive database of lego blocks is provided by Rebrickable. The data is available as csv files and the schema is shown below. . . Let us start by reading in the colors data to get a sense of the diversity of lego sets! . import pandas as pd # Read colors data colors = pd.read_csv(&#39;datasets/colors.csv&#39;) # Print the first few rows colors.head() . id name rgb is_trans . 0 -1 | Unknown | 0033B2 | f | . 1 0 | Black | 05131D | f | . 2 1 | Blue | 0055BF | f | . 3 2 | Green | 237841 | f | . 4 3 | Dark Turquoise | 008F9B | f | . 3. Exploring Colors . Now that we have read the colors data, we can start exploring it! Let us start by understanding the number of colors available. . # -- YOUR CODE FOR TASK 3 -- num_colors = len(pd.unique(colors[&#39;name&#39;])) . 4. Transparent Colors in Lego Sets . The colors data has a column named is_trans that indicates whether a color is transparent or not. It would be interesting to explore the distribution of transparent vs. non-transparent colors. . # -- YOUR CODE FOR TASK 4 -- colors_summary = colors.groupby(&#39;is_trans&#39;).count() print(colors_summary) . id name rgb is_trans f 107 107 107 t 28 28 28 . 5. Explore Lego Sets . Another interesting dataset available in this database is the sets data. It contains a comprehensive list of sets over the years and the number of parts that each of these sets contained. . . Let us use this data to explore how the average number of parts in Lego sets has varied over the years. . %matplotlib inline import matplotlib.pyplot as plt sets = pd.read_csv(&#39;datasets/sets.csv&#39;) # Create a summary of average number of parts by year: `parts_by_year` parts_by_year = sets[[&#39;year&#39;, &#39;num_parts&#39;]].groupby(&#39;year&#39;).mean() # print(parts_by_year) # Plot trends in average number of parts by year plt.plot(parts_by_year.index, parts_by_year[&#39;num_parts&#39;]) . [&lt;matplotlib.lines.Line2D at 0x7eff4d91aa90&gt;] . 6. Lego Themes Over Years . Lego blocks ship under multiple themes. Let us try to get a sense of how the number of themes shipped has varied over the years. . # -- YOUR CODE HERE -- tmp = sets[[&#39;year&#39;, &#39;theme_id&#39;]].groupby(&#39;year&#39;).count() c1 = tmp.index c2 = pd.Series(tmp[&#39;theme_id&#39;]).tolist() themes_by_year = pd.DataFrame({&#39;year&#39;:c1}) themes_by_year[&#39;theme_id&#39;] = c2 print(themes_by_year) . year theme_id 0 1950 7 1 1953 4 2 1954 14 3 1955 28 4 1956 12 5 1957 21 6 1958 42 7 1959 4 8 1960 3 9 1961 17 10 1962 40 11 1963 18 12 1964 11 13 1965 10 14 1966 89 15 1967 21 16 1968 25 17 1969 69 18 1970 29 19 1971 45 20 1972 38 21 1973 68 22 1974 39 23 1975 31 24 1976 68 25 1977 92 26 1978 73 27 1979 82 28 1980 88 29 1981 79 .. ... ... 36 1988 68 37 1989 114 38 1990 85 39 1991 106 40 1992 115 41 1993 111 42 1994 128 43 1995 128 44 1996 144 45 1997 194 46 1998 325 47 1999 300 48 2000 327 49 2001 339 50 2002 447 51 2003 415 52 2004 371 53 2005 330 54 2006 283 55 2007 319 56 2008 349 57 2009 403 58 2010 444 59 2011 502 60 2012 615 61 2013 593 62 2014 715 63 2015 670 64 2016 609 65 2017 470 [66 rows x 2 columns] . 7. Wrapping It All Up! . Lego blocks offer an unlimited amount of fun across ages. We explored some interesting trends around colors, parts, and themes. . .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2020/01/29/Exploring-67-years-of-LEGO.html",
            "relUrl": "/datacamp/projects/python/2020/01/29/Exploring-67-years-of-LEGO.html",
            "date": " • Jan 29, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Dr. Semmelweis and the Discovery of Handwashing",
            "content": "1. Meet Dr. Ignaz Semmelweis . This is Dr. Ignaz Semmelweis, a Hungarian physician born in 1818 and active at the Vienna General Hospital. If Dr. Semmelweis looks troubled it&#39;s probably because he&#39;s thinking about childbed fever: A deadly disease affecting women that just have given birth. He is thinking about it because in the early 1840s at the Vienna General Hospital as many as 10% of the women giving birth die from it. He is thinking about it because he knows the cause of childbed fever: It&#39;s the contaminated hands of the doctors delivering the babies. And they won&#39;t listen to him and wash their hands! . In this notebook, we&#39;re going to reanalyze the data that made Semmelweis discover the importance of handwashing. Let&#39;s start by looking at the data that made Semmelweis realize that something was wrong with the procedures at Vienna General Hospital. . # ... YOUR CODE FOR TASK 1 ... import pandas as pd from pandas import Series, DataFrame # Read datasets/yearly_deaths_by_clinic.csv into yearly yearly = pd.read_csv(&#39;datasets/yearly_deaths_by_clinic.csv&#39;) # Print out yearly # ... YOUR CODE FOR TASK 1 ... print(yearly) . year births deaths clinic 0 1841 3036 237 clinic 1 1 1842 3287 518 clinic 1 2 1843 3060 274 clinic 1 3 1844 3157 260 clinic 1 4 1845 3492 241 clinic 1 5 1846 4010 459 clinic 1 6 1841 2442 86 clinic 2 7 1842 2659 202 clinic 2 8 1843 2739 164 clinic 2 9 1844 2956 68 clinic 2 10 1845 3241 66 clinic 2 11 1846 3754 105 clinic 2 . 2. The alarming number of deaths . The table above shows the number of women giving birth at the two clinics at the Vienna General Hospital for the years 1841 to 1846. You&#39;ll notice that giving birth was very dangerous; an alarming number of women died as the result of childbirth, most of them from childbed fever. . We see this more clearly if we look at the proportion of deaths out of the number of women giving birth. Let&#39;s zoom in on the proportion of deaths at Clinic 1. . # ... YOUR CODE FOR TASK 2 ... yearly[&#39;proportion_deaths&#39;] = yearly[&#39;deaths&#39;]/yearly[&#39;births&#39;] # Extract clinic 1 data into yearly1 and clinic 2 data into yearly2 yearly1 = yearly[(yearly[&#39;clinic&#39;] == &#39;clinic 1&#39;)] yearly2 = yearly[(yearly[&#39;clinic&#39;] == &#39;clinic 2&#39;)] # Print out yearly1 # ... YOUR CODE FOR TASK 2 ... print(yearly1) . year births deaths clinic proportion_deaths 0 1841 3036 237 clinic 1 0.078063 1 1842 3287 518 clinic 1 0.157591 2 1843 3060 274 clinic 1 0.089542 3 1844 3157 260 clinic 1 0.082357 4 1845 3492 241 clinic 1 0.069015 5 1846 4010 459 clinic 1 0.114464 . 3. Death at the clinics . If we now plot the proportion of deaths at both clinic 1 and clinic 2 we&#39;ll see a curious pattern... . %matplotlib inline # Plot yearly proportion of deaths at the two clinics # ... YOUR CODE FOR TASK 3 ... ax = yearly1.plot(x=&#39;year&#39;, y=&#39;proportion_deaths&#39;, label=&#39;yearly1&#39;) yearly2.plot(x=&#39;year&#39;, y=&#39;proportion_deaths&#39;, label=&#39;yearly2&#39;, ax=ax) # ax = yearly[[&#39;proportion_deaths&#39;, &#39;year&#39;]].plot(label=&#39;label&#39;) ax.set_ylabel(&#39;Proportion deaths&#39;) . &lt;matplotlib.text.Text at 0x7fead1fa8f28&gt; . 4. The handwashing begins . Why is the proportion of deaths constantly so much higher in Clinic 1? Semmelweis saw the same pattern and was puzzled and distressed. The only difference between the clinics was that many medical students served at Clinic 1, while mostly midwife students served at Clinic 2. While the midwives only tended to the women giving birth, the medical students also spent time in the autopsy rooms examining corpses. . Semmelweis started to suspect that something on the corpses, spread from the hands of the medical students, caused childbed fever. So in a desperate attempt to stop the high mortality rates, he decreed: Wash your hands! This was an unorthodox and controversial request, nobody in Vienna knew about bacteria at this point in time. . Let&#39;s load in monthly data from Clinic 1 to see if the handwashing had any effect. . monthly = pd.read_csv(&#39;datasets/monthly_deaths.csv&#39;, parse_dates=[&#39;date&#39;]) # monthly[&#39;date&#39;] = pd.to_datetime(monthly[&#39;date&#39;]) # Amother method # Calculate proportion of deaths per no. births # ... YOUR CODE FOR TASK 4 ... monthly[&#39;proportion_deaths&#39;] = monthly[&#39;deaths&#39;]/monthly[&#39;births&#39;] # Print out the first rows in monthly # ... YOUR CODE FOR TASK 4 ... monthly.head() . date births deaths proportion_deaths . 0 1841-01-01 | 254 | 37 | 0.145669 | . 1 1841-02-01 | 239 | 18 | 0.075314 | . 2 1841-03-01 | 277 | 12 | 0.043321 | . 3 1841-04-01 | 255 | 4 | 0.015686 | . 4 1841-05-01 | 255 | 2 | 0.007843 | . 5. The effect of handwashing . With the data loaded we can now look at the proportion of deaths over time. In the plot below we haven&#39;t marked where obligatory handwashing started, but it reduced the proportion of deaths to such a degree that you should be able to spot it! . # ... YOUR CODE FOR TASK 5 ... ax = monthly.plot(x=&#39;date&#39;, y=&#39;proportion_deaths&#39;) ax.set_ylabel(&#39;Proportion_deaths&#39;) . &lt;matplotlib.text.Text at 0x7fead1f5dc50&gt; . 6. The effect of handwashing highlighted . Starting from the summer of 1847 the proportion of deaths is drastically reduced and, yes, this was when Semmelweis made handwashing obligatory. . The effect of handwashing is made even more clear if we highlight this in the graph. . import pandas as pd handwashing_start = pd.to_datetime(&#39;1847-06-01&#39;) # Split monthly into before and after handwashing_start before_washing = monthly[(monthly[&#39;date&#39;] &lt; handwashing_start)] after_washing = monthly[(monthly[&#39;date&#39;] &gt;= handwashing_start)] # Plot monthly proportion of deaths before and after handwashing # ... YOUR CODE FOR TASK 6 ... ax = before_washing.plot(x=&#39;date&#39;, y=&#39;proportion_deaths&#39;, label=&#39;before_washing&#39;) after_washing.plot(x=&#39;date&#39;, y=&#39;proportion_deaths&#39;, label=&#39;after_washing&#39;, ax=ax) ax.set_ylabel(&#39;Proportion deaths&#39;) . &lt;matplotlib.text.Text at 0x7fead1882f28&gt; . 7. More handwashing, fewer deaths? . Again, the graph shows that handwashing had a huge effect. How much did it reduce the monthly proportion of deaths on average? . before_proportion = before_washing[&#39;proportion_deaths&#39;] after_proportion = after_washing[&#39;proportion_deaths&#39;] mean_diff = pd.np.mean(after_proportion) - pd.np.mean(before_proportion) mean_diff . -0.08395660751183336 . 8. A Bootstrap analysis of Semmelweis handwashing data . It reduced the proportion of deaths by around 8 percentage points! From 10% on average to just 2% (which is still a high number by modern standards). . To get a feeling for the uncertainty around how much handwashing reduces mortalities we could look at a confidence interval (here calculated using the bootstrap method). . boot_mean_diff = [] for i in range(3000): boot_before = before_proportion.sample(frac=1, replace=True) boot_after = after_proportion.sample(frac=1, replace=True) boot_mean_diff.append(pd.np.mean(boot_after) - pd.np.mean(boot_before) ) # Calculating a 95% confidence interval from boot_mean_diff confidence_interval = ... confidence_interval . Ellipsis . 9. The fate of Dr. Semmelweis . So handwashing reduced the proportion of deaths by between 6.7 and 10 percentage points, according to a 95% confidence interval. All in all, it would seem that Semmelweis had solid evidence that handwashing was a simple but highly effective procedure that could save many lives. . The tragedy is that, despite the evidence, Semmelweis&#39; theory — that childbed fever was caused by some &quot;substance&quot; (what we today know as bacteria) from autopsy room corpses — was ridiculed by contemporary scientists. The medical community largely rejected his discovery and in 1849 he was forced to leave the Vienna General Hospital for good. . One reason for this was that statistics and statistical arguments were uncommon in medical science in the 1800s. Semmelweis only published his data as long tables of raw data, but he didn&#39;t show any graphs nor confidence intervals. If he would have had access to the analysis we&#39;ve just put together he might have been more successful in getting the Viennese doctors to wash their hands. . doctors_should_wash_their_hands = True .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/python/2020/01/29/Dr.-Semmelweis-and-the-Discovery-of-Handwashing.html",
            "relUrl": "/datacamp/projects/python/2020/01/29/Dr.-Semmelweis-and-the-Discovery-of-Handwashing.html",
            "date": " • Jan 29, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Introduction to DataCamp Projects(SQL)",
            "content": "1. This is a Jupyter Notebook! . A Jupyter Notebook is a document that contains text cells (what you&#39;re reading right now) and code cells. What is special with a notebook is that it&#39;s interactive: You can change or add code cells, and then run a cell by first selecting it and then clicking the run cell button above ( ▶| Run ) or hitting Ctrl + Enter. . . The result will be displayed directly in the notebook. You could use a notebook as a simple calculator. For example, it&#39;s estimated that on average 256 children were born every minute in 2016. The code cell below calculates how many children were born on average on a day. . 256 * 60 * 24 # Children × minutes × hours . 368640 . 2. Put any code in code cells . But a code cell can contain much more than a simple one-liner! This is a notebook running Python and you can put any Python code in a code cell (but notebooks can run other languages too, like R). Below is a code cell where we define a whole new function (greet). To show the output of greet we run it last in the code cell as the last value is always printed out. . def greet(first_name, last_name): greeting = &#39;My name is &#39; + last_name + &#39;, &#39; + first_name + &#39; &#39; + last_name + &#39;!&#39; return greeting # Replace with your first and last name. # That is, unless your name is already Jane Bond. greet(&#39;Anurag&#39;, &#39;Peddi&#39;) . &#39;My name is Peddi, Anurag Peddi!&#39; . 3. Jupyter Notebooks &#9825; SQL (part i) . We&#39;ve seen that notebooks can display basic objects such as numbers and strings. But notebooks also support and display the outputs of SQL commands! Using an open source Jupyter extension called ipython-sql, we can connect to a database and issue SQL commands within our notebook. For example, we can connect to a PostgreSQL database that has a table that contains country data, then inspect the first three rows of the table by putting %%sql ahead of the SQL commands (more on the meaning of %% later). . %%sql postgresql:///countries SELECT * FROM countries LIMIT 3; . 3 rows affected. . code name continent region surface_area indep_year local_name gov_form capital cap_long cap_lat . AFG | Afghanistan | Asia | Southern and Central Asia | 652090.0 | 1919 | Afganistan/Afqanestan | Islamic Emirate | Kabul | 69.1761 | 34.5228 | . NLD | Netherlands | Europe | Western Europe | 41526.0 | 1581 | Nederland | Constitutional Monarchy | Amsterdam | 4.89095 | 52.3738 | . ALB | Albania | Europe | Southern Europe | 28748.0 | 1912 | Shqiperia | Republic | Tirane | 19.8172 | 41.3317 | . 4. Jupyter Notebooks &#9825; SQL (part ii) . And after the first connection to the database, the connection code (postgresql:///countries) can be omitted. Let&#39;s do a different query this time and select the row in the countries table for Belgium. Note the single % this time. Again, more on that later. . %sql SELECT * FROM countries where name = &#39;Belgium&#39;; . * postgresql:///countries 1 rows affected. . code name continent region surface_area indep_year local_name gov_form capital cap_long cap_lat . BEL | Belgium | Europe | Western Europe | 30518.0 | 1830 | Belgie/Belgique | Constitutional Monarchy, Federation | Brussels | 4.36761 | 50.8371 | . 5. Jupyter Notebooks &#9825; SQL (part iii) . We can even convert our SQL results to a pandas DataFrame! Let&#39;s convert the entire countries table. . result = %sql SELECT * FROM countries; # To pandas DataFrame df = result.DataFrame() df.info() . * postgresql:///countries 206 rows affected. &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 206 entries, 0 to 205 Data columns (total 11 columns): code 206 non-null object name 206 non-null object continent 206 non-null object region 206 non-null object surface_area 206 non-null float64 indep_year 188 non-null float64 local_name 206 non-null object gov_form 206 non-null object capital 201 non-null object cap_long 204 non-null float64 cap_lat 204 non-null float64 dtypes: float64(4), object(7) memory usage: 17.8+ KB . 6. Jupyter Notebooks &#9825; SQLAlchemy . If SQLAlchemy is your thing, you can do that in this notebook, too! Jupyter Notebooks love everything, apparently... . What&#39;s SQLAlchemy, you ask? SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. Next, we&#39;ll run the last query we just ran except after connecting to and querying the database using SQLAlchemy. . from sqlalchemy import create_engine engine = create_engine(&quot;postgresql:///countries&quot;); # Query database result = engine.execute(&quot;SELECT * FROM countries;&quot;) # Display column names result.keys() . [&#39;code&#39;, &#39;name&#39;, &#39;continent&#39;, &#39;region&#39;, &#39;surface_area&#39;, &#39;indep_year&#39;, &#39;local_name&#39;, &#39;gov_form&#39;, &#39;capital&#39;, &#39;cap_long&#39;, &#39;cap_lat&#39;] . 7. Jupyter Notebooks &#9825; plots . Tables are nice but — as the saying goes — &quot;a plot can show a thousand data points.&quot; Notebooks handle plots as well, but it requires some more magic. Here magic does not refer to any arcane rituals but to so-called &quot;magic commands&quot; that affect how the Jupyter Notebook works. Magic commands start with either % or %% (just like we saw with %sql and %%sql) and the command we need to nicely display plots inline is %matplotlib inline. With this magic in place, all plots created in code cells will automatically be displayed inline. . Using the previously created pandas DataFrame that we named df, let&#39;s plot the number of countries in each continent as a bar chart using the plot() method of pandas DataFrames. . Now, for the difference between %%sql and %sql: ordinary assignment works for single-line %sql queries while %%sql is for multi-line queries. See the Assignment ipython-sql documentation section for more info. . %matplotlib inline # Plotting number of countries in each continent df.continent.value_counts().plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff916880668&gt; . 8. Goodbye for now! . Tables and plots are the most common outputs when doing data analysis, but Jupyter Notebooks can render many more types of outputs such as sound, animation, video, etc. Yes, almost anything that can be shown in a modern web browser. This also makes it possible to include interactive widgets directly in the notebook! Everything in this collection of Jupyter Widgets can be displayed in this notebook. . But that&#39;s enough for now! This was just a short introduction to Jupyter Notebooks, an open source technology that is increasingly used for data science and analysis. We hope you enjoyed it! :) . I_am_ready = True # P.S. Feel free to try out any other stuff in this notebook. # It&#39;s all yours! .",
            "url": "https://anuraganalog.github.io/blog/datacamp/projects/sql/2020/01/25/Introduction-to-DataCamp-Projects.html",
            "relUrl": "/datacamp/projects/sql/2020/01/25/Introduction-to-DataCamp-Projects.html",
            "date": " • Jan 25, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://anuraganalog.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://anuraganalog.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}