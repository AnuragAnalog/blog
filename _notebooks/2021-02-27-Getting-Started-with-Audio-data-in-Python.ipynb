{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Getting Started with Audio data in Python\"\n",
    "> \"In this article, I am will be explaining some of the modules which can be used to load and manipulate some audio files in python.\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [data, audio, file, sound, functions, wave, dockship, article, speech, recognition], \n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays Audio data is also becoming more and more common in the field of Data Science.\n",
    "\n",
    "In this article, I am going to explain how to load, manipulate and store audio data in python.\n",
    "\n",
    "There are different kinds of audio formats:\n",
    "\n",
    "1. mp3\n",
    "2. wav\n",
    "3. flac\n",
    "4. m4a\n",
    "\n",
    "Audio data is stored and measured in frequency(Hz)\n",
    "\n",
    "1KHz means 1000 bytes/units of information stored per second\n",
    "\n",
    "In a small sample of the audio clip, there are some thousands of byte information.\n",
    "\n",
    "Let's dive into some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required modules\n",
    "\n",
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename is the name of the audio file which needs to be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading audio file as wave object\n",
    "\n",
    "gm_wave = wave.open(filename, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting wave object into bytes\n",
    "\n",
    "gm_bytes = gm_wave.readframes(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the variable\n",
    "\n",
    "gm_bytes\n",
    "\n",
    "# Output\n",
    "# b'\\x04\\xbb\\x05\\x86\\t\\x10\\x06\\x82\\r\\xe4\\x06\\xda\\x0e...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the sound variable is in bytes which is now not in human readable form. So let's convert this into human readable form.\n",
    "\n",
    "For that we can use numpy module's frombuffer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing into bytes\n",
    "\n",
    "signal_gm = np.frombuffer(gm_bytes, dtype='int16')\n",
    "signal_gm[:10]\n",
    "\n",
    "# Output\n",
    "# array([ -3, -5, -8, -8, -9, -13, -8, -10, -9, -11], dtype=int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wave object also has so many functions which can be used to get the characteristics of the audio file, they are getframerate(), getnchannels(), getsamplewidth(), getnframes() etc.\n",
    "\n",
    "Frame rate is the number of frequency bytes in one second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the frame rate\n",
    "framerate_gm = gm_wave.getframerate()\n",
    "\n",
    "# Show the frame rate\n",
    "framerate_gm\n",
    "\n",
    "# Output\n",
    "# 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now it's time to visualize the sound wave.**\n",
    "\n",
    "Let's get the timestamp values of the audio file, for that we are using np.linspace() function to create a evenly spaced numpy array\n",
    "\n",
    "Syntax: np.linspace(start, stop, step) which creates step number of floating point values in between start and stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating timestamp values\n",
    "\n",
    "time_gm = np.linspace(start=0, stop=len(signal_gm)/framerate_gm, step=len(signal_gm))\n",
    "time_gm[:10]\n",
    "\n",
    "# Output\n",
    "\n",
    "# array([0.00000000e+00, 1.25000117e-04, 2.50000233e-04, ...,\n",
    "       1.34118250e+02, 1.34118375e+02, 1.34118500e+02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a time stamp array values. last value of the array is the duration of the audio clip.\n",
    "\n",
    "Plotting the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Audio Clip\")\n",
    "\n",
    "plt.plot(time_gm, signal_gm)\n",
    "\n",
    "# x and y axis labels\n",
    "plt.xlabel(\"Time(s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# show our plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from wave there are also some other third party libraries for processing the audio data.\n",
    "\n",
    "1. CMU Sphinx\n",
    "2. Speech Recognition\n",
    "3. Kaldi\n",
    "4. Wav2letter++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Create the instance of recognizer\n",
    "recog = sr.Recognizer()\n",
    "\n",
    "# Set the limit of the energy\n",
    "recog.energy_threshold = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizer class contains many of the built-in functions to convert audio  into text data\n",
    "\n",
    "1. recognize_bing()\n",
    "2. recognize_google()\n",
    "3. recognize_google_cloud()\n",
    "4. recognize_ibm()\n",
    "5. recognize_wit()\n",
    "6. recognize_houndify()\n",
    "Input: Audio file\n",
    "\n",
    "Output: Transcribed text\n",
    "\n",
    "**Note:** Some of the api calls require credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate using google api\n",
    "text = recog.recognize_google(audio_data=audio_file, language='en-US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the audio file which you are passing to the recognize function is of different language, then specify the corresponding language to the second argument.\n",
    "\n",
    "Else it will print the text in the English language.\n",
    "\n",
    "Creating a AudioFile class using sr module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in audio file\n",
    "audio = sr.AudioFile()\n",
    "\n",
    "# Check type of audio\n",
    "type(audio)\n",
    "\n",
    "# Output\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to pass the audio variable to any one of the recognize function, it will throw and error. As the recognize functions accept only the audio_data input.\n",
    "\n",
    "In this case, we need to convert the AudioFile to AudioData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from AudioFile to AudioData\n",
    "\n",
    "with audio as src:\n",
    "    audio_data = recognizer.record(src)\n",
    "\n",
    "# Check the type\n",
    "type(audio_data)\n",
    "\n",
    "# Output\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code snippet, the record function records the audio file in the form of audio_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave duration and offset as default\n",
    "\n",
    "with clean_support_call as source:\n",
    "    clean_support_call_audio = recognizer.record(source, duration=None, offset=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The record function also takes two other arguments which is duration and offset.\n",
    "\n",
    "Duration specifies the time for which the audio data should be recorded and the offset specifies the beginning byte from which the function start capturing the audio data.\n",
    "\n",
    "Audio file can also be of non-speech data i.e. a roar of the lion, barking of the dog etc.\n",
    "\n",
    "In this case, if you pass the audio file, you will get a **UnknownValueError**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the leopard roar audio file\n",
    "leopard_roar = sr.AudioFile(\"leopard_roar.wav\")\n",
    "\n",
    "# Convert the AudioFile to AudioData\n",
    "with leopard_roar as source:\n",
    "    leopard_roar_audio = recognizer.record(source)\n",
    "\n",
    "# Recognize the AudioData\n",
    "recognizer.recognize_google(leopard_roar_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have trouble in hearing the audio file, the api will also have trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import audio file with background nosie\n",
    "noisy_support_call = sr.AudioFile(noisy_support_call.wav)\n",
    "\n",
    "with noisy_support_call as source:\n",
    "# Adjust for ambient noise and record\n",
    "    recognizer.adjust_for_ambient_noise(source,duration=0.5)\n",
    "    noisy_support_call_audio = recognizer.record(source)\n",
    "\n",
    "# Recognize the audio\n",
    "recognizer.recognize_google(noisy_support_call_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code removes the noise from the audio clip by hearing the audio clip by listening to it for 0.5 duration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
